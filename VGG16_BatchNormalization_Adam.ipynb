{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG16_BatchNormalization_Adam.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"515823d4f4b1487a846cf085913201df":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d7906ac97e0441e7a4848560456df5fb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0b1475a4be004cf2a563b90815feb664","IPY_MODEL_121abaada5b4416e971388aaaf9b4d00"]}},"d7906ac97e0441e7a4848560456df5fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0b1475a4be004cf2a563b90815feb664":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8216fdcb58774911ab0aaef1697b5021","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bcec03aab8214a939ceb697427186d35"}},"121abaada5b4416e971388aaaf9b4d00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ea3b5b37c3f74003a881fb012f4d6287","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 169009152/? [00:30&lt;00:00, 17311754.01it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2e7fb9deb1484e0c9fe8052a231593f3"}},"8216fdcb58774911ab0aaef1697b5021":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bcec03aab8214a939ceb697427186d35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ea3b5b37c3f74003a881fb012f4d6287":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2e7fb9deb1484e0c9fe8052a231593f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"s84D8YwLHupg","executionInfo":{"status":"ok","timestamp":1602868622175,"user_tz":240,"elapsed":3832,"user":{"displayName":"Try It","photoUrl":"","userId":"11883681010807328450"}}},"source":["import torch\n","import torch.nn as nn\n","\n","\n","config = [64, 64, 'Max', 128, 128, 'Max', 256, 256, 256, 'Max', 512, 512, 512, 'Max', 512, 512, 512, 'Max']\n","\n","\n","class VGG16(nn.Module):\n","    def __init__(self, features, num_classes=100, dropout = False, init_weights=True):\n","        super(VGG16, self).__init__()\n","        self.features = features\n","        # self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n","        # Add Dropout if specified. \n","        if dropout:\n","            self.classifier = nn.Sequential(\n","                nn.Linear(512, 4096),\n","                nn.ReLU(True),\n","                nn.Dropout(),\n","                nn.Linear(4096, 4096),\n","                nn.ReLU(True),\n","                nn.Dropout(),\n","                nn.Linear(4096, num_classes),)\n","        else: \n","            self.classifier = nn.Sequential(\n","                nn.Linear(512, 4096),\n","                nn.ReLU(True),\n","                nn.Linear(4096, 4096),\n","                nn.ReLU(True),\n","                nn.Linear(4096, num_classes),)\n","            \n","        if init_weights:\n","            for m in self.modules():\n","                if isinstance(m, nn.Conv2d):\n","                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                    if m.bias is not None:\n","                        nn.init.constant_(m.bias, 0)\n","                elif isinstance(m, nn.BatchNorm2d):\n","                    nn.init.constant_(m.weight, 1)\n","                    nn.init.constant_(m.bias, 0)\n","                elif isinstance(m, nn.Linear):\n","                    nn.init.normal_(m.weight, 0, 0.01)\n","                    nn.init.constant_(m.bias, 0)\n","            \n","\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        # x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x\n","\n","\n","def make_layers(config, batch_norm=False, in_channels = 3):\n","    layers = []\n","    for layer in config:\n","        if layer == 'Max':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, layer, kernel_size=3, padding=1)\n","            if batch_norm:\n","                layers += [conv2d, nn.BatchNorm2d(layer), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(0.1)]\n","            in_channels = layer\n","    return nn.Sequential(*layers)\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYQX2c7ZH8x1"},"source":["%reload_ext tensorboard\n","\n","LOG_DIR = \"/content/drive/My Drive/Deep Learning Assignment/logs\"\n","\n","import os   \n","import datetime\n","import tensorflow as tf\n","\n","model = VGG16(make_layers(config, batch_norm = False), dropout = False, init_weights=False)\n","logdir = os.path.join(LOG_DIR, str(model.__class__.__name__), datetime.datetime.now().strftime('%d_%B_%Y_%Hh_%Mm_%Ss'))\n","\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","%tensorboard --logdir='/content/drive/My Drive/Deep Learning Assignment/logs'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHC_mfHEH-bK","executionInfo":{"status":"ok","timestamp":1602617194044,"user_tz":240,"elapsed":4454181,"user":{"displayName":"Bharat Sesham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioxQtWxi3p4Md_ppOnPwtM3F0njyeo8ZYaqLjZng=s64","userId":"09690413437452653454"}},"outputId":"06ea16ed-61ff-4fb6-e3b6-baf112a1876f","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["515823d4f4b1487a846cf085913201df","d7906ac97e0441e7a4848560456df5fb","0b1475a4be004cf2a563b90815feb664","121abaada5b4416e971388aaaf9b4d00","8216fdcb58774911ab0aaef1697b5021","bcec03aab8214a939ceb697427186d35","ea3b5b37c3f74003a881fb012f4d6287","2e7fb9deb1484e0c9fe8052a231593f3"]}},"source":["import torch.optim as optim\n","import torchvision\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision.transforms as transforms\n","import time\n","from datetime import datetime\n","import os\n","import torch\n","import numpy as np\n","from sklearn.metrics import precision_score, recall_score, accuracy_score\n","import torch.nn as nn\n","from torchvision import models\n","\n","\n","def train(epoch):\n","    start = time.time() \n","    y_pred = []\n","    y_true = []   \n","\n","    # Run model in training mode\n","    model.train()\n","\n","    for batch_index, (images, labels) in enumerate(training_loader):\n","        iter_num = (epoch - 1) * len(training_loader) + batch_index + 1\n","        # Convert the inputs to GPU compatible tensors. \n","        if cuda_available:\n","            images = images.cuda()\n","            labels = labels.cuda()\n","\n","        # Learning on the training data\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = loss_function(outputs, labels)\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), 1)\n","        optimizer.step()\n","\n","        output_values, predicted = outputs.max(1)\n","        y_pred.extend(predicted.cpu().tolist())\n","        y_true.extend(labels.cpu().tolist())\n","\n","        # Printing the training results and storing them to tensorboard\n","        print('Training Epoch: {epoch} [{num_trained}/{num_samples}]\\tLoss: {:0.2f}'.format(\n","            loss.item(),\n","            epoch = epoch,\n","            num_trained = batch_index * batch_size + len(images),\n","            num_samples = len(training_loader.dataset)))\n","        writer.add_scalar('Train: Loss', loss.item(), iter_num)\n","        \n","    # Calculating Metrics\n","    accuracy = accuracy_score(y_true, y_pred)\n","\n","    # Time consumed for a epoch\n","    time_consumed = time.time() - start\n","    print('Time taken to train epoch {epoch}: {:.2f}s'.format(time_consumed, epoch = epoch))\n","    writer.add_scalar('Train Set: Accuracy', accuracy, epoch)\n","\n","\n","\n","@torch.no_grad()\n","def test(epoch):\n","    test_loss = 0.0\n","    y_pred = []\n","    y_true = []\n","    len_test_loader = len(test_loader.dataset)\n","    start = time.time()\n","    \n","    # Run model in evaluation mode\n","    model.eval()\n","\n","    for (images, labels) in test_loader:\n","        # Convert the inputs to GPU compatible tensors. \n","        if cuda_available:\n","            images = images.cuda()\n","            labels = labels.cuda()\n","\n","        # Predicting labels of test image set\n","        model_outputs = model(images)\n","        model_loss = loss_function(model_outputs, labels)\n","        test_loss += model_loss.item()\n","        output_values, predicted = model_outputs.max(1)\n","        y_pred.extend(predicted.cpu().tolist())\n","        y_true.extend(labels.cpu().tolist())\n","        \n","    # Calculating Metrics\n","    accuracy = accuracy_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n","    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n","    loss = test_loss / len_test_loader\n","    time_consumed = time.time() - start\n","\n","    # GPU stats\n","    # if cuda_available:\n","    #     print(torch.cuda.memory_summary())\n","\n","    # Printing the testing results and storing them to tensorboard\n","    print('Testing Network for epoch: ', epoch)\n","    print('Evaluation: Evaluation Time: {:.2f}s, Average loss: {:.4f}, Accuracy: {:.4f}, Recall: {:.4f}, Precision: {:.4f}'.format(time_consumed, loss, accuracy, recall, precision))\n","    writer.add_scalar('Test Set: Average loss', loss, epoch)\n","    writer.add_scalar('Test Set: Accuracy', accuracy, epoch)\n","    return accuracy, test_loss\n","\n","\n","global cuda_available\n","global writer\n","global batch_size\n","DATA_ROOT = './data'\n","batch_size = 256\n","epochs = 200\n","min_early_stopping = 150\n","patience = 20\n","milestones = [50, 100, 150]\n","LOG_DIR = '/content/drive/My Drive/Deep Learning Assignment/logs'\n","checkpoints_path = '/content/drive/My Drive/Deep Learning Assignment/checkpoints'\n","batch_norm = True\n","dropout = False\n","\n","\n","setting = ''\n","if dropout:\n","    setting = 'Dropout'\n","elif batch_norm:\n","    setting = 'BatchNormalization'\n","else:\n","    setting = 'NoRegularization'\n","\n","# Method to compute mean and std. \n","# def compute_mean_std(cifar100_dataset):\n","#     data_r = numpy.dstack([cifar100_dataset[i][1][:, :, 0] for i in range(len(cifar100_dataset))])\n","#     data_g = numpy.dstack([cifar100_dataset[i][1][:, :, 1] for i in range(len(cifar100_dataset))])\n","#     data_b = numpy.dstack([cifar100_dataset[i][1][:, :, 2] for i in range(len(cifar100_dataset))])\n","#     mean = numpy.mean(data_r), numpy.mean(data_g), numpy.mean(data_b)\n","#     std = numpy.std(data_r), numpy.std(data_g), numpy.std(data_b)\n","#     return mean, std\n","\n","# mean and std are computed on the training set. \n","mean = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n","std = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n","\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","])\n","\n","# Load training data from CIFAR100\n","train_data = torchvision.datasets.CIFAR100(root=DATA_ROOT, train=True, download=True, transform=transform_train)\n","training_loader = DataLoader(train_data, shuffle=True, num_workers=4, batch_size=batch_size)\n","\n","# Load testing data from CIFAR100\n","test_data = torchvision.datasets.CIFAR100(root=DATA_ROOT, train=False, download=True, transform=transform_test)\n","test_loader = DataLoader(test_data, shuffle=True, num_workers=4, batch_size=batch_size)\n","\n","# Model\n","model = VGG16(make_layers(config, batch_norm = batch_norm), dropout = dropout, init_weights=True)\n","print(model)\n","\n","# Check if any GPU is available\n","cuda_available = torch.cuda.is_available()\n","if torch.cuda.is_available():\n","    model.cuda()             # Convert the model to GPU compatible. \n","\n","# Define Loss Function, Optimizer\n","loss_function = nn.CrossEntropyLoss()\n","# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n","scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.2) \n","\n","# Tensorboard\n","log_model_path = os.path.join(LOG_DIR, \n","                              '{model}_{setting}_{optimizer}'.format(model=str(model.__class__.__name__), \n","                               setting=setting, \n","                               optimizer=optimizer.__class__.__name__))\n","if not os.path.exists(log_model_path):\n","    os.mkdir(log_model_path)\n","writer = SummaryWriter(log_dir=os.path.join(log_model_path, datetime.now().strftime('%d_%B_%Y_%Hh_%Mm_%Ss')))\n","dummy_input_tensor = torch.Tensor(1, 3, 32, 32).cuda()\n","writer.add_graph(model, dummy_input_tensor)\n","\n","# Checkpoints folder to save model\n","if not os.path.exists(checkpoints_path):\n","    os.makedirs(checkpoints_path)\n","checkpoints_path = os.path.join(checkpoints_path, '{model}_{setting}_{optimizer}')\n","\n","# Initialize the early_stopping_counter.\n","early_stopping_counter = 0\n","\n","# Train and Evaluting the model\n","min_validation_loss = float('inf')\n","for epoch in range(1, epochs):\n","    # Adaptive Learning Rate\n","    scheduler.step(epoch)\n","    # Checking for early stopping condition. \n","    if early_stopping_counter > patience and epoch >= min_early_stopping:\n","        print(\"Early Stopping the model training as there no significant improvment in the eval loss.\")\n","        break\n","    train(epoch)\n","    accuracy, validation_loss = test(epoch)\n","    if validation_loss < min_validation_loss:\n","        torch.save(model.state_dict(), checkpoints_path.format(\n","            model=str(model.__class__.__name__), \n","            setting=setting, \n","            optimizer=str(optimizer.__class__.__name__)))\n","        min_validation_loss = validation_loss\n","        early_stopping_counter = 0\n","    else: \n","        early_stopping_counter += 1 \n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"515823d4f4b1487a846cf085913201df","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-100-python.tar.gz to ./data\n","Files already downloaded and verified\n","VGG16(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (9): ReLU(inplace=True)\n","    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (12): ReLU(inplace=True)\n","    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (16): ReLU(inplace=True)\n","    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (19): ReLU(inplace=True)\n","    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (26): ReLU(inplace=True)\n","    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (29): ReLU(inplace=True)\n","    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (32): ReLU(inplace=True)\n","    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (36): ReLU(inplace=True)\n","    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (39): ReLU(inplace=True)\n","    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (42): ReLU(inplace=True)\n","    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=512, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Linear(in_features=4096, out_features=4096, bias=True)\n","    (3): ReLU(inplace=True)\n","    (4): Linear(in_features=4096, out_features=100, bias=True)\n","  )\n",")\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py:1119: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n","With rtol=1e-05 and atol=1e-05, found 10 element(s) (out of 100) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 9.260265525002195e+21 (3.3285522678181585e+26 vs. 3.3284596651629084e+26), which occurred at index (0, 84).\n","  check_tolerance, strict, _force_outplace, True, _module_class)\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Training Epoch: 124 [45312/50000]\tLoss: 0.75\n","Training Epoch: 124 [45568/50000]\tLoss: 0.67\n","Training Epoch: 124 [45824/50000]\tLoss: 0.66\n","Training Epoch: 124 [46080/50000]\tLoss: 0.77\n","Training Epoch: 124 [46336/50000]\tLoss: 0.70\n","Training Epoch: 124 [46592/50000]\tLoss: 0.83\n","Training Epoch: 124 [46848/50000]\tLoss: 0.92\n","Training Epoch: 124 [47104/50000]\tLoss: 0.72\n","Training Epoch: 124 [47360/50000]\tLoss: 0.82\n","Training Epoch: 124 [47616/50000]\tLoss: 0.81\n","Training Epoch: 124 [47872/50000]\tLoss: 0.80\n","Training Epoch: 124 [48128/50000]\tLoss: 0.75\n","Training Epoch: 124 [48384/50000]\tLoss: 0.78\n","Training Epoch: 124 [48640/50000]\tLoss: 0.86\n","Training Epoch: 124 [48896/50000]\tLoss: 0.85\n","Training Epoch: 124 [49152/50000]\tLoss: 0.87\n","Training Epoch: 124 [49408/50000]\tLoss: 1.00\n","Training Epoch: 124 [49664/50000]\tLoss: 0.84\n","Training Epoch: 124 [49920/50000]\tLoss: 0.69\n","Training Epoch: 124 [50000/50000]\tLoss: 0.77\n","Time taken to train epoch 124: 27.18s\n","Testing Network for epoch:  124\n","Evaluation: Evaluation Time: 2.57s, Average loss: 0.0062, Accuracy: 0.6218, Recall: 0.6218, Precision: 0.6269\n","Training Epoch: 125 [256/50000]\tLoss: 0.74\n","Training Epoch: 125 [512/50000]\tLoss: 0.71\n","Training Epoch: 125 [768/50000]\tLoss: 0.70\n","Training Epoch: 125 [1024/50000]\tLoss: 0.64\n","Training Epoch: 125 [1280/50000]\tLoss: 0.65\n","Training Epoch: 125 [1536/50000]\tLoss: 0.81\n","Training Epoch: 125 [1792/50000]\tLoss: 0.85\n","Training Epoch: 125 [2048/50000]\tLoss: 0.73\n","Training Epoch: 125 [2304/50000]\tLoss: 0.76\n","Training Epoch: 125 [2560/50000]\tLoss: 0.72\n","Training Epoch: 125 [2816/50000]\tLoss: 0.71\n","Training Epoch: 125 [3072/50000]\tLoss: 0.77\n","Training Epoch: 125 [3328/50000]\tLoss: 0.76\n","Training Epoch: 125 [3584/50000]\tLoss: 0.81\n","Training Epoch: 125 [3840/50000]\tLoss: 0.67\n","Training Epoch: 125 [4096/50000]\tLoss: 0.63\n","Training Epoch: 125 [4352/50000]\tLoss: 0.82\n","Training Epoch: 125 [4608/50000]\tLoss: 0.77\n","Training Epoch: 125 [4864/50000]\tLoss: 0.78\n","Training Epoch: 125 [5120/50000]\tLoss: 0.84\n","Training Epoch: 125 [5376/50000]\tLoss: 0.79\n","Training Epoch: 125 [5632/50000]\tLoss: 0.69\n","Training Epoch: 125 [5888/50000]\tLoss: 0.68\n","Training Epoch: 125 [6144/50000]\tLoss: 0.67\n","Training Epoch: 125 [6400/50000]\tLoss: 0.76\n","Training Epoch: 125 [6656/50000]\tLoss: 0.74\n","Training Epoch: 125 [6912/50000]\tLoss: 0.75\n","Training Epoch: 125 [7168/50000]\tLoss: 0.78\n","Training Epoch: 125 [7424/50000]\tLoss: 0.76\n","Training Epoch: 125 [7680/50000]\tLoss: 0.61\n","Training Epoch: 125 [7936/50000]\tLoss: 0.81\n","Training Epoch: 125 [8192/50000]\tLoss: 0.77\n","Training Epoch: 125 [8448/50000]\tLoss: 0.84\n","Training Epoch: 125 [8704/50000]\tLoss: 0.71\n","Training Epoch: 125 [8960/50000]\tLoss: 0.85\n","Training Epoch: 125 [9216/50000]\tLoss: 0.78\n","Training Epoch: 125 [9472/50000]\tLoss: 0.75\n","Training Epoch: 125 [9728/50000]\tLoss: 0.82\n","Training Epoch: 125 [9984/50000]\tLoss: 0.74\n","Training Epoch: 125 [10240/50000]\tLoss: 0.82\n","Training Epoch: 125 [10496/50000]\tLoss: 0.89\n","Training Epoch: 125 [10752/50000]\tLoss: 0.81\n","Training Epoch: 125 [11008/50000]\tLoss: 0.76\n","Training Epoch: 125 [11264/50000]\tLoss: 0.77\n","Training Epoch: 125 [11520/50000]\tLoss: 0.93\n","Training Epoch: 125 [11776/50000]\tLoss: 0.74\n","Training Epoch: 125 [12032/50000]\tLoss: 0.94\n","Training Epoch: 125 [12288/50000]\tLoss: 0.86\n","Training Epoch: 125 [12544/50000]\tLoss: 0.75\n","Training Epoch: 125 [12800/50000]\tLoss: 0.71\n","Training Epoch: 125 [13056/50000]\tLoss: 0.85\n","Training Epoch: 125 [13312/50000]\tLoss: 0.81\n","Training Epoch: 125 [13568/50000]\tLoss: 0.97\n","Training Epoch: 125 [13824/50000]\tLoss: 0.81\n","Training Epoch: 125 [14080/50000]\tLoss: 0.81\n","Training Epoch: 125 [14336/50000]\tLoss: 0.79\n","Training Epoch: 125 [14592/50000]\tLoss: 0.75\n","Training Epoch: 125 [14848/50000]\tLoss: 0.87\n","Training Epoch: 125 [15104/50000]\tLoss: 0.67\n","Training Epoch: 125 [15360/50000]\tLoss: 0.75\n","Training Epoch: 125 [15616/50000]\tLoss: 0.78\n","Training Epoch: 125 [15872/50000]\tLoss: 0.90\n","Training Epoch: 125 [16128/50000]\tLoss: 0.75\n","Training Epoch: 125 [16384/50000]\tLoss: 0.79\n","Training Epoch: 125 [16640/50000]\tLoss: 0.74\n","Training Epoch: 125 [16896/50000]\tLoss: 0.84\n","Training Epoch: 125 [17152/50000]\tLoss: 0.73\n","Training Epoch: 125 [17408/50000]\tLoss: 0.73\n","Training Epoch: 125 [17664/50000]\tLoss: 0.91\n","Training Epoch: 125 [17920/50000]\tLoss: 0.83\n","Training Epoch: 125 [18176/50000]\tLoss: 0.88\n","Training Epoch: 125 [18432/50000]\tLoss: 0.83\n","Training Epoch: 125 [18688/50000]\tLoss: 0.78\n","Training Epoch: 125 [18944/50000]\tLoss: 0.82\n","Training Epoch: 125 [19200/50000]\tLoss: 0.73\n","Training Epoch: 125 [19456/50000]\tLoss: 0.86\n","Training Epoch: 125 [19712/50000]\tLoss: 0.74\n","Training Epoch: 125 [19968/50000]\tLoss: 0.81\n","Training Epoch: 125 [20224/50000]\tLoss: 0.87\n","Training Epoch: 125 [20480/50000]\tLoss: 0.83\n","Training Epoch: 125 [20736/50000]\tLoss: 0.93\n","Training Epoch: 125 [20992/50000]\tLoss: 0.71\n","Training Epoch: 125 [21248/50000]\tLoss: 0.74\n","Training Epoch: 125 [21504/50000]\tLoss: 0.82\n","Training Epoch: 125 [21760/50000]\tLoss: 0.74\n","Training Epoch: 125 [22016/50000]\tLoss: 0.83\n","Training Epoch: 125 [22272/50000]\tLoss: 0.73\n","Training Epoch: 125 [22528/50000]\tLoss: 0.73\n","Training Epoch: 125 [22784/50000]\tLoss: 0.72\n","Training Epoch: 125 [23040/50000]\tLoss: 0.80\n","Training Epoch: 125 [23296/50000]\tLoss: 0.69\n","Training Epoch: 125 [23552/50000]\tLoss: 0.82\n","Training Epoch: 125 [23808/50000]\tLoss: 0.82\n","Training Epoch: 125 [24064/50000]\tLoss: 0.76\n","Training Epoch: 125 [24320/50000]\tLoss: 0.79\n","Training Epoch: 125 [24576/50000]\tLoss: 0.69\n","Training Epoch: 125 [24832/50000]\tLoss: 0.71\n","Training Epoch: 125 [25088/50000]\tLoss: 0.84\n","Training Epoch: 125 [25344/50000]\tLoss: 0.81\n","Training Epoch: 125 [25600/50000]\tLoss: 0.85\n","Training Epoch: 125 [25856/50000]\tLoss: 0.72\n","Training Epoch: 125 [26112/50000]\tLoss: 0.77\n","Training Epoch: 125 [26368/50000]\tLoss: 0.74\n","Training Epoch: 125 [26624/50000]\tLoss: 0.83\n","Training Epoch: 125 [26880/50000]\tLoss: 0.74\n","Training Epoch: 125 [27136/50000]\tLoss: 0.79\n","Training Epoch: 125 [27392/50000]\tLoss: 0.87\n","Training Epoch: 125 [27648/50000]\tLoss: 0.84\n","Training Epoch: 125 [27904/50000]\tLoss: 0.84\n","Training Epoch: 125 [28160/50000]\tLoss: 0.78\n","Training Epoch: 125 [28416/50000]\tLoss: 0.94\n","Training Epoch: 125 [28672/50000]\tLoss: 0.87\n","Training Epoch: 125 [28928/50000]\tLoss: 0.66\n","Training Epoch: 125 [29184/50000]\tLoss: 0.90\n","Training Epoch: 125 [29440/50000]\tLoss: 0.78\n","Training Epoch: 125 [29696/50000]\tLoss: 0.84\n","Training Epoch: 125 [29952/50000]\tLoss: 0.78\n","Training Epoch: 125 [30208/50000]\tLoss: 0.74\n","Training Epoch: 125 [30464/50000]\tLoss: 0.74\n","Training Epoch: 125 [30720/50000]\tLoss: 0.76\n","Training Epoch: 125 [30976/50000]\tLoss: 0.89\n","Training Epoch: 125 [31232/50000]\tLoss: 0.74\n","Training Epoch: 125 [31488/50000]\tLoss: 0.73\n","Training Epoch: 125 [31744/50000]\tLoss: 0.86\n","Training Epoch: 125 [32000/50000]\tLoss: 0.68\n","Training Epoch: 125 [32256/50000]\tLoss: 0.73\n","Training Epoch: 125 [32512/50000]\tLoss: 0.57\n","Training Epoch: 125 [32768/50000]\tLoss: 0.84\n","Training Epoch: 125 [33024/50000]\tLoss: 0.71\n","Training Epoch: 125 [33280/50000]\tLoss: 0.85\n","Training Epoch: 125 [33536/50000]\tLoss: 0.76\n","Training Epoch: 125 [33792/50000]\tLoss: 0.94\n","Training Epoch: 125 [34048/50000]\tLoss: 0.79\n","Training Epoch: 125 [34304/50000]\tLoss: 0.90\n","Training Epoch: 125 [34560/50000]\tLoss: 0.75\n","Training Epoch: 125 [34816/50000]\tLoss: 0.75\n","Training Epoch: 125 [35072/50000]\tLoss: 0.83\n","Training Epoch: 125 [35328/50000]\tLoss: 0.84\n","Training Epoch: 125 [35584/50000]\tLoss: 0.94\n","Training Epoch: 125 [35840/50000]\tLoss: 0.85\n","Training Epoch: 125 [36096/50000]\tLoss: 0.80\n","Training Epoch: 125 [36352/50000]\tLoss: 0.75\n","Training Epoch: 125 [36608/50000]\tLoss: 0.73\n","Training Epoch: 125 [36864/50000]\tLoss: 0.82\n","Training Epoch: 125 [37120/50000]\tLoss: 0.67\n","Training Epoch: 125 [37376/50000]\tLoss: 0.69\n","Training Epoch: 125 [37632/50000]\tLoss: 0.67\n","Training Epoch: 125 [37888/50000]\tLoss: 0.78\n","Training Epoch: 125 [38144/50000]\tLoss: 0.83\n","Training Epoch: 125 [38400/50000]\tLoss: 0.82\n","Training Epoch: 125 [38656/50000]\tLoss: 0.77\n","Training Epoch: 125 [38912/50000]\tLoss: 0.83\n","Training Epoch: 125 [39168/50000]\tLoss: 0.85\n","Training Epoch: 125 [39424/50000]\tLoss: 0.83\n","Training Epoch: 125 [39680/50000]\tLoss: 0.77\n","Training Epoch: 125 [39936/50000]\tLoss: 0.74\n","Training Epoch: 125 [40192/50000]\tLoss: 0.62\n","Training Epoch: 125 [40448/50000]\tLoss: 0.69\n","Training Epoch: 125 [40704/50000]\tLoss: 0.82\n","Training Epoch: 125 [40960/50000]\tLoss: 0.68\n","Training Epoch: 125 [41216/50000]\tLoss: 0.94\n","Training Epoch: 125 [41472/50000]\tLoss: 0.88\n","Training Epoch: 125 [41728/50000]\tLoss: 0.75\n","Training Epoch: 125 [41984/50000]\tLoss: 0.80\n","Training Epoch: 125 [42240/50000]\tLoss: 0.82\n","Training Epoch: 125 [42496/50000]\tLoss: 0.78\n","Training Epoch: 125 [42752/50000]\tLoss: 0.80\n","Training Epoch: 125 [43008/50000]\tLoss: 0.90\n","Training Epoch: 125 [43264/50000]\tLoss: 0.71\n","Training Epoch: 125 [43520/50000]\tLoss: 0.79\n","Training Epoch: 125 [43776/50000]\tLoss: 0.82\n","Training Epoch: 125 [44032/50000]\tLoss: 0.83\n","Training Epoch: 125 [44288/50000]\tLoss: 0.87\n","Training Epoch: 125 [44544/50000]\tLoss: 0.74\n","Training Epoch: 125 [44800/50000]\tLoss: 0.81\n","Training Epoch: 125 [45056/50000]\tLoss: 0.73\n","Training Epoch: 125 [45312/50000]\tLoss: 0.71\n","Training Epoch: 125 [45568/50000]\tLoss: 0.71\n","Training Epoch: 125 [45824/50000]\tLoss: 0.84\n","Training Epoch: 125 [46080/50000]\tLoss: 0.91\n","Training Epoch: 125 [46336/50000]\tLoss: 0.69\n","Training Epoch: 125 [46592/50000]\tLoss: 0.79\n","Training Epoch: 125 [46848/50000]\tLoss: 0.93\n","Training Epoch: 125 [47104/50000]\tLoss: 0.73\n","Training Epoch: 125 [47360/50000]\tLoss: 1.00\n","Training Epoch: 125 [47616/50000]\tLoss: 0.76\n","Training Epoch: 125 [47872/50000]\tLoss: 0.84\n","Training Epoch: 125 [48128/50000]\tLoss: 0.81\n","Training Epoch: 125 [48384/50000]\tLoss: 0.80\n","Training Epoch: 125 [48640/50000]\tLoss: 0.80\n","Training Epoch: 125 [48896/50000]\tLoss: 0.72\n","Training Epoch: 125 [49152/50000]\tLoss: 0.83\n","Training Epoch: 125 [49408/50000]\tLoss: 0.82\n","Training Epoch: 125 [49664/50000]\tLoss: 0.79\n","Training Epoch: 125 [49920/50000]\tLoss: 0.81\n","Training Epoch: 125 [50000/50000]\tLoss: 0.87\n","Time taken to train epoch 125: 27.26s\n","Testing Network for epoch:  125\n","Evaluation: Evaluation Time: 2.65s, Average loss: 0.0061, Accuracy: 0.6237, Recall: 0.6237, Precision: 0.6273\n","Training Epoch: 126 [256/50000]\tLoss: 0.76\n","Training Epoch: 126 [512/50000]\tLoss: 0.69\n","Training Epoch: 126 [768/50000]\tLoss: 0.79\n","Training Epoch: 126 [1024/50000]\tLoss: 0.75\n","Training Epoch: 126 [1280/50000]\tLoss: 0.66\n","Training Epoch: 126 [1536/50000]\tLoss: 0.86\n","Training Epoch: 126 [1792/50000]\tLoss: 0.73\n","Training Epoch: 126 [2048/50000]\tLoss: 0.76\n","Training Epoch: 126 [2304/50000]\tLoss: 0.74\n","Training Epoch: 126 [2560/50000]\tLoss: 0.97\n","Training Epoch: 126 [2816/50000]\tLoss: 0.77\n","Training Epoch: 126 [3072/50000]\tLoss: 0.87\n","Training Epoch: 126 [3328/50000]\tLoss: 0.69\n","Training Epoch: 126 [3584/50000]\tLoss: 0.92\n","Training Epoch: 126 [3840/50000]\tLoss: 0.75\n","Training Epoch: 126 [4096/50000]\tLoss: 0.96\n","Training Epoch: 126 [4352/50000]\tLoss: 0.85\n","Training Epoch: 126 [4608/50000]\tLoss: 0.80\n","Training Epoch: 126 [4864/50000]\tLoss: 0.74\n","Training Epoch: 126 [5120/50000]\tLoss: 0.73\n","Training Epoch: 126 [5376/50000]\tLoss: 0.65\n","Training Epoch: 126 [5632/50000]\tLoss: 0.97\n","Training Epoch: 126 [5888/50000]\tLoss: 0.72\n","Training Epoch: 126 [6144/50000]\tLoss: 0.88\n","Training Epoch: 126 [6400/50000]\tLoss: 0.72\n","Training Epoch: 126 [6656/50000]\tLoss: 0.87\n","Training Epoch: 126 [6912/50000]\tLoss: 0.71\n","Training Epoch: 126 [7168/50000]\tLoss: 0.81\n","Training Epoch: 126 [7424/50000]\tLoss: 0.78\n","Training Epoch: 126 [7680/50000]\tLoss: 0.83\n","Training Epoch: 126 [7936/50000]\tLoss: 0.75\n","Training Epoch: 126 [8192/50000]\tLoss: 0.80\n","Training Epoch: 126 [8448/50000]\tLoss: 0.72\n","Training Epoch: 126 [8704/50000]\tLoss: 0.71\n","Training Epoch: 126 [8960/50000]\tLoss: 0.62\n","Training Epoch: 126 [9216/50000]\tLoss: 0.72\n","Training Epoch: 126 [9472/50000]\tLoss: 0.79\n","Training Epoch: 126 [9728/50000]\tLoss: 0.86\n","Training Epoch: 126 [9984/50000]\tLoss: 0.83\n","Training Epoch: 126 [10240/50000]\tLoss: 0.68\n","Training Epoch: 126 [10496/50000]\tLoss: 0.77\n","Training Epoch: 126 [10752/50000]\tLoss: 0.76\n","Training Epoch: 126 [11008/50000]\tLoss: 0.70\n","Training Epoch: 126 [11264/50000]\tLoss: 0.64\n","Training Epoch: 126 [11520/50000]\tLoss: 0.81\n","Training Epoch: 126 [11776/50000]\tLoss: 0.81\n","Training Epoch: 126 [12032/50000]\tLoss: 0.74\n","Training Epoch: 126 [12288/50000]\tLoss: 0.86\n","Training Epoch: 126 [12544/50000]\tLoss: 0.87\n","Training Epoch: 126 [12800/50000]\tLoss: 0.76\n","Training Epoch: 126 [13056/50000]\tLoss: 0.64\n","Training Epoch: 126 [13312/50000]\tLoss: 0.79\n","Training Epoch: 126 [13568/50000]\tLoss: 0.76\n","Training Epoch: 126 [13824/50000]\tLoss: 0.66\n","Training Epoch: 126 [14080/50000]\tLoss: 0.76\n","Training Epoch: 126 [14336/50000]\tLoss: 0.72\n","Training Epoch: 126 [14592/50000]\tLoss: 0.79\n","Training Epoch: 126 [14848/50000]\tLoss: 0.88\n","Training Epoch: 126 [15104/50000]\tLoss: 0.81\n","Training Epoch: 126 [15360/50000]\tLoss: 0.74\n","Training Epoch: 126 [15616/50000]\tLoss: 0.90\n","Training Epoch: 126 [15872/50000]\tLoss: 0.79\n","Training Epoch: 126 [16128/50000]\tLoss: 0.79\n","Training Epoch: 126 [16384/50000]\tLoss: 0.95\n","Training Epoch: 126 [16640/50000]\tLoss: 0.83\n","Training Epoch: 126 [16896/50000]\tLoss: 0.94\n","Training Epoch: 126 [17152/50000]\tLoss: 0.73\n","Training Epoch: 126 [17408/50000]\tLoss: 0.81\n","Training Epoch: 126 [17664/50000]\tLoss: 0.71\n","Training Epoch: 126 [17920/50000]\tLoss: 0.95\n","Training Epoch: 126 [18176/50000]\tLoss: 0.72\n","Training Epoch: 126 [18432/50000]\tLoss: 0.75\n","Training Epoch: 126 [18688/50000]\tLoss: 0.76\n","Training Epoch: 126 [18944/50000]\tLoss: 0.78\n","Training Epoch: 126 [19200/50000]\tLoss: 0.86\n","Training Epoch: 126 [19456/50000]\tLoss: 0.78\n","Training Epoch: 126 [19712/50000]\tLoss: 0.82\n","Training Epoch: 126 [19968/50000]\tLoss: 0.86\n","Training Epoch: 126 [20224/50000]\tLoss: 0.91\n","Training Epoch: 126 [20480/50000]\tLoss: 0.75\n","Training Epoch: 126 [20736/50000]\tLoss: 0.77\n","Training Epoch: 126 [20992/50000]\tLoss: 0.71\n","Training Epoch: 126 [21248/50000]\tLoss: 0.65\n","Training Epoch: 126 [21504/50000]\tLoss: 0.81\n","Training Epoch: 126 [21760/50000]\tLoss: 0.80\n","Training Epoch: 126 [22016/50000]\tLoss: 0.79\n","Training Epoch: 126 [22272/50000]\tLoss: 0.78\n","Training Epoch: 126 [22528/50000]\tLoss: 0.81\n","Training Epoch: 126 [22784/50000]\tLoss: 0.90\n","Training Epoch: 126 [23040/50000]\tLoss: 0.72\n","Training Epoch: 126 [23296/50000]\tLoss: 0.80\n","Training Epoch: 126 [23552/50000]\tLoss: 0.76\n","Training Epoch: 126 [23808/50000]\tLoss: 0.83\n","Training Epoch: 126 [24064/50000]\tLoss: 0.79\n","Training Epoch: 126 [24320/50000]\tLoss: 0.71\n","Training Epoch: 126 [24576/50000]\tLoss: 0.87\n","Training Epoch: 126 [24832/50000]\tLoss: 0.76\n","Training Epoch: 126 [25088/50000]\tLoss: 0.68\n","Training Epoch: 126 [25344/50000]\tLoss: 0.79\n","Training Epoch: 126 [25600/50000]\tLoss: 0.65\n","Training Epoch: 126 [25856/50000]\tLoss: 0.74\n","Training Epoch: 126 [26112/50000]\tLoss: 0.78\n","Training Epoch: 126 [26368/50000]\tLoss: 0.87\n","Training Epoch: 126 [26624/50000]\tLoss: 0.71\n","Training Epoch: 126 [26880/50000]\tLoss: 0.87\n","Training Epoch: 126 [27136/50000]\tLoss: 0.73\n","Training Epoch: 126 [27392/50000]\tLoss: 0.71\n","Training Epoch: 126 [27648/50000]\tLoss: 0.98\n","Training Epoch: 126 [27904/50000]\tLoss: 0.75\n","Training Epoch: 126 [28160/50000]\tLoss: 0.85\n","Training Epoch: 126 [28416/50000]\tLoss: 0.73\n","Training Epoch: 126 [28672/50000]\tLoss: 0.73\n","Training Epoch: 126 [28928/50000]\tLoss: 0.82\n","Training Epoch: 126 [29184/50000]\tLoss: 0.65\n","Training Epoch: 126 [29440/50000]\tLoss: 0.70\n","Training Epoch: 126 [29696/50000]\tLoss: 0.75\n","Training Epoch: 126 [29952/50000]\tLoss: 0.83\n","Training Epoch: 126 [30208/50000]\tLoss: 0.77\n","Training Epoch: 126 [30464/50000]\tLoss: 0.74\n","Training Epoch: 126 [30720/50000]\tLoss: 0.86\n","Training Epoch: 126 [30976/50000]\tLoss: 0.66\n","Training Epoch: 126 [31232/50000]\tLoss: 0.91\n","Training Epoch: 126 [31488/50000]\tLoss: 0.78\n","Training Epoch: 126 [31744/50000]\tLoss: 0.76\n","Training Epoch: 126 [32000/50000]\tLoss: 0.72\n","Training Epoch: 126 [32256/50000]\tLoss: 0.79\n","Training Epoch: 126 [32512/50000]\tLoss: 0.76\n","Training Epoch: 126 [32768/50000]\tLoss: 0.83\n","Training Epoch: 126 [33024/50000]\tLoss: 0.81\n","Training Epoch: 126 [33280/50000]\tLoss: 0.67\n","Training Epoch: 126 [33536/50000]\tLoss: 0.79\n","Training Epoch: 126 [33792/50000]\tLoss: 0.78\n","Training Epoch: 126 [34048/50000]\tLoss: 0.89\n","Training Epoch: 126 [34304/50000]\tLoss: 0.84\n","Training Epoch: 126 [34560/50000]\tLoss: 0.70\n","Training Epoch: 126 [34816/50000]\tLoss: 0.85\n","Training Epoch: 126 [35072/50000]\tLoss: 0.78\n","Training Epoch: 126 [35328/50000]\tLoss: 0.78\n","Training Epoch: 126 [35584/50000]\tLoss: 0.71\n","Training Epoch: 126 [35840/50000]\tLoss: 0.76\n","Training Epoch: 126 [36096/50000]\tLoss: 0.93\n","Training Epoch: 126 [36352/50000]\tLoss: 0.81\n","Training Epoch: 126 [36608/50000]\tLoss: 0.75\n","Training Epoch: 126 [36864/50000]\tLoss: 0.68\n","Training Epoch: 126 [37120/50000]\tLoss: 0.81\n","Training Epoch: 126 [37376/50000]\tLoss: 0.84\n","Training Epoch: 126 [37632/50000]\tLoss: 0.88\n","Training Epoch: 126 [37888/50000]\tLoss: 0.91\n","Training Epoch: 126 [38144/50000]\tLoss: 0.83\n","Training Epoch: 126 [38400/50000]\tLoss: 0.79\n","Training Epoch: 126 [38656/50000]\tLoss: 0.81\n","Training Epoch: 126 [38912/50000]\tLoss: 0.99\n","Training Epoch: 126 [39168/50000]\tLoss: 0.74\n","Training Epoch: 126 [39424/50000]\tLoss: 1.02\n","Training Epoch: 126 [39680/50000]\tLoss: 0.77\n","Training Epoch: 126 [39936/50000]\tLoss: 0.76\n","Training Epoch: 126 [40192/50000]\tLoss: 0.86\n","Training Epoch: 126 [40448/50000]\tLoss: 0.72\n","Training Epoch: 126 [40704/50000]\tLoss: 0.71\n","Training Epoch: 126 [40960/50000]\tLoss: 0.81\n","Training Epoch: 126 [41216/50000]\tLoss: 0.80\n","Training Epoch: 126 [41472/50000]\tLoss: 0.92\n","Training Epoch: 126 [41728/50000]\tLoss: 0.70\n","Training Epoch: 126 [41984/50000]\tLoss: 0.78\n","Training Epoch: 126 [42240/50000]\tLoss: 0.81\n","Training Epoch: 126 [42496/50000]\tLoss: 0.83\n","Training Epoch: 126 [42752/50000]\tLoss: 0.83\n","Training Epoch: 126 [43008/50000]\tLoss: 0.84\n","Training Epoch: 126 [43264/50000]\tLoss: 0.80\n","Training Epoch: 126 [43520/50000]\tLoss: 0.74\n","Training Epoch: 126 [43776/50000]\tLoss: 0.76\n","Training Epoch: 126 [44032/50000]\tLoss: 0.78\n","Training Epoch: 126 [44288/50000]\tLoss: 0.77\n","Training Epoch: 126 [44544/50000]\tLoss: 0.80\n","Training Epoch: 126 [44800/50000]\tLoss: 0.63\n","Training Epoch: 126 [45056/50000]\tLoss: 0.94\n","Training Epoch: 126 [45312/50000]\tLoss: 0.60\n","Training Epoch: 126 [45568/50000]\tLoss: 0.91\n","Training Epoch: 126 [45824/50000]\tLoss: 0.65\n","Training Epoch: 126 [46080/50000]\tLoss: 0.75\n","Training Epoch: 126 [46336/50000]\tLoss: 0.99\n","Training Epoch: 126 [46592/50000]\tLoss: 0.74\n","Training Epoch: 126 [46848/50000]\tLoss: 0.82\n","Training Epoch: 126 [47104/50000]\tLoss: 0.76\n","Training Epoch: 126 [47360/50000]\tLoss: 0.69\n","Training Epoch: 126 [47616/50000]\tLoss: 0.77\n","Training Epoch: 126 [47872/50000]\tLoss: 0.72\n","Training Epoch: 126 [48128/50000]\tLoss: 0.70\n","Training Epoch: 126 [48384/50000]\tLoss: 0.80\n","Training Epoch: 126 [48640/50000]\tLoss: 0.75\n","Training Epoch: 126 [48896/50000]\tLoss: 0.73\n","Training Epoch: 126 [49152/50000]\tLoss: 0.74\n","Training Epoch: 126 [49408/50000]\tLoss: 0.81\n","Training Epoch: 126 [49664/50000]\tLoss: 0.70\n","Training Epoch: 126 [49920/50000]\tLoss: 0.79\n","Training Epoch: 126 [50000/50000]\tLoss: 0.91\n","Time taken to train epoch 126: 27.21s\n","Testing Network for epoch:  126\n","Evaluation: Evaluation Time: 2.58s, Average loss: 0.0061, Accuracy: 0.6213, Recall: 0.6213, Precision: 0.6244\n","Training Epoch: 127 [256/50000]\tLoss: 0.78\n","Training Epoch: 127 [512/50000]\tLoss: 0.78\n","Training Epoch: 127 [768/50000]\tLoss: 0.71\n","Training Epoch: 127 [1024/50000]\tLoss: 0.76\n","Training Epoch: 127 [1280/50000]\tLoss: 0.76\n","Training Epoch: 127 [1536/50000]\tLoss: 0.77\n","Training Epoch: 127 [1792/50000]\tLoss: 0.68\n","Training Epoch: 127 [2048/50000]\tLoss: 0.68\n","Training Epoch: 127 [2304/50000]\tLoss: 0.63\n","Training Epoch: 127 [2560/50000]\tLoss: 0.70\n","Training Epoch: 127 [2816/50000]\tLoss: 0.86\n","Training Epoch: 127 [3072/50000]\tLoss: 0.78\n","Training Epoch: 127 [3328/50000]\tLoss: 0.81\n","Training Epoch: 127 [3584/50000]\tLoss: 0.69\n","Training Epoch: 127 [3840/50000]\tLoss: 0.78\n","Training Epoch: 127 [4096/50000]\tLoss: 0.89\n","Training Epoch: 127 [4352/50000]\tLoss: 0.87\n","Training Epoch: 127 [4608/50000]\tLoss: 0.77\n","Training Epoch: 127 [4864/50000]\tLoss: 0.82\n","Training Epoch: 127 [5120/50000]\tLoss: 0.79\n","Training Epoch: 127 [5376/50000]\tLoss: 0.79\n","Training Epoch: 127 [5632/50000]\tLoss: 0.76\n","Training Epoch: 127 [5888/50000]\tLoss: 0.78\n","Training Epoch: 127 [6144/50000]\tLoss: 0.71\n","Training Epoch: 127 [6400/50000]\tLoss: 0.82\n","Training Epoch: 127 [6656/50000]\tLoss: 0.74\n","Training Epoch: 127 [6912/50000]\tLoss: 0.83\n","Training Epoch: 127 [7168/50000]\tLoss: 0.76\n","Training Epoch: 127 [7424/50000]\tLoss: 0.62\n","Training Epoch: 127 [7680/50000]\tLoss: 0.68\n","Training Epoch: 127 [7936/50000]\tLoss: 0.67\n","Training Epoch: 127 [8192/50000]\tLoss: 0.66\n","Training Epoch: 127 [8448/50000]\tLoss: 0.77\n","Training Epoch: 127 [8704/50000]\tLoss: 0.88\n","Training Epoch: 127 [8960/50000]\tLoss: 0.88\n","Training Epoch: 127 [9216/50000]\tLoss: 0.82\n","Training Epoch: 127 [9472/50000]\tLoss: 0.70\n","Training Epoch: 127 [9728/50000]\tLoss: 0.79\n","Training Epoch: 127 [9984/50000]\tLoss: 0.80\n","Training Epoch: 127 [10240/50000]\tLoss: 0.81\n","Training Epoch: 127 [10496/50000]\tLoss: 0.77\n","Training Epoch: 127 [10752/50000]\tLoss: 0.70\n","Training Epoch: 127 [11008/50000]\tLoss: 0.88\n","Training Epoch: 127 [11264/50000]\tLoss: 0.94\n","Training Epoch: 127 [11520/50000]\tLoss: 0.67\n","Training Epoch: 127 [11776/50000]\tLoss: 0.76\n","Training Epoch: 127 [12032/50000]\tLoss: 0.70\n","Training Epoch: 127 [12288/50000]\tLoss: 0.74\n","Training Epoch: 127 [12544/50000]\tLoss: 0.72\n","Training Epoch: 127 [12800/50000]\tLoss: 0.83\n","Training Epoch: 127 [13056/50000]\tLoss: 0.86\n","Training Epoch: 127 [13312/50000]\tLoss: 0.70\n","Training Epoch: 127 [13568/50000]\tLoss: 0.93\n","Training Epoch: 127 [13824/50000]\tLoss: 0.79\n","Training Epoch: 127 [14080/50000]\tLoss: 0.63\n","Training Epoch: 127 [14336/50000]\tLoss: 0.73\n","Training Epoch: 127 [14592/50000]\tLoss: 0.95\n","Training Epoch: 127 [14848/50000]\tLoss: 0.77\n","Training Epoch: 127 [15104/50000]\tLoss: 0.73\n","Training Epoch: 127 [15360/50000]\tLoss: 0.74\n","Training Epoch: 127 [15616/50000]\tLoss: 0.69\n","Training Epoch: 127 [15872/50000]\tLoss: 0.81\n","Training Epoch: 127 [16128/50000]\tLoss: 0.78\n","Training Epoch: 127 [16384/50000]\tLoss: 0.68\n","Training Epoch: 127 [16640/50000]\tLoss: 1.06\n","Training Epoch: 127 [16896/50000]\tLoss: 0.84\n","Training Epoch: 127 [17152/50000]\tLoss: 0.77\n","Training Epoch: 127 [17408/50000]\tLoss: 0.71\n","Training Epoch: 127 [17664/50000]\tLoss: 0.83\n","Training Epoch: 127 [17920/50000]\tLoss: 0.79\n","Training Epoch: 127 [18176/50000]\tLoss: 0.72\n","Training Epoch: 127 [18432/50000]\tLoss: 0.82\n","Training Epoch: 127 [18688/50000]\tLoss: 0.78\n","Training Epoch: 127 [18944/50000]\tLoss: 0.75\n","Training Epoch: 127 [19200/50000]\tLoss: 1.02\n","Training Epoch: 127 [19456/50000]\tLoss: 0.82\n","Training Epoch: 127 [19712/50000]\tLoss: 0.89\n","Training Epoch: 127 [19968/50000]\tLoss: 0.70\n","Training Epoch: 127 [20224/50000]\tLoss: 0.88\n","Training Epoch: 127 [20480/50000]\tLoss: 0.80\n","Training Epoch: 127 [20736/50000]\tLoss: 0.69\n","Training Epoch: 127 [20992/50000]\tLoss: 0.75\n","Training Epoch: 127 [21248/50000]\tLoss: 0.70\n","Training Epoch: 127 [21504/50000]\tLoss: 0.75\n","Training Epoch: 127 [21760/50000]\tLoss: 0.70\n","Training Epoch: 127 [22016/50000]\tLoss: 0.86\n","Training Epoch: 127 [22272/50000]\tLoss: 0.76\n","Training Epoch: 127 [22528/50000]\tLoss: 0.77\n","Training Epoch: 127 [22784/50000]\tLoss: 0.76\n","Training Epoch: 127 [23040/50000]\tLoss: 0.91\n","Training Epoch: 127 [23296/50000]\tLoss: 0.67\n","Training Epoch: 127 [23552/50000]\tLoss: 0.84\n","Training Epoch: 127 [23808/50000]\tLoss: 0.73\n","Training Epoch: 127 [24064/50000]\tLoss: 0.67\n","Training Epoch: 127 [24320/50000]\tLoss: 0.71\n","Training Epoch: 127 [24576/50000]\tLoss: 0.72\n","Training Epoch: 127 [24832/50000]\tLoss: 0.92\n","Training Epoch: 127 [25088/50000]\tLoss: 0.77\n","Training Epoch: 127 [25344/50000]\tLoss: 0.81\n","Training Epoch: 127 [25600/50000]\tLoss: 0.92\n","Training Epoch: 127 [25856/50000]\tLoss: 0.96\n","Training Epoch: 127 [26112/50000]\tLoss: 0.90\n","Training Epoch: 127 [26368/50000]\tLoss: 0.74\n","Training Epoch: 127 [26624/50000]\tLoss: 0.71\n","Training Epoch: 127 [26880/50000]\tLoss: 0.84\n","Training Epoch: 127 [27136/50000]\tLoss: 0.77\n","Training Epoch: 127 [27392/50000]\tLoss: 0.86\n","Training Epoch: 127 [27648/50000]\tLoss: 0.72\n","Training Epoch: 127 [27904/50000]\tLoss: 0.80\n","Training Epoch: 127 [28160/50000]\tLoss: 0.72\n","Training Epoch: 127 [28416/50000]\tLoss: 0.76\n","Training Epoch: 127 [28672/50000]\tLoss: 0.74\n","Training Epoch: 127 [28928/50000]\tLoss: 0.71\n","Training Epoch: 127 [29184/50000]\tLoss: 0.81\n","Training Epoch: 127 [29440/50000]\tLoss: 0.78\n","Training Epoch: 127 [29696/50000]\tLoss: 0.76\n","Training Epoch: 127 [29952/50000]\tLoss: 0.87\n","Training Epoch: 127 [30208/50000]\tLoss: 0.96\n","Training Epoch: 127 [30464/50000]\tLoss: 0.78\n","Training Epoch: 127 [30720/50000]\tLoss: 0.79\n","Training Epoch: 127 [30976/50000]\tLoss: 0.92\n","Training Epoch: 127 [31232/50000]\tLoss: 0.73\n","Training Epoch: 127 [31488/50000]\tLoss: 0.78\n","Training Epoch: 127 [31744/50000]\tLoss: 0.71\n","Training Epoch: 127 [32000/50000]\tLoss: 0.81\n","Training Epoch: 127 [32256/50000]\tLoss: 0.69\n","Training Epoch: 127 [32512/50000]\tLoss: 0.80\n","Training Epoch: 127 [32768/50000]\tLoss: 0.76\n","Training Epoch: 127 [33024/50000]\tLoss: 0.80\n","Training Epoch: 127 [33280/50000]\tLoss: 0.81\n","Training Epoch: 127 [33536/50000]\tLoss: 0.85\n","Training Epoch: 127 [33792/50000]\tLoss: 0.82\n","Training Epoch: 127 [34048/50000]\tLoss: 0.73\n","Training Epoch: 127 [34304/50000]\tLoss: 0.78\n","Training Epoch: 127 [34560/50000]\tLoss: 0.82\n","Training Epoch: 127 [34816/50000]\tLoss: 0.72\n","Training Epoch: 127 [35072/50000]\tLoss: 0.83\n","Training Epoch: 127 [35328/50000]\tLoss: 0.82\n","Training Epoch: 127 [35584/50000]\tLoss: 0.79\n","Training Epoch: 127 [35840/50000]\tLoss: 0.77\n","Training Epoch: 127 [36096/50000]\tLoss: 0.79\n","Training Epoch: 127 [36352/50000]\tLoss: 0.73\n","Training Epoch: 127 [36608/50000]\tLoss: 0.81\n","Training Epoch: 127 [36864/50000]\tLoss: 0.75\n","Training Epoch: 127 [37120/50000]\tLoss: 0.82\n","Training Epoch: 127 [37376/50000]\tLoss: 0.84\n","Training Epoch: 127 [37632/50000]\tLoss: 0.70\n","Training Epoch: 127 [37888/50000]\tLoss: 0.79\n","Training Epoch: 127 [38144/50000]\tLoss: 0.66\n","Training Epoch: 127 [38400/50000]\tLoss: 0.83\n","Training Epoch: 127 [38656/50000]\tLoss: 0.68\n","Training Epoch: 127 [38912/50000]\tLoss: 0.73\n","Training Epoch: 127 [39168/50000]\tLoss: 0.92\n","Training Epoch: 127 [39424/50000]\tLoss: 0.70\n","Training Epoch: 127 [39680/50000]\tLoss: 0.92\n","Training Epoch: 127 [39936/50000]\tLoss: 0.75\n","Training Epoch: 127 [40192/50000]\tLoss: 0.77\n","Training Epoch: 127 [40448/50000]\tLoss: 0.68\n","Training Epoch: 127 [40704/50000]\tLoss: 0.90\n","Training Epoch: 127 [40960/50000]\tLoss: 0.81\n","Training Epoch: 127 [41216/50000]\tLoss: 0.71\n","Training Epoch: 127 [41472/50000]\tLoss: 0.81\n","Training Epoch: 127 [41728/50000]\tLoss: 0.70\n","Training Epoch: 127 [41984/50000]\tLoss: 1.08\n","Training Epoch: 127 [42240/50000]\tLoss: 0.78\n","Training Epoch: 127 [42496/50000]\tLoss: 0.87\n","Training Epoch: 127 [42752/50000]\tLoss: 0.81\n","Training Epoch: 127 [43008/50000]\tLoss: 0.72\n","Training Epoch: 127 [43264/50000]\tLoss: 0.71\n","Training Epoch: 127 [43520/50000]\tLoss: 0.71\n","Training Epoch: 127 [43776/50000]\tLoss: 0.82\n","Training Epoch: 127 [44032/50000]\tLoss: 0.76\n","Training Epoch: 127 [44288/50000]\tLoss: 0.83\n","Training Epoch: 127 [44544/50000]\tLoss: 0.81\n","Training Epoch: 127 [44800/50000]\tLoss: 0.84\n","Training Epoch: 127 [45056/50000]\tLoss: 0.93\n","Training Epoch: 127 [45312/50000]\tLoss: 0.76\n","Training Epoch: 127 [45568/50000]\tLoss: 0.75\n","Training Epoch: 127 [45824/50000]\tLoss: 0.74\n","Training Epoch: 127 [46080/50000]\tLoss: 0.71\n","Training Epoch: 127 [46336/50000]\tLoss: 0.94\n","Training Epoch: 127 [46592/50000]\tLoss: 0.86\n","Training Epoch: 127 [46848/50000]\tLoss: 0.80\n","Training Epoch: 127 [47104/50000]\tLoss: 0.93\n","Training Epoch: 127 [47360/50000]\tLoss: 0.74\n","Training Epoch: 127 [47616/50000]\tLoss: 0.82\n","Training Epoch: 127 [47872/50000]\tLoss: 0.77\n","Training Epoch: 127 [48128/50000]\tLoss: 0.79\n","Training Epoch: 127 [48384/50000]\tLoss: 0.80\n","Training Epoch: 127 [48640/50000]\tLoss: 0.81\n","Training Epoch: 127 [48896/50000]\tLoss: 0.84\n","Training Epoch: 127 [49152/50000]\tLoss: 0.93\n","Training Epoch: 127 [49408/50000]\tLoss: 0.76\n","Training Epoch: 127 [49664/50000]\tLoss: 0.86\n","Training Epoch: 127 [49920/50000]\tLoss: 0.76\n","Training Epoch: 127 [50000/50000]\tLoss: 0.95\n","Time taken to train epoch 127: 27.40s\n","Testing Network for epoch:  127\n","Evaluation: Evaluation Time: 2.55s, Average loss: 0.0062, Accuracy: 0.6229, Recall: 0.6229, Precision: 0.6281\n","Training Epoch: 128 [256/50000]\tLoss: 0.71\n","Training Epoch: 128 [512/50000]\tLoss: 0.88\n","Training Epoch: 128 [768/50000]\tLoss: 0.74\n","Training Epoch: 128 [1024/50000]\tLoss: 0.77\n","Training Epoch: 128 [1280/50000]\tLoss: 0.84\n","Training Epoch: 128 [1536/50000]\tLoss: 0.92\n","Training Epoch: 128 [1792/50000]\tLoss: 0.75\n","Training Epoch: 128 [2048/50000]\tLoss: 0.71\n","Training Epoch: 128 [2304/50000]\tLoss: 0.76\n","Training Epoch: 128 [2560/50000]\tLoss: 0.72\n","Training Epoch: 128 [2816/50000]\tLoss: 0.83\n","Training Epoch: 128 [3072/50000]\tLoss: 0.77\n","Training Epoch: 128 [3328/50000]\tLoss: 0.74\n","Training Epoch: 128 [3584/50000]\tLoss: 0.77\n","Training Epoch: 128 [3840/50000]\tLoss: 0.79\n","Training Epoch: 128 [4096/50000]\tLoss: 0.65\n","Training Epoch: 128 [4352/50000]\tLoss: 0.69\n","Training Epoch: 128 [4608/50000]\tLoss: 0.83\n","Training Epoch: 128 [4864/50000]\tLoss: 0.84\n","Training Epoch: 128 [5120/50000]\tLoss: 0.76\n","Training Epoch: 128 [5376/50000]\tLoss: 0.80\n","Training Epoch: 128 [5632/50000]\tLoss: 0.79\n","Training Epoch: 128 [5888/50000]\tLoss: 0.75\n","Training Epoch: 128 [6144/50000]\tLoss: 0.80\n","Training Epoch: 128 [6400/50000]\tLoss: 0.92\n","Training Epoch: 128 [6656/50000]\tLoss: 0.61\n","Training Epoch: 128 [6912/50000]\tLoss: 0.81\n","Training Epoch: 128 [7168/50000]\tLoss: 0.73\n","Training Epoch: 128 [7424/50000]\tLoss: 0.80\n","Training Epoch: 128 [7680/50000]\tLoss: 0.75\n","Training Epoch: 128 [7936/50000]\tLoss: 0.71\n","Training Epoch: 128 [8192/50000]\tLoss: 0.79\n","Training Epoch: 128 [8448/50000]\tLoss: 0.86\n","Training Epoch: 128 [8704/50000]\tLoss: 0.68\n","Training Epoch: 128 [8960/50000]\tLoss: 0.90\n","Training Epoch: 128 [9216/50000]\tLoss: 0.70\n","Training Epoch: 128 [9472/50000]\tLoss: 0.74\n","Training Epoch: 128 [9728/50000]\tLoss: 0.83\n","Training Epoch: 128 [9984/50000]\tLoss: 0.84\n","Training Epoch: 128 [10240/50000]\tLoss: 0.92\n","Training Epoch: 128 [10496/50000]\tLoss: 0.68\n","Training Epoch: 128 [10752/50000]\tLoss: 0.80\n","Training Epoch: 128 [11008/50000]\tLoss: 0.72\n","Training Epoch: 128 [11264/50000]\tLoss: 0.68\n","Training Epoch: 128 [11520/50000]\tLoss: 0.83\n","Training Epoch: 128 [11776/50000]\tLoss: 0.70\n","Training Epoch: 128 [12032/50000]\tLoss: 0.85\n","Training Epoch: 128 [12288/50000]\tLoss: 0.82\n","Training Epoch: 128 [12544/50000]\tLoss: 0.80\n","Training Epoch: 128 [12800/50000]\tLoss: 0.77\n","Training Epoch: 128 [13056/50000]\tLoss: 0.78\n","Training Epoch: 128 [13312/50000]\tLoss: 0.87\n","Training Epoch: 128 [13568/50000]\tLoss: 0.79\n","Training Epoch: 128 [13824/50000]\tLoss: 0.73\n","Training Epoch: 128 [14080/50000]\tLoss: 0.80\n","Training Epoch: 128 [14336/50000]\tLoss: 0.87\n","Training Epoch: 128 [14592/50000]\tLoss: 0.83\n","Training Epoch: 128 [14848/50000]\tLoss: 0.75\n","Training Epoch: 128 [15104/50000]\tLoss: 0.77\n","Training Epoch: 128 [15360/50000]\tLoss: 0.88\n","Training Epoch: 128 [15616/50000]\tLoss: 0.75\n","Training Epoch: 128 [15872/50000]\tLoss: 0.83\n","Training Epoch: 128 [16128/50000]\tLoss: 0.70\n","Training Epoch: 128 [16384/50000]\tLoss: 0.94\n","Training Epoch: 128 [16640/50000]\tLoss: 0.87\n","Training Epoch: 128 [16896/50000]\tLoss: 0.82\n","Training Epoch: 128 [17152/50000]\tLoss: 0.70\n","Training Epoch: 128 [17408/50000]\tLoss: 0.77\n","Training Epoch: 128 [17664/50000]\tLoss: 0.81\n","Training Epoch: 128 [17920/50000]\tLoss: 0.76\n","Training Epoch: 128 [18176/50000]\tLoss: 0.79\n","Training Epoch: 128 [18432/50000]\tLoss: 0.64\n","Training Epoch: 128 [18688/50000]\tLoss: 0.73\n","Training Epoch: 128 [18944/50000]\tLoss: 0.68\n","Training Epoch: 128 [19200/50000]\tLoss: 0.77\n","Training Epoch: 128 [19456/50000]\tLoss: 0.71\n","Training Epoch: 128 [19712/50000]\tLoss: 0.74\n","Training Epoch: 128 [19968/50000]\tLoss: 0.85\n","Training Epoch: 128 [20224/50000]\tLoss: 0.69\n","Training Epoch: 128 [20480/50000]\tLoss: 0.81\n","Training Epoch: 128 [20736/50000]\tLoss: 0.62\n","Training Epoch: 128 [20992/50000]\tLoss: 0.96\n","Training Epoch: 128 [21248/50000]\tLoss: 0.83\n","Training Epoch: 128 [21504/50000]\tLoss: 0.77\n","Training Epoch: 128 [21760/50000]\tLoss: 0.74\n","Training Epoch: 128 [22016/50000]\tLoss: 0.62\n","Training Epoch: 128 [22272/50000]\tLoss: 0.72\n","Training Epoch: 128 [22528/50000]\tLoss: 0.72\n","Training Epoch: 128 [22784/50000]\tLoss: 0.78\n","Training Epoch: 128 [23040/50000]\tLoss: 0.77\n","Training Epoch: 128 [23296/50000]\tLoss: 0.69\n","Training Epoch: 128 [23552/50000]\tLoss: 0.86\n","Training Epoch: 128 [23808/50000]\tLoss: 0.78\n","Training Epoch: 128 [24064/50000]\tLoss: 0.72\n","Training Epoch: 128 [24320/50000]\tLoss: 0.71\n","Training Epoch: 128 [24576/50000]\tLoss: 0.74\n","Training Epoch: 128 [24832/50000]\tLoss: 0.76\n","Training Epoch: 128 [25088/50000]\tLoss: 0.82\n","Training Epoch: 128 [25344/50000]\tLoss: 0.81\n","Training Epoch: 128 [25600/50000]\tLoss: 0.79\n","Training Epoch: 128 [25856/50000]\tLoss: 0.83\n","Training Epoch: 128 [26112/50000]\tLoss: 0.73\n","Training Epoch: 128 [26368/50000]\tLoss: 0.70\n","Training Epoch: 128 [26624/50000]\tLoss: 0.85\n","Training Epoch: 128 [26880/50000]\tLoss: 0.76\n","Training Epoch: 128 [27136/50000]\tLoss: 0.88\n","Training Epoch: 128 [27392/50000]\tLoss: 0.70\n","Training Epoch: 128 [27648/50000]\tLoss: 0.75\n","Training Epoch: 128 [27904/50000]\tLoss: 0.85\n","Training Epoch: 128 [28160/50000]\tLoss: 0.77\n","Training Epoch: 128 [28416/50000]\tLoss: 0.91\n","Training Epoch: 128 [28672/50000]\tLoss: 0.76\n","Training Epoch: 128 [28928/50000]\tLoss: 0.76\n","Training Epoch: 128 [29184/50000]\tLoss: 0.73\n","Training Epoch: 128 [29440/50000]\tLoss: 0.76\n","Training Epoch: 128 [29696/50000]\tLoss: 0.66\n","Training Epoch: 128 [29952/50000]\tLoss: 0.93\n","Training Epoch: 128 [30208/50000]\tLoss: 0.82\n","Training Epoch: 128 [30464/50000]\tLoss: 0.86\n","Training Epoch: 128 [30720/50000]\tLoss: 0.83\n","Training Epoch: 128 [30976/50000]\tLoss: 0.69\n","Training Epoch: 128 [31232/50000]\tLoss: 0.79\n","Training Epoch: 128 [31488/50000]\tLoss: 0.82\n","Training Epoch: 128 [31744/50000]\tLoss: 0.72\n","Training Epoch: 128 [32000/50000]\tLoss: 0.68\n","Training Epoch: 128 [32256/50000]\tLoss: 0.89\n","Training Epoch: 128 [32512/50000]\tLoss: 0.89\n","Training Epoch: 128 [32768/50000]\tLoss: 0.79\n","Training Epoch: 128 [33024/50000]\tLoss: 0.70\n","Training Epoch: 128 [33280/50000]\tLoss: 0.65\n","Training Epoch: 128 [33536/50000]\tLoss: 0.78\n","Training Epoch: 128 [33792/50000]\tLoss: 0.76\n","Training Epoch: 128 [34048/50000]\tLoss: 0.79\n","Training Epoch: 128 [34304/50000]\tLoss: 0.82\n","Training Epoch: 128 [34560/50000]\tLoss: 0.90\n","Training Epoch: 128 [34816/50000]\tLoss: 0.87\n","Training Epoch: 128 [35072/50000]\tLoss: 0.65\n","Training Epoch: 128 [35328/50000]\tLoss: 0.85\n","Training Epoch: 128 [35584/50000]\tLoss: 0.90\n","Training Epoch: 128 [35840/50000]\tLoss: 0.70\n","Training Epoch: 128 [36096/50000]\tLoss: 0.80\n","Training Epoch: 128 [36352/50000]\tLoss: 0.86\n","Training Epoch: 128 [36608/50000]\tLoss: 0.73\n","Training Epoch: 128 [36864/50000]\tLoss: 0.77\n","Training Epoch: 128 [37120/50000]\tLoss: 0.73\n","Training Epoch: 128 [37376/50000]\tLoss: 0.74\n","Training Epoch: 128 [37632/50000]\tLoss: 0.85\n","Training Epoch: 128 [37888/50000]\tLoss: 0.67\n","Training Epoch: 128 [38144/50000]\tLoss: 0.81\n","Training Epoch: 128 [38400/50000]\tLoss: 0.89\n","Training Epoch: 128 [38656/50000]\tLoss: 0.89\n","Training Epoch: 128 [38912/50000]\tLoss: 0.72\n","Training Epoch: 128 [39168/50000]\tLoss: 0.79\n","Training Epoch: 128 [39424/50000]\tLoss: 0.96\n","Training Epoch: 128 [39680/50000]\tLoss: 0.77\n","Training Epoch: 128 [39936/50000]\tLoss: 0.78\n","Training Epoch: 128 [40192/50000]\tLoss: 0.96\n","Training Epoch: 128 [40448/50000]\tLoss: 0.75\n","Training Epoch: 128 [40704/50000]\tLoss: 0.82\n","Training Epoch: 128 [40960/50000]\tLoss: 0.75\n","Training Epoch: 128 [41216/50000]\tLoss: 0.76\n","Training Epoch: 128 [41472/50000]\tLoss: 0.82\n","Training Epoch: 128 [41728/50000]\tLoss: 0.77\n","Training Epoch: 128 [41984/50000]\tLoss: 0.78\n","Training Epoch: 128 [42240/50000]\tLoss: 0.75\n","Training Epoch: 128 [42496/50000]\tLoss: 0.86\n","Training Epoch: 128 [42752/50000]\tLoss: 0.88\n","Training Epoch: 128 [43008/50000]\tLoss: 0.75\n","Training Epoch: 128 [43264/50000]\tLoss: 0.91\n","Training Epoch: 128 [43520/50000]\tLoss: 0.75\n","Training Epoch: 128 [43776/50000]\tLoss: 0.69\n","Training Epoch: 128 [44032/50000]\tLoss: 0.78\n","Training Epoch: 128 [44288/50000]\tLoss: 0.77\n","Training Epoch: 128 [44544/50000]\tLoss: 0.75\n","Training Epoch: 128 [44800/50000]\tLoss: 0.71\n","Training Epoch: 128 [45056/50000]\tLoss: 0.84\n","Training Epoch: 128 [45312/50000]\tLoss: 0.87\n","Training Epoch: 128 [45568/50000]\tLoss: 0.67\n","Training Epoch: 128 [45824/50000]\tLoss: 0.96\n","Training Epoch: 128 [46080/50000]\tLoss: 0.81\n","Training Epoch: 128 [46336/50000]\tLoss: 0.74\n","Training Epoch: 128 [46592/50000]\tLoss: 0.82\n","Training Epoch: 128 [46848/50000]\tLoss: 0.85\n","Training Epoch: 128 [47104/50000]\tLoss: 0.70\n","Training Epoch: 128 [47360/50000]\tLoss: 0.80\n","Training Epoch: 128 [47616/50000]\tLoss: 0.85\n","Training Epoch: 128 [47872/50000]\tLoss: 0.82\n","Training Epoch: 128 [48128/50000]\tLoss: 0.83\n","Training Epoch: 128 [48384/50000]\tLoss: 0.85\n","Training Epoch: 128 [48640/50000]\tLoss: 0.63\n","Training Epoch: 128 [48896/50000]\tLoss: 0.92\n","Training Epoch: 128 [49152/50000]\tLoss: 0.69\n","Training Epoch: 128 [49408/50000]\tLoss: 0.79\n","Training Epoch: 128 [49664/50000]\tLoss: 0.77\n","Training Epoch: 128 [49920/50000]\tLoss: 0.76\n","Training Epoch: 128 [50000/50000]\tLoss: 0.82\n","Time taken to train epoch 128: 27.16s\n","Testing Network for epoch:  128\n","Evaluation: Evaluation Time: 2.55s, Average loss: 0.0061, Accuracy: 0.6258, Recall: 0.6258, Precision: 0.6287\n","Training Epoch: 129 [256/50000]\tLoss: 0.67\n","Training Epoch: 129 [512/50000]\tLoss: 0.74\n","Training Epoch: 129 [768/50000]\tLoss: 0.84\n","Training Epoch: 129 [1024/50000]\tLoss: 0.78\n","Training Epoch: 129 [1280/50000]\tLoss: 0.74\n","Training Epoch: 129 [1536/50000]\tLoss: 0.74\n","Training Epoch: 129 [1792/50000]\tLoss: 0.85\n","Training Epoch: 129 [2048/50000]\tLoss: 0.96\n","Training Epoch: 129 [2304/50000]\tLoss: 0.80\n","Training Epoch: 129 [2560/50000]\tLoss: 0.81\n","Training Epoch: 129 [2816/50000]\tLoss: 0.72\n","Training Epoch: 129 [3072/50000]\tLoss: 0.80\n","Training Epoch: 129 [3328/50000]\tLoss: 0.74\n","Training Epoch: 129 [3584/50000]\tLoss: 0.81\n","Training Epoch: 129 [3840/50000]\tLoss: 0.70\n","Training Epoch: 129 [4096/50000]\tLoss: 0.80\n","Training Epoch: 129 [4352/50000]\tLoss: 0.95\n","Training Epoch: 129 [4608/50000]\tLoss: 0.83\n","Training Epoch: 129 [4864/50000]\tLoss: 0.85\n","Training Epoch: 129 [5120/50000]\tLoss: 0.88\n","Training Epoch: 129 [5376/50000]\tLoss: 0.79\n","Training Epoch: 129 [5632/50000]\tLoss: 0.78\n","Training Epoch: 129 [5888/50000]\tLoss: 0.79\n","Training Epoch: 129 [6144/50000]\tLoss: 0.66\n","Training Epoch: 129 [6400/50000]\tLoss: 0.83\n","Training Epoch: 129 [6656/50000]\tLoss: 0.78\n","Training Epoch: 129 [6912/50000]\tLoss: 0.76\n","Training Epoch: 129 [7168/50000]\tLoss: 0.77\n","Training Epoch: 129 [7424/50000]\tLoss: 0.77\n","Training Epoch: 129 [7680/50000]\tLoss: 0.85\n","Training Epoch: 129 [7936/50000]\tLoss: 0.68\n","Training Epoch: 129 [8192/50000]\tLoss: 0.80\n","Training Epoch: 129 [8448/50000]\tLoss: 0.78\n","Training Epoch: 129 [8704/50000]\tLoss: 0.75\n","Training Epoch: 129 [8960/50000]\tLoss: 0.75\n","Training Epoch: 129 [9216/50000]\tLoss: 0.67\n","Training Epoch: 129 [9472/50000]\tLoss: 0.73\n","Training Epoch: 129 [9728/50000]\tLoss: 0.77\n","Training Epoch: 129 [9984/50000]\tLoss: 0.78\n","Training Epoch: 129 [10240/50000]\tLoss: 0.84\n","Training Epoch: 129 [10496/50000]\tLoss: 0.82\n","Training Epoch: 129 [10752/50000]\tLoss: 0.85\n","Training Epoch: 129 [11008/50000]\tLoss: 0.81\n","Training Epoch: 129 [11264/50000]\tLoss: 0.81\n","Training Epoch: 129 [11520/50000]\tLoss: 0.70\n","Training Epoch: 129 [11776/50000]\tLoss: 0.69\n","Training Epoch: 129 [12032/50000]\tLoss: 0.83\n","Training Epoch: 129 [12288/50000]\tLoss: 0.86\n","Training Epoch: 129 [12544/50000]\tLoss: 0.81\n","Training Epoch: 129 [12800/50000]\tLoss: 0.74\n","Training Epoch: 129 [13056/50000]\tLoss: 0.68\n","Training Epoch: 129 [13312/50000]\tLoss: 0.75\n","Training Epoch: 129 [13568/50000]\tLoss: 0.82\n","Training Epoch: 129 [13824/50000]\tLoss: 0.74\n","Training Epoch: 129 [14080/50000]\tLoss: 0.86\n","Training Epoch: 129 [14336/50000]\tLoss: 0.85\n","Training Epoch: 129 [14592/50000]\tLoss: 0.78\n","Training Epoch: 129 [14848/50000]\tLoss: 0.74\n","Training Epoch: 129 [15104/50000]\tLoss: 0.73\n","Training Epoch: 129 [15360/50000]\tLoss: 0.79\n","Training Epoch: 129 [15616/50000]\tLoss: 0.85\n","Training Epoch: 129 [15872/50000]\tLoss: 0.78\n","Training Epoch: 129 [16128/50000]\tLoss: 0.93\n","Training Epoch: 129 [16384/50000]\tLoss: 0.74\n","Training Epoch: 129 [16640/50000]\tLoss: 0.84\n","Training Epoch: 129 [16896/50000]\tLoss: 0.80\n","Training Epoch: 129 [17152/50000]\tLoss: 0.68\n","Training Epoch: 129 [17408/50000]\tLoss: 0.75\n","Training Epoch: 129 [17664/50000]\tLoss: 0.63\n","Training Epoch: 129 [17920/50000]\tLoss: 0.79\n","Training Epoch: 129 [18176/50000]\tLoss: 0.71\n","Training Epoch: 129 [18432/50000]\tLoss: 0.88\n","Training Epoch: 129 [18688/50000]\tLoss: 0.76\n","Training Epoch: 129 [18944/50000]\tLoss: 0.76\n","Training Epoch: 129 [19200/50000]\tLoss: 0.78\n","Training Epoch: 129 [19456/50000]\tLoss: 1.04\n","Training Epoch: 129 [19712/50000]\tLoss: 0.87\n","Training Epoch: 129 [19968/50000]\tLoss: 0.90\n","Training Epoch: 129 [20224/50000]\tLoss: 0.72\n","Training Epoch: 129 [20480/50000]\tLoss: 0.76\n","Training Epoch: 129 [20736/50000]\tLoss: 0.83\n","Training Epoch: 129 [20992/50000]\tLoss: 0.74\n","Training Epoch: 129 [21248/50000]\tLoss: 0.65\n","Training Epoch: 129 [21504/50000]\tLoss: 0.80\n","Training Epoch: 129 [21760/50000]\tLoss: 0.82\n","Training Epoch: 129 [22016/50000]\tLoss: 0.92\n","Training Epoch: 129 [22272/50000]\tLoss: 0.83\n","Training Epoch: 129 [22528/50000]\tLoss: 0.71\n","Training Epoch: 129 [22784/50000]\tLoss: 0.73\n","Training Epoch: 129 [23040/50000]\tLoss: 0.77\n","Training Epoch: 129 [23296/50000]\tLoss: 0.66\n","Training Epoch: 129 [23552/50000]\tLoss: 0.74\n","Training Epoch: 129 [23808/50000]\tLoss: 0.87\n","Training Epoch: 129 [24064/50000]\tLoss: 0.92\n","Training Epoch: 129 [24320/50000]\tLoss: 0.72\n","Training Epoch: 129 [24576/50000]\tLoss: 0.71\n","Training Epoch: 129 [24832/50000]\tLoss: 0.89\n","Training Epoch: 129 [25088/50000]\tLoss: 0.99\n","Training Epoch: 129 [25344/50000]\tLoss: 0.84\n","Training Epoch: 129 [25600/50000]\tLoss: 0.81\n","Training Epoch: 129 [25856/50000]\tLoss: 0.88\n","Training Epoch: 129 [26112/50000]\tLoss: 0.85\n","Training Epoch: 129 [26368/50000]\tLoss: 0.61\n","Training Epoch: 129 [26624/50000]\tLoss: 0.80\n","Training Epoch: 129 [26880/50000]\tLoss: 0.79\n","Training Epoch: 129 [27136/50000]\tLoss: 0.97\n","Training Epoch: 129 [27392/50000]\tLoss: 0.77\n","Training Epoch: 129 [27648/50000]\tLoss: 0.84\n","Training Epoch: 129 [27904/50000]\tLoss: 0.87\n","Training Epoch: 129 [28160/50000]\tLoss: 0.78\n","Training Epoch: 129 [28416/50000]\tLoss: 0.76\n","Training Epoch: 129 [28672/50000]\tLoss: 0.95\n","Training Epoch: 129 [28928/50000]\tLoss: 0.75\n","Training Epoch: 129 [29184/50000]\tLoss: 0.77\n","Training Epoch: 129 [29440/50000]\tLoss: 0.77\n","Training Epoch: 129 [29696/50000]\tLoss: 0.76\n","Training Epoch: 129 [29952/50000]\tLoss: 0.88\n","Training Epoch: 129 [30208/50000]\tLoss: 0.62\n","Training Epoch: 129 [30464/50000]\tLoss: 0.78\n","Training Epoch: 129 [30720/50000]\tLoss: 0.68\n","Training Epoch: 129 [30976/50000]\tLoss: 0.88\n","Training Epoch: 129 [31232/50000]\tLoss: 0.97\n","Training Epoch: 129 [31488/50000]\tLoss: 0.72\n","Training Epoch: 129 [31744/50000]\tLoss: 0.76\n","Training Epoch: 129 [32000/50000]\tLoss: 0.82\n","Training Epoch: 129 [32256/50000]\tLoss: 0.75\n","Training Epoch: 129 [32512/50000]\tLoss: 0.65\n","Training Epoch: 129 [32768/50000]\tLoss: 0.77\n","Training Epoch: 129 [33024/50000]\tLoss: 0.82\n","Training Epoch: 129 [33280/50000]\tLoss: 0.77\n","Training Epoch: 129 [33536/50000]\tLoss: 0.78\n","Training Epoch: 129 [33792/50000]\tLoss: 0.93\n","Training Epoch: 129 [34048/50000]\tLoss: 0.67\n","Training Epoch: 129 [34304/50000]\tLoss: 0.85\n","Training Epoch: 129 [34560/50000]\tLoss: 0.70\n","Training Epoch: 129 [34816/50000]\tLoss: 0.74\n","Training Epoch: 129 [35072/50000]\tLoss: 0.74\n","Training Epoch: 129 [35328/50000]\tLoss: 0.69\n","Training Epoch: 129 [35584/50000]\tLoss: 0.75\n","Training Epoch: 129 [35840/50000]\tLoss: 0.85\n","Training Epoch: 129 [36096/50000]\tLoss: 0.72\n","Training Epoch: 129 [36352/50000]\tLoss: 0.80\n","Training Epoch: 129 [36608/50000]\tLoss: 0.73\n","Training Epoch: 129 [36864/50000]\tLoss: 0.66\n","Training Epoch: 129 [37120/50000]\tLoss: 0.77\n","Training Epoch: 129 [37376/50000]\tLoss: 0.96\n","Training Epoch: 129 [37632/50000]\tLoss: 0.88\n","Training Epoch: 129 [37888/50000]\tLoss: 1.04\n","Training Epoch: 129 [38144/50000]\tLoss: 0.66\n","Training Epoch: 129 [38400/50000]\tLoss: 0.72\n","Training Epoch: 129 [38656/50000]\tLoss: 0.81\n","Training Epoch: 129 [38912/50000]\tLoss: 0.80\n","Training Epoch: 129 [39168/50000]\tLoss: 0.77\n","Training Epoch: 129 [39424/50000]\tLoss: 0.93\n","Training Epoch: 129 [39680/50000]\tLoss: 0.73\n","Training Epoch: 129 [39936/50000]\tLoss: 0.78\n","Training Epoch: 129 [40192/50000]\tLoss: 0.90\n","Training Epoch: 129 [40448/50000]\tLoss: 0.71\n","Training Epoch: 129 [40704/50000]\tLoss: 0.69\n","Training Epoch: 129 [40960/50000]\tLoss: 0.67\n","Training Epoch: 129 [41216/50000]\tLoss: 0.76\n","Training Epoch: 129 [41472/50000]\tLoss: 0.83\n","Training Epoch: 129 [41728/50000]\tLoss: 0.74\n","Training Epoch: 129 [41984/50000]\tLoss: 0.79\n","Training Epoch: 129 [42240/50000]\tLoss: 0.89\n","Training Epoch: 129 [42496/50000]\tLoss: 0.67\n","Training Epoch: 129 [42752/50000]\tLoss: 0.84\n","Training Epoch: 129 [43008/50000]\tLoss: 0.74\n","Training Epoch: 129 [43264/50000]\tLoss: 0.93\n","Training Epoch: 129 [43520/50000]\tLoss: 0.75\n","Training Epoch: 129 [43776/50000]\tLoss: 0.82\n","Training Epoch: 129 [44032/50000]\tLoss: 0.91\n","Training Epoch: 129 [44288/50000]\tLoss: 0.64\n","Training Epoch: 129 [44544/50000]\tLoss: 0.85\n","Training Epoch: 129 [44800/50000]\tLoss: 0.71\n","Training Epoch: 129 [45056/50000]\tLoss: 0.84\n","Training Epoch: 129 [45312/50000]\tLoss: 0.76\n","Training Epoch: 129 [45568/50000]\tLoss: 0.71\n","Training Epoch: 129 [45824/50000]\tLoss: 0.69\n","Training Epoch: 129 [46080/50000]\tLoss: 0.71\n","Training Epoch: 129 [46336/50000]\tLoss: 0.68\n","Training Epoch: 129 [46592/50000]\tLoss: 0.82\n","Training Epoch: 129 [46848/50000]\tLoss: 0.77\n","Training Epoch: 129 [47104/50000]\tLoss: 0.90\n","Training Epoch: 129 [47360/50000]\tLoss: 0.80\n","Training Epoch: 129 [47616/50000]\tLoss: 0.84\n","Training Epoch: 129 [47872/50000]\tLoss: 0.81\n","Training Epoch: 129 [48128/50000]\tLoss: 0.68\n","Training Epoch: 129 [48384/50000]\tLoss: 0.90\n","Training Epoch: 129 [48640/50000]\tLoss: 0.80\n","Training Epoch: 129 [48896/50000]\tLoss: 0.79\n","Training Epoch: 129 [49152/50000]\tLoss: 0.72\n","Training Epoch: 129 [49408/50000]\tLoss: 0.80\n","Training Epoch: 129 [49664/50000]\tLoss: 0.84\n","Training Epoch: 129 [49920/50000]\tLoss: 0.79\n","Training Epoch: 129 [50000/50000]\tLoss: 0.68\n","Time taken to train epoch 129: 27.30s\n","Testing Network for epoch:  129\n","Evaluation: Evaluation Time: 2.59s, Average loss: 0.0063, Accuracy: 0.6251, Recall: 0.6251, Precision: 0.6293\n","Training Epoch: 130 [256/50000]\tLoss: 0.70\n","Training Epoch: 130 [512/50000]\tLoss: 0.73\n","Training Epoch: 130 [768/50000]\tLoss: 0.60\n","Training Epoch: 130 [1024/50000]\tLoss: 0.69\n","Training Epoch: 130 [1280/50000]\tLoss: 0.77\n","Training Epoch: 130 [1536/50000]\tLoss: 0.57\n","Training Epoch: 130 [1792/50000]\tLoss: 0.76\n","Training Epoch: 130 [2048/50000]\tLoss: 0.80\n","Training Epoch: 130 [2304/50000]\tLoss: 0.66\n","Training Epoch: 130 [2560/50000]\tLoss: 0.58\n","Training Epoch: 130 [2816/50000]\tLoss: 0.78\n","Training Epoch: 130 [3072/50000]\tLoss: 0.80\n","Training Epoch: 130 [3328/50000]\tLoss: 0.82\n","Training Epoch: 130 [3584/50000]\tLoss: 0.79\n","Training Epoch: 130 [3840/50000]\tLoss: 0.77\n","Training Epoch: 130 [4096/50000]\tLoss: 0.90\n","Training Epoch: 130 [4352/50000]\tLoss: 0.83\n","Training Epoch: 130 [4608/50000]\tLoss: 0.83\n","Training Epoch: 130 [4864/50000]\tLoss: 0.86\n","Training Epoch: 130 [5120/50000]\tLoss: 0.68\n","Training Epoch: 130 [5376/50000]\tLoss: 0.85\n","Training Epoch: 130 [5632/50000]\tLoss: 0.72\n","Training Epoch: 130 [5888/50000]\tLoss: 0.75\n","Training Epoch: 130 [6144/50000]\tLoss: 0.74\n","Training Epoch: 130 [6400/50000]\tLoss: 0.70\n","Training Epoch: 130 [6656/50000]\tLoss: 0.79\n","Training Epoch: 130 [6912/50000]\tLoss: 0.72\n","Training Epoch: 130 [7168/50000]\tLoss: 0.88\n","Training Epoch: 130 [7424/50000]\tLoss: 0.93\n","Training Epoch: 130 [7680/50000]\tLoss: 0.67\n","Training Epoch: 130 [7936/50000]\tLoss: 0.83\n","Training Epoch: 130 [8192/50000]\tLoss: 0.91\n","Training Epoch: 130 [8448/50000]\tLoss: 0.77\n","Training Epoch: 130 [8704/50000]\tLoss: 0.76\n","Training Epoch: 130 [8960/50000]\tLoss: 0.81\n","Training Epoch: 130 [9216/50000]\tLoss: 0.71\n","Training Epoch: 130 [9472/50000]\tLoss: 0.79\n","Training Epoch: 130 [9728/50000]\tLoss: 0.84\n","Training Epoch: 130 [9984/50000]\tLoss: 0.70\n","Training Epoch: 130 [10240/50000]\tLoss: 0.83\n","Training Epoch: 130 [10496/50000]\tLoss: 0.71\n","Training Epoch: 130 [10752/50000]\tLoss: 0.83\n","Training Epoch: 130 [11008/50000]\tLoss: 0.85\n","Training Epoch: 130 [11264/50000]\tLoss: 0.71\n","Training Epoch: 130 [11520/50000]\tLoss: 0.79\n","Training Epoch: 130 [11776/50000]\tLoss: 0.77\n","Training Epoch: 130 [12032/50000]\tLoss: 0.84\n","Training Epoch: 130 [12288/50000]\tLoss: 0.84\n","Training Epoch: 130 [12544/50000]\tLoss: 0.73\n","Training Epoch: 130 [12800/50000]\tLoss: 0.70\n","Training Epoch: 130 [13056/50000]\tLoss: 0.77\n","Training Epoch: 130 [13312/50000]\tLoss: 0.82\n","Training Epoch: 130 [13568/50000]\tLoss: 0.75\n","Training Epoch: 130 [13824/50000]\tLoss: 0.87\n","Training Epoch: 130 [14080/50000]\tLoss: 0.74\n","Training Epoch: 130 [14336/50000]\tLoss: 0.74\n","Training Epoch: 130 [14592/50000]\tLoss: 0.62\n","Training Epoch: 130 [14848/50000]\tLoss: 0.68\n","Training Epoch: 130 [15104/50000]\tLoss: 0.74\n","Training Epoch: 130 [15360/50000]\tLoss: 0.74\n","Training Epoch: 130 [15616/50000]\tLoss: 0.78\n","Training Epoch: 130 [15872/50000]\tLoss: 0.77\n","Training Epoch: 130 [16128/50000]\tLoss: 0.80\n","Training Epoch: 130 [16384/50000]\tLoss: 0.83\n","Training Epoch: 130 [16640/50000]\tLoss: 0.74\n","Training Epoch: 130 [16896/50000]\tLoss: 0.92\n","Training Epoch: 130 [17152/50000]\tLoss: 0.81\n","Training Epoch: 130 [17408/50000]\tLoss: 0.84\n","Training Epoch: 130 [17664/50000]\tLoss: 0.82\n","Training Epoch: 130 [17920/50000]\tLoss: 0.82\n","Training Epoch: 130 [18176/50000]\tLoss: 0.72\n","Training Epoch: 130 [18432/50000]\tLoss: 0.83\n","Training Epoch: 130 [18688/50000]\tLoss: 0.82\n","Training Epoch: 130 [18944/50000]\tLoss: 0.77\n","Training Epoch: 130 [19200/50000]\tLoss: 0.87\n","Training Epoch: 130 [19456/50000]\tLoss: 0.70\n","Training Epoch: 130 [19712/50000]\tLoss: 0.86\n","Training Epoch: 130 [19968/50000]\tLoss: 0.74\n","Training Epoch: 130 [20224/50000]\tLoss: 0.75\n","Training Epoch: 130 [20480/50000]\tLoss: 0.79\n","Training Epoch: 130 [20736/50000]\tLoss: 0.81\n","Training Epoch: 130 [20992/50000]\tLoss: 0.79\n","Training Epoch: 130 [21248/50000]\tLoss: 0.76\n","Training Epoch: 130 [21504/50000]\tLoss: 0.72\n","Training Epoch: 130 [21760/50000]\tLoss: 0.75\n","Training Epoch: 130 [22016/50000]\tLoss: 0.85\n","Training Epoch: 130 [22272/50000]\tLoss: 0.71\n","Training Epoch: 130 [22528/50000]\tLoss: 0.71\n","Training Epoch: 130 [22784/50000]\tLoss: 0.74\n","Training Epoch: 130 [23040/50000]\tLoss: 0.76\n","Training Epoch: 130 [23296/50000]\tLoss: 0.82\n","Training Epoch: 130 [23552/50000]\tLoss: 0.75\n","Training Epoch: 130 [23808/50000]\tLoss: 0.70\n","Training Epoch: 130 [24064/50000]\tLoss: 0.64\n","Training Epoch: 130 [24320/50000]\tLoss: 0.80\n","Training Epoch: 130 [24576/50000]\tLoss: 0.77\n","Training Epoch: 130 [24832/50000]\tLoss: 0.85\n","Training Epoch: 130 [25088/50000]\tLoss: 0.80\n","Training Epoch: 130 [25344/50000]\tLoss: 0.80\n","Training Epoch: 130 [25600/50000]\tLoss: 0.83\n","Training Epoch: 130 [25856/50000]\tLoss: 0.88\n","Training Epoch: 130 [26112/50000]\tLoss: 0.68\n","Training Epoch: 130 [26368/50000]\tLoss: 0.70\n","Training Epoch: 130 [26624/50000]\tLoss: 0.78\n","Training Epoch: 130 [26880/50000]\tLoss: 0.75\n","Training Epoch: 130 [27136/50000]\tLoss: 0.84\n","Training Epoch: 130 [27392/50000]\tLoss: 0.78\n","Training Epoch: 130 [27648/50000]\tLoss: 0.95\n","Training Epoch: 130 [27904/50000]\tLoss: 0.80\n","Training Epoch: 130 [28160/50000]\tLoss: 0.85\n","Training Epoch: 130 [28416/50000]\tLoss: 0.70\n","Training Epoch: 130 [28672/50000]\tLoss: 0.74\n","Training Epoch: 130 [28928/50000]\tLoss: 0.75\n","Training Epoch: 130 [29184/50000]\tLoss: 0.77\n","Training Epoch: 130 [29440/50000]\tLoss: 0.83\n","Training Epoch: 130 [29696/50000]\tLoss: 0.76\n","Training Epoch: 130 [29952/50000]\tLoss: 0.81\n","Training Epoch: 130 [30208/50000]\tLoss: 0.84\n","Training Epoch: 130 [30464/50000]\tLoss: 0.82\n","Training Epoch: 130 [30720/50000]\tLoss: 0.75\n","Training Epoch: 130 [30976/50000]\tLoss: 0.96\n","Training Epoch: 130 [31232/50000]\tLoss: 0.82\n","Training Epoch: 130 [31488/50000]\tLoss: 0.72\n","Training Epoch: 130 [31744/50000]\tLoss: 0.82\n","Training Epoch: 130 [32000/50000]\tLoss: 0.78\n","Training Epoch: 130 [32256/50000]\tLoss: 0.86\n","Training Epoch: 130 [32512/50000]\tLoss: 0.64\n","Training Epoch: 130 [32768/50000]\tLoss: 0.81\n","Training Epoch: 130 [33024/50000]\tLoss: 0.72\n","Training Epoch: 130 [33280/50000]\tLoss: 0.86\n","Training Epoch: 130 [33536/50000]\tLoss: 0.91\n","Training Epoch: 130 [33792/50000]\tLoss: 0.71\n","Training Epoch: 130 [34048/50000]\tLoss: 0.71\n","Training Epoch: 130 [34304/50000]\tLoss: 0.85\n","Training Epoch: 130 [34560/50000]\tLoss: 0.71\n","Training Epoch: 130 [34816/50000]\tLoss: 0.72\n","Training Epoch: 130 [35072/50000]\tLoss: 0.85\n","Training Epoch: 130 [35328/50000]\tLoss: 0.72\n","Training Epoch: 130 [35584/50000]\tLoss: 0.86\n","Training Epoch: 130 [35840/50000]\tLoss: 0.75\n","Training Epoch: 130 [36096/50000]\tLoss: 0.80\n","Training Epoch: 130 [36352/50000]\tLoss: 0.81\n","Training Epoch: 130 [36608/50000]\tLoss: 0.76\n","Training Epoch: 130 [36864/50000]\tLoss: 0.83\n","Training Epoch: 130 [37120/50000]\tLoss: 0.83\n","Training Epoch: 130 [37376/50000]\tLoss: 0.66\n","Training Epoch: 130 [37632/50000]\tLoss: 0.78\n","Training Epoch: 130 [37888/50000]\tLoss: 0.77\n","Training Epoch: 130 [38144/50000]\tLoss: 0.81\n","Training Epoch: 130 [38400/50000]\tLoss: 0.68\n","Training Epoch: 130 [38656/50000]\tLoss: 0.87\n","Training Epoch: 130 [38912/50000]\tLoss: 0.94\n","Training Epoch: 130 [39168/50000]\tLoss: 0.76\n","Training Epoch: 130 [39424/50000]\tLoss: 0.79\n","Training Epoch: 130 [39680/50000]\tLoss: 0.69\n","Training Epoch: 130 [39936/50000]\tLoss: 0.83\n","Training Epoch: 130 [40192/50000]\tLoss: 0.86\n","Training Epoch: 130 [40448/50000]\tLoss: 0.77\n","Training Epoch: 130 [40704/50000]\tLoss: 0.60\n","Training Epoch: 130 [40960/50000]\tLoss: 0.69\n","Training Epoch: 130 [41216/50000]\tLoss: 0.87\n","Training Epoch: 130 [41472/50000]\tLoss: 0.73\n","Training Epoch: 130 [41728/50000]\tLoss: 0.74\n","Training Epoch: 130 [41984/50000]\tLoss: 0.75\n","Training Epoch: 130 [42240/50000]\tLoss: 0.92\n","Training Epoch: 130 [42496/50000]\tLoss: 0.92\n","Training Epoch: 130 [42752/50000]\tLoss: 0.83\n","Training Epoch: 130 [43008/50000]\tLoss: 0.82\n","Training Epoch: 130 [43264/50000]\tLoss: 0.67\n","Training Epoch: 130 [43520/50000]\tLoss: 0.90\n","Training Epoch: 130 [43776/50000]\tLoss: 0.76\n","Training Epoch: 130 [44032/50000]\tLoss: 0.77\n","Training Epoch: 130 [44288/50000]\tLoss: 1.00\n","Training Epoch: 130 [44544/50000]\tLoss: 0.71\n","Training Epoch: 130 [44800/50000]\tLoss: 0.95\n","Training Epoch: 130 [45056/50000]\tLoss: 0.76\n","Training Epoch: 130 [45312/50000]\tLoss: 0.72\n","Training Epoch: 130 [45568/50000]\tLoss: 0.79\n","Training Epoch: 130 [45824/50000]\tLoss: 0.74\n","Training Epoch: 130 [46080/50000]\tLoss: 0.78\n","Training Epoch: 130 [46336/50000]\tLoss: 0.81\n","Training Epoch: 130 [46592/50000]\tLoss: 0.76\n","Training Epoch: 130 [46848/50000]\tLoss: 0.74\n","Training Epoch: 130 [47104/50000]\tLoss: 0.83\n","Training Epoch: 130 [47360/50000]\tLoss: 0.70\n","Training Epoch: 130 [47616/50000]\tLoss: 0.71\n","Training Epoch: 130 [47872/50000]\tLoss: 0.78\n","Training Epoch: 130 [48128/50000]\tLoss: 0.72\n","Training Epoch: 130 [48384/50000]\tLoss: 0.77\n","Training Epoch: 130 [48640/50000]\tLoss: 0.69\n","Training Epoch: 130 [48896/50000]\tLoss: 0.82\n","Training Epoch: 130 [49152/50000]\tLoss: 0.82\n","Training Epoch: 130 [49408/50000]\tLoss: 0.72\n","Training Epoch: 130 [49664/50000]\tLoss: 0.77\n","Training Epoch: 130 [49920/50000]\tLoss: 0.74\n","Training Epoch: 130 [50000/50000]\tLoss: 0.86\n","Time taken to train epoch 130: 27.24s\n","Testing Network for epoch:  130\n","Evaluation: Evaluation Time: 2.59s, Average loss: 0.0062, Accuracy: 0.6175, Recall: 0.6175, Precision: 0.6222\n","Training Epoch: 131 [256/50000]\tLoss: 0.68\n","Training Epoch: 131 [512/50000]\tLoss: 0.69\n","Training Epoch: 131 [768/50000]\tLoss: 0.76\n","Training Epoch: 131 [1024/50000]\tLoss: 0.77\n","Training Epoch: 131 [1280/50000]\tLoss: 0.73\n","Training Epoch: 131 [1536/50000]\tLoss: 0.87\n","Training Epoch: 131 [1792/50000]\tLoss: 0.82\n","Training Epoch: 131 [2048/50000]\tLoss: 0.79\n","Training Epoch: 131 [2304/50000]\tLoss: 0.77\n","Training Epoch: 131 [2560/50000]\tLoss: 0.81\n","Training Epoch: 131 [2816/50000]\tLoss: 0.66\n","Training Epoch: 131 [3072/50000]\tLoss: 0.78\n","Training Epoch: 131 [3328/50000]\tLoss: 0.82\n","Training Epoch: 131 [3584/50000]\tLoss: 0.69\n","Training Epoch: 131 [3840/50000]\tLoss: 0.83\n","Training Epoch: 131 [4096/50000]\tLoss: 0.81\n","Training Epoch: 131 [4352/50000]\tLoss: 0.76\n","Training Epoch: 131 [4608/50000]\tLoss: 0.69\n","Training Epoch: 131 [4864/50000]\tLoss: 0.78\n","Training Epoch: 131 [5120/50000]\tLoss: 0.75\n","Training Epoch: 131 [5376/50000]\tLoss: 0.91\n","Training Epoch: 131 [5632/50000]\tLoss: 0.78\n","Training Epoch: 131 [5888/50000]\tLoss: 0.83\n","Training Epoch: 131 [6144/50000]\tLoss: 0.67\n","Training Epoch: 131 [6400/50000]\tLoss: 0.75\n","Training Epoch: 131 [6656/50000]\tLoss: 0.75\n","Training Epoch: 131 [6912/50000]\tLoss: 0.75\n","Training Epoch: 131 [7168/50000]\tLoss: 0.90\n","Training Epoch: 131 [7424/50000]\tLoss: 0.67\n","Training Epoch: 131 [7680/50000]\tLoss: 0.80\n","Training Epoch: 131 [7936/50000]\tLoss: 0.76\n","Training Epoch: 131 [8192/50000]\tLoss: 0.66\n","Training Epoch: 131 [8448/50000]\tLoss: 0.90\n","Training Epoch: 131 [8704/50000]\tLoss: 0.71\n","Training Epoch: 131 [8960/50000]\tLoss: 0.85\n","Training Epoch: 131 [9216/50000]\tLoss: 0.62\n","Training Epoch: 131 [9472/50000]\tLoss: 0.65\n","Training Epoch: 131 [9728/50000]\tLoss: 0.78\n","Training Epoch: 131 [9984/50000]\tLoss: 0.74\n","Training Epoch: 131 [10240/50000]\tLoss: 0.96\n","Training Epoch: 131 [10496/50000]\tLoss: 0.76\n","Training Epoch: 131 [10752/50000]\tLoss: 0.93\n","Training Epoch: 131 [11008/50000]\tLoss: 0.66\n","Training Epoch: 131 [11264/50000]\tLoss: 0.73\n","Training Epoch: 131 [11520/50000]\tLoss: 0.74\n","Training Epoch: 131 [11776/50000]\tLoss: 0.72\n","Training Epoch: 131 [12032/50000]\tLoss: 0.69\n","Training Epoch: 131 [12288/50000]\tLoss: 0.71\n","Training Epoch: 131 [12544/50000]\tLoss: 0.69\n","Training Epoch: 131 [12800/50000]\tLoss: 0.81\n","Training Epoch: 131 [13056/50000]\tLoss: 0.80\n","Training Epoch: 131 [13312/50000]\tLoss: 0.75\n","Training Epoch: 131 [13568/50000]\tLoss: 0.68\n","Training Epoch: 131 [13824/50000]\tLoss: 0.69\n","Training Epoch: 131 [14080/50000]\tLoss: 0.73\n","Training Epoch: 131 [14336/50000]\tLoss: 0.79\n","Training Epoch: 131 [14592/50000]\tLoss: 0.68\n","Training Epoch: 131 [14848/50000]\tLoss: 0.77\n","Training Epoch: 131 [15104/50000]\tLoss: 0.67\n","Training Epoch: 131 [15360/50000]\tLoss: 0.81\n","Training Epoch: 131 [15616/50000]\tLoss: 0.77\n","Training Epoch: 131 [15872/50000]\tLoss: 0.57\n","Training Epoch: 131 [16128/50000]\tLoss: 0.78\n","Training Epoch: 131 [16384/50000]\tLoss: 0.90\n","Training Epoch: 131 [16640/50000]\tLoss: 1.00\n","Training Epoch: 131 [16896/50000]\tLoss: 0.69\n","Training Epoch: 131 [17152/50000]\tLoss: 0.80\n","Training Epoch: 131 [17408/50000]\tLoss: 0.71\n","Training Epoch: 131 [17664/50000]\tLoss: 0.80\n","Training Epoch: 131 [17920/50000]\tLoss: 0.76\n","Training Epoch: 131 [18176/50000]\tLoss: 0.96\n","Training Epoch: 131 [18432/50000]\tLoss: 0.63\n","Training Epoch: 131 [18688/50000]\tLoss: 0.70\n","Training Epoch: 131 [18944/50000]\tLoss: 0.68\n","Training Epoch: 131 [19200/50000]\tLoss: 0.63\n","Training Epoch: 131 [19456/50000]\tLoss: 0.73\n","Training Epoch: 131 [19712/50000]\tLoss: 0.85\n","Training Epoch: 131 [19968/50000]\tLoss: 0.68\n","Training Epoch: 131 [20224/50000]\tLoss: 0.79\n","Training Epoch: 131 [20480/50000]\tLoss: 0.74\n","Training Epoch: 131 [20736/50000]\tLoss: 0.74\n","Training Epoch: 131 [20992/50000]\tLoss: 0.78\n","Training Epoch: 131 [21248/50000]\tLoss: 0.77\n","Training Epoch: 131 [21504/50000]\tLoss: 0.70\n","Training Epoch: 131 [21760/50000]\tLoss: 0.87\n","Training Epoch: 131 [22016/50000]\tLoss: 0.80\n","Training Epoch: 131 [22272/50000]\tLoss: 0.71\n","Training Epoch: 131 [22528/50000]\tLoss: 0.94\n","Training Epoch: 131 [22784/50000]\tLoss: 0.87\n","Training Epoch: 131 [23040/50000]\tLoss: 0.81\n","Training Epoch: 131 [23296/50000]\tLoss: 0.66\n","Training Epoch: 131 [23552/50000]\tLoss: 0.87\n","Training Epoch: 131 [23808/50000]\tLoss: 0.78\n","Training Epoch: 131 [24064/50000]\tLoss: 0.75\n","Training Epoch: 131 [24320/50000]\tLoss: 0.72\n","Training Epoch: 131 [24576/50000]\tLoss: 0.84\n","Training Epoch: 131 [24832/50000]\tLoss: 0.82\n","Training Epoch: 131 [25088/50000]\tLoss: 0.86\n","Training Epoch: 131 [25344/50000]\tLoss: 0.87\n","Training Epoch: 131 [25600/50000]\tLoss: 0.71\n","Training Epoch: 131 [25856/50000]\tLoss: 0.68\n","Training Epoch: 131 [26112/50000]\tLoss: 0.79\n","Training Epoch: 131 [26368/50000]\tLoss: 0.82\n","Training Epoch: 131 [26624/50000]\tLoss: 0.75\n","Training Epoch: 131 [26880/50000]\tLoss: 0.76\n","Training Epoch: 131 [27136/50000]\tLoss: 0.82\n","Training Epoch: 131 [27392/50000]\tLoss: 0.65\n","Training Epoch: 131 [27648/50000]\tLoss: 0.81\n","Training Epoch: 131 [27904/50000]\tLoss: 0.76\n","Training Epoch: 131 [28160/50000]\tLoss: 0.79\n","Training Epoch: 131 [28416/50000]\tLoss: 0.91\n","Training Epoch: 131 [28672/50000]\tLoss: 0.64\n","Training Epoch: 131 [28928/50000]\tLoss: 0.71\n","Training Epoch: 131 [29184/50000]\tLoss: 0.86\n","Training Epoch: 131 [29440/50000]\tLoss: 0.80\n","Training Epoch: 131 [29696/50000]\tLoss: 0.71\n","Training Epoch: 131 [29952/50000]\tLoss: 0.80\n","Training Epoch: 131 [30208/50000]\tLoss: 0.85\n","Training Epoch: 131 [30464/50000]\tLoss: 0.88\n","Training Epoch: 131 [30720/50000]\tLoss: 0.92\n","Training Epoch: 131 [30976/50000]\tLoss: 0.71\n","Training Epoch: 131 [31232/50000]\tLoss: 0.76\n","Training Epoch: 131 [31488/50000]\tLoss: 0.80\n","Training Epoch: 131 [31744/50000]\tLoss: 0.76\n","Training Epoch: 131 [32000/50000]\tLoss: 0.66\n","Training Epoch: 131 [32256/50000]\tLoss: 0.65\n","Training Epoch: 131 [32512/50000]\tLoss: 0.71\n","Training Epoch: 131 [32768/50000]\tLoss: 0.80\n","Training Epoch: 131 [33024/50000]\tLoss: 0.93\n","Training Epoch: 131 [33280/50000]\tLoss: 0.71\n","Training Epoch: 131 [33536/50000]\tLoss: 0.75\n","Training Epoch: 131 [33792/50000]\tLoss: 0.96\n","Training Epoch: 131 [34048/50000]\tLoss: 0.86\n","Training Epoch: 131 [34304/50000]\tLoss: 0.91\n","Training Epoch: 131 [34560/50000]\tLoss: 0.73\n","Training Epoch: 131 [34816/50000]\tLoss: 0.83\n","Training Epoch: 131 [35072/50000]\tLoss: 0.75\n","Training Epoch: 131 [35328/50000]\tLoss: 0.72\n","Training Epoch: 131 [35584/50000]\tLoss: 0.89\n","Training Epoch: 131 [35840/50000]\tLoss: 0.69\n","Training Epoch: 131 [36096/50000]\tLoss: 0.71\n","Training Epoch: 131 [36352/50000]\tLoss: 0.82\n","Training Epoch: 131 [36608/50000]\tLoss: 0.85\n","Training Epoch: 131 [36864/50000]\tLoss: 0.72\n","Training Epoch: 131 [37120/50000]\tLoss: 0.94\n","Training Epoch: 131 [37376/50000]\tLoss: 0.87\n","Training Epoch: 131 [37632/50000]\tLoss: 0.81\n","Training Epoch: 131 [37888/50000]\tLoss: 0.79\n","Training Epoch: 131 [38144/50000]\tLoss: 0.80\n","Training Epoch: 131 [38400/50000]\tLoss: 0.71\n","Training Epoch: 131 [38656/50000]\tLoss: 0.70\n","Training Epoch: 131 [38912/50000]\tLoss: 0.79\n","Training Epoch: 131 [39168/50000]\tLoss: 0.71\n","Training Epoch: 131 [39424/50000]\tLoss: 0.76\n","Training Epoch: 131 [39680/50000]\tLoss: 0.66\n","Training Epoch: 131 [39936/50000]\tLoss: 0.71\n","Training Epoch: 131 [40192/50000]\tLoss: 0.78\n","Training Epoch: 131 [40448/50000]\tLoss: 0.68\n","Training Epoch: 131 [40704/50000]\tLoss: 0.71\n","Training Epoch: 131 [40960/50000]\tLoss: 0.65\n","Training Epoch: 131 [41216/50000]\tLoss: 0.70\n","Training Epoch: 131 [41472/50000]\tLoss: 0.78\n","Training Epoch: 131 [41728/50000]\tLoss: 0.70\n","Training Epoch: 131 [41984/50000]\tLoss: 0.81\n","Training Epoch: 131 [42240/50000]\tLoss: 0.63\n","Training Epoch: 131 [42496/50000]\tLoss: 0.76\n","Training Epoch: 131 [42752/50000]\tLoss: 0.70\n","Training Epoch: 131 [43008/50000]\tLoss: 0.82\n","Training Epoch: 131 [43264/50000]\tLoss: 0.81\n","Training Epoch: 131 [43520/50000]\tLoss: 0.71\n","Training Epoch: 131 [43776/50000]\tLoss: 0.76\n","Training Epoch: 131 [44032/50000]\tLoss: 0.86\n","Training Epoch: 131 [44288/50000]\tLoss: 0.77\n","Training Epoch: 131 [44544/50000]\tLoss: 0.79\n","Training Epoch: 131 [44800/50000]\tLoss: 0.76\n","Training Epoch: 131 [45056/50000]\tLoss: 0.75\n","Training Epoch: 131 [45312/50000]\tLoss: 0.83\n","Training Epoch: 131 [45568/50000]\tLoss: 0.86\n","Training Epoch: 131 [45824/50000]\tLoss: 0.81\n","Training Epoch: 131 [46080/50000]\tLoss: 0.79\n","Training Epoch: 131 [46336/50000]\tLoss: 0.85\n","Training Epoch: 131 [46592/50000]\tLoss: 0.76\n","Training Epoch: 131 [46848/50000]\tLoss: 0.65\n","Training Epoch: 131 [47104/50000]\tLoss: 0.86\n","Training Epoch: 131 [47360/50000]\tLoss: 0.91\n","Training Epoch: 131 [47616/50000]\tLoss: 0.80\n","Training Epoch: 131 [47872/50000]\tLoss: 0.74\n","Training Epoch: 131 [48128/50000]\tLoss: 0.81\n","Training Epoch: 131 [48384/50000]\tLoss: 0.71\n","Training Epoch: 131 [48640/50000]\tLoss: 0.64\n","Training Epoch: 131 [48896/50000]\tLoss: 0.80\n","Training Epoch: 131 [49152/50000]\tLoss: 0.84\n","Training Epoch: 131 [49408/50000]\tLoss: 0.80\n","Training Epoch: 131 [49664/50000]\tLoss: 0.73\n","Training Epoch: 131 [49920/50000]\tLoss: 0.75\n","Training Epoch: 131 [50000/50000]\tLoss: 0.99\n","Time taken to train epoch 131: 27.27s\n","Testing Network for epoch:  131\n","Evaluation: Evaluation Time: 2.62s, Average loss: 0.0062, Accuracy: 0.6221, Recall: 0.6221, Precision: 0.6279\n","Training Epoch: 132 [256/50000]\tLoss: 0.67\n","Training Epoch: 132 [512/50000]\tLoss: 0.72\n","Training Epoch: 132 [768/50000]\tLoss: 0.83\n","Training Epoch: 132 [1024/50000]\tLoss: 0.68\n","Training Epoch: 132 [1280/50000]\tLoss: 0.73\n","Training Epoch: 132 [1536/50000]\tLoss: 0.61\n","Training Epoch: 132 [1792/50000]\tLoss: 0.92\n","Training Epoch: 132 [2048/50000]\tLoss: 0.78\n","Training Epoch: 132 [2304/50000]\tLoss: 0.72\n","Training Epoch: 132 [2560/50000]\tLoss: 0.74\n","Training Epoch: 132 [2816/50000]\tLoss: 0.69\n","Training Epoch: 132 [3072/50000]\tLoss: 0.65\n","Training Epoch: 132 [3328/50000]\tLoss: 0.63\n","Training Epoch: 132 [3584/50000]\tLoss: 0.76\n","Training Epoch: 132 [3840/50000]\tLoss: 0.82\n","Training Epoch: 132 [4096/50000]\tLoss: 0.86\n","Training Epoch: 132 [4352/50000]\tLoss: 0.78\n","Training Epoch: 132 [4608/50000]\tLoss: 0.74\n","Training Epoch: 132 [4864/50000]\tLoss: 0.76\n","Training Epoch: 132 [5120/50000]\tLoss: 0.68\n","Training Epoch: 132 [5376/50000]\tLoss: 0.72\n","Training Epoch: 132 [5632/50000]\tLoss: 0.84\n","Training Epoch: 132 [5888/50000]\tLoss: 0.75\n","Training Epoch: 132 [6144/50000]\tLoss: 0.75\n","Training Epoch: 132 [6400/50000]\tLoss: 0.80\n","Training Epoch: 132 [6656/50000]\tLoss: 0.65\n","Training Epoch: 132 [6912/50000]\tLoss: 0.87\n","Training Epoch: 132 [7168/50000]\tLoss: 0.70\n","Training Epoch: 132 [7424/50000]\tLoss: 0.82\n","Training Epoch: 132 [7680/50000]\tLoss: 0.87\n","Training Epoch: 132 [7936/50000]\tLoss: 0.70\n","Training Epoch: 132 [8192/50000]\tLoss: 0.76\n","Training Epoch: 132 [8448/50000]\tLoss: 0.77\n","Training Epoch: 132 [8704/50000]\tLoss: 0.67\n","Training Epoch: 132 [8960/50000]\tLoss: 0.75\n","Training Epoch: 132 [9216/50000]\tLoss: 0.69\n","Training Epoch: 132 [9472/50000]\tLoss: 0.73\n","Training Epoch: 132 [9728/50000]\tLoss: 0.86\n","Training Epoch: 132 [9984/50000]\tLoss: 0.86\n","Training Epoch: 132 [10240/50000]\tLoss: 0.80\n","Training Epoch: 132 [10496/50000]\tLoss: 0.81\n","Training Epoch: 132 [10752/50000]\tLoss: 0.87\n","Training Epoch: 132 [11008/50000]\tLoss: 0.78\n","Training Epoch: 132 [11264/50000]\tLoss: 0.83\n","Training Epoch: 132 [11520/50000]\tLoss: 0.86\n","Training Epoch: 132 [11776/50000]\tLoss: 0.77\n","Training Epoch: 132 [12032/50000]\tLoss: 0.83\n","Training Epoch: 132 [12288/50000]\tLoss: 0.79\n","Training Epoch: 132 [12544/50000]\tLoss: 0.76\n","Training Epoch: 132 [12800/50000]\tLoss: 0.75\n","Training Epoch: 132 [13056/50000]\tLoss: 0.81\n","Training Epoch: 132 [13312/50000]\tLoss: 0.73\n","Training Epoch: 132 [13568/50000]\tLoss: 0.73\n","Training Epoch: 132 [13824/50000]\tLoss: 0.68\n","Training Epoch: 132 [14080/50000]\tLoss: 0.71\n","Training Epoch: 132 [14336/50000]\tLoss: 0.85\n","Training Epoch: 132 [14592/50000]\tLoss: 0.73\n","Training Epoch: 132 [14848/50000]\tLoss: 0.90\n","Training Epoch: 132 [15104/50000]\tLoss: 0.66\n","Training Epoch: 132 [15360/50000]\tLoss: 0.78\n","Training Epoch: 132 [15616/50000]\tLoss: 0.73\n","Training Epoch: 132 [15872/50000]\tLoss: 0.70\n","Training Epoch: 132 [16128/50000]\tLoss: 0.68\n","Training Epoch: 132 [16384/50000]\tLoss: 0.95\n","Training Epoch: 132 [16640/50000]\tLoss: 0.77\n","Training Epoch: 132 [16896/50000]\tLoss: 0.68\n","Training Epoch: 132 [17152/50000]\tLoss: 0.86\n","Training Epoch: 132 [17408/50000]\tLoss: 0.86\n","Training Epoch: 132 [17664/50000]\tLoss: 0.64\n","Training Epoch: 132 [17920/50000]\tLoss: 0.78\n","Training Epoch: 132 [18176/50000]\tLoss: 0.71\n","Training Epoch: 132 [18432/50000]\tLoss: 0.69\n","Training Epoch: 132 [18688/50000]\tLoss: 0.76\n","Training Epoch: 132 [18944/50000]\tLoss: 0.80\n","Training Epoch: 132 [19200/50000]\tLoss: 0.79\n","Training Epoch: 132 [19456/50000]\tLoss: 0.77\n","Training Epoch: 132 [19712/50000]\tLoss: 0.73\n","Training Epoch: 132 [19968/50000]\tLoss: 0.85\n","Training Epoch: 132 [20224/50000]\tLoss: 0.69\n","Training Epoch: 132 [20480/50000]\tLoss: 0.68\n","Training Epoch: 132 [20736/50000]\tLoss: 0.67\n","Training Epoch: 132 [20992/50000]\tLoss: 0.88\n","Training Epoch: 132 [21248/50000]\tLoss: 0.80\n","Training Epoch: 132 [21504/50000]\tLoss: 0.70\n","Training Epoch: 132 [21760/50000]\tLoss: 0.80\n","Training Epoch: 132 [22016/50000]\tLoss: 0.63\n","Training Epoch: 132 [22272/50000]\tLoss: 0.69\n","Training Epoch: 132 [22528/50000]\tLoss: 0.61\n","Training Epoch: 132 [22784/50000]\tLoss: 0.81\n","Training Epoch: 132 [23040/50000]\tLoss: 0.68\n","Training Epoch: 132 [23296/50000]\tLoss: 0.86\n","Training Epoch: 132 [23552/50000]\tLoss: 0.84\n","Training Epoch: 132 [23808/50000]\tLoss: 0.86\n","Training Epoch: 132 [24064/50000]\tLoss: 0.76\n","Training Epoch: 132 [24320/50000]\tLoss: 0.87\n","Training Epoch: 132 [24576/50000]\tLoss: 0.90\n","Training Epoch: 132 [24832/50000]\tLoss: 0.89\n","Training Epoch: 132 [25088/50000]\tLoss: 0.72\n","Training Epoch: 132 [25344/50000]\tLoss: 0.86\n","Training Epoch: 132 [25600/50000]\tLoss: 0.68\n","Training Epoch: 132 [25856/50000]\tLoss: 0.89\n","Training Epoch: 132 [26112/50000]\tLoss: 0.75\n","Training Epoch: 132 [26368/50000]\tLoss: 0.86\n","Training Epoch: 132 [26624/50000]\tLoss: 0.73\n","Training Epoch: 132 [26880/50000]\tLoss: 0.72\n","Training Epoch: 132 [27136/50000]\tLoss: 0.60\n","Training Epoch: 132 [27392/50000]\tLoss: 0.78\n","Training Epoch: 132 [27648/50000]\tLoss: 0.77\n","Training Epoch: 132 [27904/50000]\tLoss: 0.82\n","Training Epoch: 132 [28160/50000]\tLoss: 0.74\n","Training Epoch: 132 [28416/50000]\tLoss: 0.88\n","Training Epoch: 132 [28672/50000]\tLoss: 0.71\n","Training Epoch: 132 [28928/50000]\tLoss: 0.82\n","Training Epoch: 132 [29184/50000]\tLoss: 0.77\n","Training Epoch: 132 [29440/50000]\tLoss: 0.85\n","Training Epoch: 132 [29696/50000]\tLoss: 0.66\n","Training Epoch: 132 [29952/50000]\tLoss: 0.73\n","Training Epoch: 132 [30208/50000]\tLoss: 0.79\n","Training Epoch: 132 [30464/50000]\tLoss: 0.84\n","Training Epoch: 132 [30720/50000]\tLoss: 0.81\n","Training Epoch: 132 [30976/50000]\tLoss: 0.92\n","Training Epoch: 132 [31232/50000]\tLoss: 0.88\n","Training Epoch: 132 [31488/50000]\tLoss: 0.90\n","Training Epoch: 132 [31744/50000]\tLoss: 0.77\n","Training Epoch: 132 [32000/50000]\tLoss: 0.73\n","Training Epoch: 132 [32256/50000]\tLoss: 0.76\n","Training Epoch: 132 [32512/50000]\tLoss: 0.88\n","Training Epoch: 132 [32768/50000]\tLoss: 0.78\n","Training Epoch: 132 [33024/50000]\tLoss: 0.73\n","Training Epoch: 132 [33280/50000]\tLoss: 0.67\n","Training Epoch: 132 [33536/50000]\tLoss: 0.79\n","Training Epoch: 132 [33792/50000]\tLoss: 0.74\n","Training Epoch: 132 [34048/50000]\tLoss: 0.69\n","Training Epoch: 132 [34304/50000]\tLoss: 0.75\n","Training Epoch: 132 [34560/50000]\tLoss: 0.65\n","Training Epoch: 132 [34816/50000]\tLoss: 0.85\n","Training Epoch: 132 [35072/50000]\tLoss: 0.68\n","Training Epoch: 132 [35328/50000]\tLoss: 0.75\n","Training Epoch: 132 [35584/50000]\tLoss: 0.71\n","Training Epoch: 132 [35840/50000]\tLoss: 0.71\n","Training Epoch: 132 [36096/50000]\tLoss: 0.83\n","Training Epoch: 132 [36352/50000]\tLoss: 0.74\n","Training Epoch: 132 [36608/50000]\tLoss: 0.86\n","Training Epoch: 132 [36864/50000]\tLoss: 0.75\n","Training Epoch: 132 [37120/50000]\tLoss: 0.72\n","Training Epoch: 132 [37376/50000]\tLoss: 0.78\n","Training Epoch: 132 [37632/50000]\tLoss: 0.66\n","Training Epoch: 132 [37888/50000]\tLoss: 0.80\n","Training Epoch: 132 [38144/50000]\tLoss: 0.78\n","Training Epoch: 132 [38400/50000]\tLoss: 0.94\n","Training Epoch: 132 [38656/50000]\tLoss: 0.98\n","Training Epoch: 132 [38912/50000]\tLoss: 0.65\n","Training Epoch: 132 [39168/50000]\tLoss: 0.78\n","Training Epoch: 132 [39424/50000]\tLoss: 0.96\n","Training Epoch: 132 [39680/50000]\tLoss: 0.68\n","Training Epoch: 132 [39936/50000]\tLoss: 0.80\n","Training Epoch: 132 [40192/50000]\tLoss: 0.89\n","Training Epoch: 132 [40448/50000]\tLoss: 0.75\n","Training Epoch: 132 [40704/50000]\tLoss: 0.92\n","Training Epoch: 132 [40960/50000]\tLoss: 0.94\n","Training Epoch: 132 [41216/50000]\tLoss: 0.80\n","Training Epoch: 132 [41472/50000]\tLoss: 0.75\n","Training Epoch: 132 [41728/50000]\tLoss: 0.70\n","Training Epoch: 132 [41984/50000]\tLoss: 0.80\n","Training Epoch: 132 [42240/50000]\tLoss: 0.64\n","Training Epoch: 132 [42496/50000]\tLoss: 0.85\n","Training Epoch: 132 [42752/50000]\tLoss: 0.88\n","Training Epoch: 132 [43008/50000]\tLoss: 0.75\n","Training Epoch: 132 [43264/50000]\tLoss: 0.92\n","Training Epoch: 132 [43520/50000]\tLoss: 0.89\n","Training Epoch: 132 [43776/50000]\tLoss: 0.88\n","Training Epoch: 132 [44032/50000]\tLoss: 0.76\n","Training Epoch: 132 [44288/50000]\tLoss: 0.82\n","Training Epoch: 132 [44544/50000]\tLoss: 0.83\n","Training Epoch: 132 [44800/50000]\tLoss: 0.74\n","Training Epoch: 132 [45056/50000]\tLoss: 0.78\n","Training Epoch: 132 [45312/50000]\tLoss: 0.77\n","Training Epoch: 132 [45568/50000]\tLoss: 0.65\n","Training Epoch: 132 [45824/50000]\tLoss: 0.68\n","Training Epoch: 132 [46080/50000]\tLoss: 0.83\n","Training Epoch: 132 [46336/50000]\tLoss: 0.87\n","Training Epoch: 132 [46592/50000]\tLoss: 0.81\n","Training Epoch: 132 [46848/50000]\tLoss: 0.74\n","Training Epoch: 132 [47104/50000]\tLoss: 0.71\n","Training Epoch: 132 [47360/50000]\tLoss: 0.76\n","Training Epoch: 132 [47616/50000]\tLoss: 0.74\n","Training Epoch: 132 [47872/50000]\tLoss: 0.87\n","Training Epoch: 132 [48128/50000]\tLoss: 0.78\n","Training Epoch: 132 [48384/50000]\tLoss: 0.82\n","Training Epoch: 132 [48640/50000]\tLoss: 0.78\n","Training Epoch: 132 [48896/50000]\tLoss: 0.79\n","Training Epoch: 132 [49152/50000]\tLoss: 0.71\n","Training Epoch: 132 [49408/50000]\tLoss: 0.67\n","Training Epoch: 132 [49664/50000]\tLoss: 0.85\n","Training Epoch: 132 [49920/50000]\tLoss: 0.78\n","Training Epoch: 132 [50000/50000]\tLoss: 1.08\n","Time taken to train epoch 132: 27.32s\n","Testing Network for epoch:  132\n","Evaluation: Evaluation Time: 2.62s, Average loss: 0.0061, Accuracy: 0.6214, Recall: 0.6214, Precision: 0.6249\n","Training Epoch: 133 [256/50000]\tLoss: 0.71\n","Training Epoch: 133 [512/50000]\tLoss: 0.91\n","Training Epoch: 133 [768/50000]\tLoss: 0.71\n","Training Epoch: 133 [1024/50000]\tLoss: 0.75\n","Training Epoch: 133 [1280/50000]\tLoss: 0.96\n","Training Epoch: 133 [1536/50000]\tLoss: 0.63\n","Training Epoch: 133 [1792/50000]\tLoss: 0.88\n","Training Epoch: 133 [2048/50000]\tLoss: 0.74\n","Training Epoch: 133 [2304/50000]\tLoss: 0.71\n","Training Epoch: 133 [2560/50000]\tLoss: 0.77\n","Training Epoch: 133 [2816/50000]\tLoss: 0.74\n","Training Epoch: 133 [3072/50000]\tLoss: 0.74\n","Training Epoch: 133 [3328/50000]\tLoss: 0.75\n","Training Epoch: 133 [3584/50000]\tLoss: 0.75\n","Training Epoch: 133 [3840/50000]\tLoss: 0.80\n","Training Epoch: 133 [4096/50000]\tLoss: 0.84\n","Training Epoch: 133 [4352/50000]\tLoss: 0.76\n","Training Epoch: 133 [4608/50000]\tLoss: 0.77\n","Training Epoch: 133 [4864/50000]\tLoss: 0.63\n","Training Epoch: 133 [5120/50000]\tLoss: 0.73\n","Training Epoch: 133 [5376/50000]\tLoss: 0.79\n","Training Epoch: 133 [5632/50000]\tLoss: 0.70\n","Training Epoch: 133 [5888/50000]\tLoss: 0.76\n","Training Epoch: 133 [6144/50000]\tLoss: 0.63\n","Training Epoch: 133 [6400/50000]\tLoss: 0.74\n","Training Epoch: 133 [6656/50000]\tLoss: 0.73\n","Training Epoch: 133 [6912/50000]\tLoss: 0.76\n","Training Epoch: 133 [7168/50000]\tLoss: 0.77\n","Training Epoch: 133 [7424/50000]\tLoss: 0.87\n","Training Epoch: 133 [7680/50000]\tLoss: 0.76\n","Training Epoch: 133 [7936/50000]\tLoss: 0.71\n","Training Epoch: 133 [8192/50000]\tLoss: 0.82\n","Training Epoch: 133 [8448/50000]\tLoss: 0.84\n","Training Epoch: 133 [8704/50000]\tLoss: 0.90\n","Training Epoch: 133 [8960/50000]\tLoss: 0.69\n","Training Epoch: 133 [9216/50000]\tLoss: 0.79\n","Training Epoch: 133 [9472/50000]\tLoss: 0.74\n","Training Epoch: 133 [9728/50000]\tLoss: 0.73\n","Training Epoch: 133 [9984/50000]\tLoss: 0.73\n","Training Epoch: 133 [10240/50000]\tLoss: 0.80\n","Training Epoch: 133 [10496/50000]\tLoss: 0.76\n","Training Epoch: 133 [10752/50000]\tLoss: 0.83\n","Training Epoch: 133 [11008/50000]\tLoss: 0.68\n","Training Epoch: 133 [11264/50000]\tLoss: 0.81\n","Training Epoch: 133 [11520/50000]\tLoss: 0.67\n","Training Epoch: 133 [11776/50000]\tLoss: 0.69\n","Training Epoch: 133 [12032/50000]\tLoss: 0.79\n","Training Epoch: 133 [12288/50000]\tLoss: 0.78\n","Training Epoch: 133 [12544/50000]\tLoss: 0.92\n","Training Epoch: 133 [12800/50000]\tLoss: 0.71\n","Training Epoch: 133 [13056/50000]\tLoss: 0.72\n","Training Epoch: 133 [13312/50000]\tLoss: 0.91\n","Training Epoch: 133 [13568/50000]\tLoss: 0.74\n","Training Epoch: 133 [13824/50000]\tLoss: 0.70\n","Training Epoch: 133 [14080/50000]\tLoss: 0.77\n","Training Epoch: 133 [14336/50000]\tLoss: 0.76\n","Training Epoch: 133 [14592/50000]\tLoss: 0.83\n","Training Epoch: 133 [14848/50000]\tLoss: 0.76\n","Training Epoch: 133 [15104/50000]\tLoss: 0.76\n","Training Epoch: 133 [15360/50000]\tLoss: 0.78\n","Training Epoch: 133 [15616/50000]\tLoss: 0.71\n","Training Epoch: 133 [15872/50000]\tLoss: 0.70\n","Training Epoch: 133 [16128/50000]\tLoss: 0.69\n","Training Epoch: 133 [16384/50000]\tLoss: 0.68\n","Training Epoch: 133 [16640/50000]\tLoss: 0.83\n","Training Epoch: 133 [16896/50000]\tLoss: 0.90\n","Training Epoch: 133 [17152/50000]\tLoss: 0.77\n","Training Epoch: 133 [17408/50000]\tLoss: 0.87\n","Training Epoch: 133 [17664/50000]\tLoss: 0.68\n","Training Epoch: 133 [17920/50000]\tLoss: 0.83\n","Training Epoch: 133 [18176/50000]\tLoss: 0.82\n","Training Epoch: 133 [18432/50000]\tLoss: 0.93\n","Training Epoch: 133 [18688/50000]\tLoss: 0.79\n","Training Epoch: 133 [18944/50000]\tLoss: 0.61\n","Training Epoch: 133 [19200/50000]\tLoss: 0.76\n","Training Epoch: 133 [19456/50000]\tLoss: 0.83\n","Training Epoch: 133 [19712/50000]\tLoss: 0.79\n","Training Epoch: 133 [19968/50000]\tLoss: 0.61\n","Training Epoch: 133 [20224/50000]\tLoss: 0.79\n","Training Epoch: 133 [20480/50000]\tLoss: 0.72\n","Training Epoch: 133 [20736/50000]\tLoss: 0.91\n","Training Epoch: 133 [20992/50000]\tLoss: 0.79\n","Training Epoch: 133 [21248/50000]\tLoss: 0.70\n","Training Epoch: 133 [21504/50000]\tLoss: 0.77\n","Training Epoch: 133 [21760/50000]\tLoss: 0.78\n","Training Epoch: 133 [22016/50000]\tLoss: 0.80\n","Training Epoch: 133 [22272/50000]\tLoss: 0.81\n","Training Epoch: 133 [22528/50000]\tLoss: 0.76\n","Training Epoch: 133 [22784/50000]\tLoss: 0.87\n","Training Epoch: 133 [23040/50000]\tLoss: 0.79\n","Training Epoch: 133 [23296/50000]\tLoss: 0.64\n","Training Epoch: 133 [23552/50000]\tLoss: 0.73\n","Training Epoch: 133 [23808/50000]\tLoss: 0.74\n","Training Epoch: 133 [24064/50000]\tLoss: 0.91\n","Training Epoch: 133 [24320/50000]\tLoss: 0.78\n","Training Epoch: 133 [24576/50000]\tLoss: 0.76\n","Training Epoch: 133 [24832/50000]\tLoss: 1.00\n","Training Epoch: 133 [25088/50000]\tLoss: 0.79\n","Training Epoch: 133 [25344/50000]\tLoss: 0.71\n","Training Epoch: 133 [25600/50000]\tLoss: 0.91\n","Training Epoch: 133 [25856/50000]\tLoss: 0.77\n","Training Epoch: 133 [26112/50000]\tLoss: 0.91\n","Training Epoch: 133 [26368/50000]\tLoss: 0.72\n","Training Epoch: 133 [26624/50000]\tLoss: 0.71\n","Training Epoch: 133 [26880/50000]\tLoss: 0.73\n","Training Epoch: 133 [27136/50000]\tLoss: 0.76\n","Training Epoch: 133 [27392/50000]\tLoss: 0.66\n","Training Epoch: 133 [27648/50000]\tLoss: 0.84\n","Training Epoch: 133 [27904/50000]\tLoss: 0.77\n","Training Epoch: 133 [28160/50000]\tLoss: 0.82\n","Training Epoch: 133 [28416/50000]\tLoss: 0.69\n","Training Epoch: 133 [28672/50000]\tLoss: 0.89\n","Training Epoch: 133 [28928/50000]\tLoss: 0.78\n","Training Epoch: 133 [29184/50000]\tLoss: 0.84\n","Training Epoch: 133 [29440/50000]\tLoss: 0.64\n","Training Epoch: 133 [29696/50000]\tLoss: 0.89\n","Training Epoch: 133 [29952/50000]\tLoss: 0.81\n","Training Epoch: 133 [30208/50000]\tLoss: 0.76\n","Training Epoch: 133 [30464/50000]\tLoss: 0.77\n","Training Epoch: 133 [30720/50000]\tLoss: 0.82\n","Training Epoch: 133 [30976/50000]\tLoss: 0.84\n","Training Epoch: 133 [31232/50000]\tLoss: 0.75\n","Training Epoch: 133 [31488/50000]\tLoss: 0.68\n","Training Epoch: 133 [31744/50000]\tLoss: 0.70\n","Training Epoch: 133 [32000/50000]\tLoss: 0.77\n","Training Epoch: 133 [32256/50000]\tLoss: 0.86\n","Training Epoch: 133 [32512/50000]\tLoss: 0.74\n","Training Epoch: 133 [32768/50000]\tLoss: 0.81\n","Training Epoch: 133 [33024/50000]\tLoss: 0.72\n","Training Epoch: 133 [33280/50000]\tLoss: 0.81\n","Training Epoch: 133 [33536/50000]\tLoss: 0.82\n","Training Epoch: 133 [33792/50000]\tLoss: 0.73\n","Training Epoch: 133 [34048/50000]\tLoss: 0.89\n","Training Epoch: 133 [34304/50000]\tLoss: 0.78\n","Training Epoch: 133 [34560/50000]\tLoss: 0.78\n","Training Epoch: 133 [34816/50000]\tLoss: 0.78\n","Training Epoch: 133 [35072/50000]\tLoss: 0.90\n","Training Epoch: 133 [35328/50000]\tLoss: 0.66\n","Training Epoch: 133 [35584/50000]\tLoss: 0.72\n","Training Epoch: 133 [35840/50000]\tLoss: 0.84\n","Training Epoch: 133 [36096/50000]\tLoss: 0.71\n","Training Epoch: 133 [36352/50000]\tLoss: 0.73\n","Training Epoch: 133 [36608/50000]\tLoss: 0.86\n","Training Epoch: 133 [36864/50000]\tLoss: 0.82\n","Training Epoch: 133 [37120/50000]\tLoss: 0.87\n","Training Epoch: 133 [37376/50000]\tLoss: 0.88\n","Training Epoch: 133 [37632/50000]\tLoss: 0.72\n","Training Epoch: 133 [37888/50000]\tLoss: 0.72\n","Training Epoch: 133 [38144/50000]\tLoss: 0.83\n","Training Epoch: 133 [38400/50000]\tLoss: 0.80\n","Training Epoch: 133 [38656/50000]\tLoss: 0.84\n","Training Epoch: 133 [38912/50000]\tLoss: 0.83\n","Training Epoch: 133 [39168/50000]\tLoss: 0.68\n","Training Epoch: 133 [39424/50000]\tLoss: 0.80\n","Training Epoch: 133 [39680/50000]\tLoss: 0.70\n","Training Epoch: 133 [39936/50000]\tLoss: 0.78\n","Training Epoch: 133 [40192/50000]\tLoss: 0.83\n","Training Epoch: 133 [40448/50000]\tLoss: 0.80\n","Training Epoch: 133 [40704/50000]\tLoss: 0.81\n","Training Epoch: 133 [40960/50000]\tLoss: 0.87\n","Training Epoch: 133 [41216/50000]\tLoss: 0.82\n","Training Epoch: 133 [41472/50000]\tLoss: 0.79\n","Training Epoch: 133 [41728/50000]\tLoss: 0.67\n","Training Epoch: 133 [41984/50000]\tLoss: 0.74\n","Training Epoch: 133 [42240/50000]\tLoss: 0.76\n","Training Epoch: 133 [42496/50000]\tLoss: 0.71\n","Training Epoch: 133 [42752/50000]\tLoss: 0.83\n","Training Epoch: 133 [43008/50000]\tLoss: 0.74\n","Training Epoch: 133 [43264/50000]\tLoss: 0.79\n","Training Epoch: 133 [43520/50000]\tLoss: 0.67\n","Training Epoch: 133 [43776/50000]\tLoss: 0.85\n","Training Epoch: 133 [44032/50000]\tLoss: 0.74\n","Training Epoch: 133 [44288/50000]\tLoss: 0.88\n","Training Epoch: 133 [44544/50000]\tLoss: 0.81\n","Training Epoch: 133 [44800/50000]\tLoss: 0.75\n","Training Epoch: 133 [45056/50000]\tLoss: 0.75\n","Training Epoch: 133 [45312/50000]\tLoss: 0.82\n","Training Epoch: 133 [45568/50000]\tLoss: 0.80\n","Training Epoch: 133 [45824/50000]\tLoss: 0.74\n","Training Epoch: 133 [46080/50000]\tLoss: 0.80\n","Training Epoch: 133 [46336/50000]\tLoss: 0.75\n","Training Epoch: 133 [46592/50000]\tLoss: 0.83\n","Training Epoch: 133 [46848/50000]\tLoss: 0.88\n","Training Epoch: 133 [47104/50000]\tLoss: 0.67\n","Training Epoch: 133 [47360/50000]\tLoss: 0.82\n","Training Epoch: 133 [47616/50000]\tLoss: 0.86\n","Training Epoch: 133 [47872/50000]\tLoss: 0.77\n","Training Epoch: 133 [48128/50000]\tLoss: 0.81\n","Training Epoch: 133 [48384/50000]\tLoss: 0.87\n","Training Epoch: 133 [48640/50000]\tLoss: 0.87\n","Training Epoch: 133 [48896/50000]\tLoss: 0.69\n","Training Epoch: 133 [49152/50000]\tLoss: 0.82\n","Training Epoch: 133 [49408/50000]\tLoss: 0.74\n","Training Epoch: 133 [49664/50000]\tLoss: 0.91\n","Training Epoch: 133 [49920/50000]\tLoss: 0.67\n","Training Epoch: 133 [50000/50000]\tLoss: 0.76\n","Time taken to train epoch 133: 27.18s\n","Testing Network for epoch:  133\n","Evaluation: Evaluation Time: 2.59s, Average loss: 0.0062, Accuracy: 0.6203, Recall: 0.6203, Precision: 0.6247\n","Training Epoch: 134 [256/50000]\tLoss: 0.77\n","Training Epoch: 134 [512/50000]\tLoss: 0.75\n","Training Epoch: 134 [768/50000]\tLoss: 0.59\n","Training Epoch: 134 [1024/50000]\tLoss: 0.63\n","Training Epoch: 134 [1280/50000]\tLoss: 0.71\n","Training Epoch: 134 [1536/50000]\tLoss: 0.83\n","Training Epoch: 134 [1792/50000]\tLoss: 0.78\n","Training Epoch: 134 [2048/50000]\tLoss: 0.75\n","Training Epoch: 134 [2304/50000]\tLoss: 0.64\n","Training Epoch: 134 [2560/50000]\tLoss: 0.72\n","Training Epoch: 134 [2816/50000]\tLoss: 0.86\n","Training Epoch: 134 [3072/50000]\tLoss: 0.75\n","Training Epoch: 134 [3328/50000]\tLoss: 0.79\n","Training Epoch: 134 [3584/50000]\tLoss: 0.78\n","Training Epoch: 134 [3840/50000]\tLoss: 0.82\n","Training Epoch: 134 [4096/50000]\tLoss: 0.71\n","Training Epoch: 134 [4352/50000]\tLoss: 0.78\n","Training Epoch: 134 [4608/50000]\tLoss: 0.80\n","Training Epoch: 134 [4864/50000]\tLoss: 0.73\n","Training Epoch: 134 [5120/50000]\tLoss: 0.81\n","Training Epoch: 134 [5376/50000]\tLoss: 0.76\n","Training Epoch: 134 [5632/50000]\tLoss: 0.77\n","Training Epoch: 134 [5888/50000]\tLoss: 0.70\n","Training Epoch: 134 [6144/50000]\tLoss: 0.73\n","Training Epoch: 134 [6400/50000]\tLoss: 0.68\n","Training Epoch: 134 [6656/50000]\tLoss: 0.79\n","Training Epoch: 134 [6912/50000]\tLoss: 0.71\n","Training Epoch: 134 [7168/50000]\tLoss: 0.96\n","Training Epoch: 134 [7424/50000]\tLoss: 0.70\n","Training Epoch: 134 [7680/50000]\tLoss: 0.73\n","Training Epoch: 134 [7936/50000]\tLoss: 0.79\n","Training Epoch: 134 [8192/50000]\tLoss: 0.73\n","Training Epoch: 134 [8448/50000]\tLoss: 0.84\n","Training Epoch: 134 [8704/50000]\tLoss: 0.75\n","Training Epoch: 134 [8960/50000]\tLoss: 0.83\n","Training Epoch: 134 [9216/50000]\tLoss: 0.79\n","Training Epoch: 134 [9472/50000]\tLoss: 0.82\n","Training Epoch: 134 [9728/50000]\tLoss: 0.96\n","Training Epoch: 134 [9984/50000]\tLoss: 0.86\n","Training Epoch: 134 [10240/50000]\tLoss: 0.70\n","Training Epoch: 134 [10496/50000]\tLoss: 0.91\n","Training Epoch: 134 [10752/50000]\tLoss: 0.71\n","Training Epoch: 134 [11008/50000]\tLoss: 0.72\n","Training Epoch: 134 [11264/50000]\tLoss: 0.74\n","Training Epoch: 134 [11520/50000]\tLoss: 0.70\n","Training Epoch: 134 [11776/50000]\tLoss: 0.84\n","Training Epoch: 134 [12032/50000]\tLoss: 0.68\n","Training Epoch: 134 [12288/50000]\tLoss: 0.71\n","Training Epoch: 134 [12544/50000]\tLoss: 0.84\n","Training Epoch: 134 [12800/50000]\tLoss: 0.74\n","Training Epoch: 134 [13056/50000]\tLoss: 0.81\n","Training Epoch: 134 [13312/50000]\tLoss: 0.79\n","Training Epoch: 134 [13568/50000]\tLoss: 0.84\n","Training Epoch: 134 [13824/50000]\tLoss: 0.73\n","Training Epoch: 134 [14080/50000]\tLoss: 0.82\n","Training Epoch: 134 [14336/50000]\tLoss: 0.72\n","Training Epoch: 134 [14592/50000]\tLoss: 0.82\n","Training Epoch: 134 [14848/50000]\tLoss: 0.81\n","Training Epoch: 134 [15104/50000]\tLoss: 0.69\n","Training Epoch: 134 [15360/50000]\tLoss: 0.65\n","Training Epoch: 134 [15616/50000]\tLoss: 0.95\n","Training Epoch: 134 [15872/50000]\tLoss: 0.79\n","Training Epoch: 134 [16128/50000]\tLoss: 0.79\n","Training Epoch: 134 [16384/50000]\tLoss: 0.79\n","Training Epoch: 134 [16640/50000]\tLoss: 0.67\n","Training Epoch: 134 [16896/50000]\tLoss: 0.76\n","Training Epoch: 134 [17152/50000]\tLoss: 0.86\n","Training Epoch: 134 [17408/50000]\tLoss: 0.68\n","Training Epoch: 134 [17664/50000]\tLoss: 0.72\n","Training Epoch: 134 [17920/50000]\tLoss: 0.68\n","Training Epoch: 134 [18176/50000]\tLoss: 0.79\n","Training Epoch: 134 [18432/50000]\tLoss: 0.85\n","Training Epoch: 134 [18688/50000]\tLoss: 0.82\n","Training Epoch: 134 [18944/50000]\tLoss: 0.81\n","Training Epoch: 134 [19200/50000]\tLoss: 0.84\n","Training Epoch: 134 [19456/50000]\tLoss: 0.77\n","Training Epoch: 134 [19712/50000]\tLoss: 0.83\n","Training Epoch: 134 [19968/50000]\tLoss: 0.87\n","Training Epoch: 134 [20224/50000]\tLoss: 0.73\n","Training Epoch: 134 [20480/50000]\tLoss: 0.78\n","Training Epoch: 134 [20736/50000]\tLoss: 0.71\n","Training Epoch: 134 [20992/50000]\tLoss: 0.73\n","Training Epoch: 134 [21248/50000]\tLoss: 0.75\n","Training Epoch: 134 [21504/50000]\tLoss: 0.93\n","Training Epoch: 134 [21760/50000]\tLoss: 0.74\n","Training Epoch: 134 [22016/50000]\tLoss: 0.77\n","Training Epoch: 134 [22272/50000]\tLoss: 0.72\n","Training Epoch: 134 [22528/50000]\tLoss: 0.85\n","Training Epoch: 134 [22784/50000]\tLoss: 0.77\n","Training Epoch: 134 [23040/50000]\tLoss: 0.75\n","Training Epoch: 134 [23296/50000]\tLoss: 0.60\n","Training Epoch: 134 [23552/50000]\tLoss: 0.79\n","Training Epoch: 134 [23808/50000]\tLoss: 1.03\n","Training Epoch: 134 [24064/50000]\tLoss: 0.88\n","Training Epoch: 134 [24320/50000]\tLoss: 0.84\n","Training Epoch: 134 [24576/50000]\tLoss: 0.76\n","Training Epoch: 134 [24832/50000]\tLoss: 0.85\n","Training Epoch: 134 [25088/50000]\tLoss: 0.83\n","Training Epoch: 134 [25344/50000]\tLoss: 0.83\n","Training Epoch: 134 [25600/50000]\tLoss: 0.80\n","Training Epoch: 134 [25856/50000]\tLoss: 0.77\n","Training Epoch: 134 [26112/50000]\tLoss: 0.68\n","Training Epoch: 134 [26368/50000]\tLoss: 0.72\n","Training Epoch: 134 [26624/50000]\tLoss: 0.74\n","Training Epoch: 134 [26880/50000]\tLoss: 0.79\n","Training Epoch: 134 [27136/50000]\tLoss: 0.87\n","Training Epoch: 134 [27392/50000]\tLoss: 0.76\n","Training Epoch: 134 [27648/50000]\tLoss: 0.77\n","Training Epoch: 134 [27904/50000]\tLoss: 0.74\n","Training Epoch: 134 [28160/50000]\tLoss: 0.83\n","Training Epoch: 134 [28416/50000]\tLoss: 0.93\n","Training Epoch: 134 [28672/50000]\tLoss: 0.68\n","Training Epoch: 134 [28928/50000]\tLoss: 0.63\n","Training Epoch: 134 [29184/50000]\tLoss: 0.76\n","Training Epoch: 134 [29440/50000]\tLoss: 0.75\n","Training Epoch: 134 [29696/50000]\tLoss: 0.68\n","Training Epoch: 134 [29952/50000]\tLoss: 0.94\n","Training Epoch: 134 [30208/50000]\tLoss: 0.69\n","Training Epoch: 134 [30464/50000]\tLoss: 0.75\n","Training Epoch: 134 [30720/50000]\tLoss: 0.73\n","Training Epoch: 134 [30976/50000]\tLoss: 0.67\n","Training Epoch: 134 [31232/50000]\tLoss: 0.72\n","Training Epoch: 134 [31488/50000]\tLoss: 0.74\n","Training Epoch: 134 [31744/50000]\tLoss: 0.64\n","Training Epoch: 134 [32000/50000]\tLoss: 0.73\n","Training Epoch: 134 [32256/50000]\tLoss: 0.68\n","Training Epoch: 134 [32512/50000]\tLoss: 0.79\n","Training Epoch: 134 [32768/50000]\tLoss: 0.78\n","Training Epoch: 134 [33024/50000]\tLoss: 0.78\n","Training Epoch: 134 [33280/50000]\tLoss: 0.77\n","Training Epoch: 134 [33536/50000]\tLoss: 0.66\n","Training Epoch: 134 [33792/50000]\tLoss: 0.69\n","Training Epoch: 134 [34048/50000]\tLoss: 0.69\n","Training Epoch: 134 [34304/50000]\tLoss: 0.65\n","Training Epoch: 134 [34560/50000]\tLoss: 0.71\n","Training Epoch: 134 [34816/50000]\tLoss: 0.75\n","Training Epoch: 134 [35072/50000]\tLoss: 0.68\n","Training Epoch: 134 [35328/50000]\tLoss: 1.00\n","Training Epoch: 134 [35584/50000]\tLoss: 0.68\n","Training Epoch: 134 [35840/50000]\tLoss: 0.88\n","Training Epoch: 134 [36096/50000]\tLoss: 0.73\n","Training Epoch: 134 [36352/50000]\tLoss: 0.86\n","Training Epoch: 134 [36608/50000]\tLoss: 0.79\n","Training Epoch: 134 [36864/50000]\tLoss: 0.94\n","Training Epoch: 134 [37120/50000]\tLoss: 0.76\n","Training Epoch: 134 [37376/50000]\tLoss: 0.78\n","Training Epoch: 134 [37632/50000]\tLoss: 0.75\n","Training Epoch: 134 [37888/50000]\tLoss: 0.80\n","Training Epoch: 134 [38144/50000]\tLoss: 0.72\n","Training Epoch: 134 [38400/50000]\tLoss: 0.70\n","Training Epoch: 134 [38656/50000]\tLoss: 0.75\n","Training Epoch: 134 [38912/50000]\tLoss: 0.80\n","Training Epoch: 134 [39168/50000]\tLoss: 0.70\n","Training Epoch: 134 [39424/50000]\tLoss: 0.82\n","Training Epoch: 134 [39680/50000]\tLoss: 0.77\n","Training Epoch: 134 [39936/50000]\tLoss: 0.70\n","Training Epoch: 134 [40192/50000]\tLoss: 0.79\n","Training Epoch: 134 [40448/50000]\tLoss: 0.70\n","Training Epoch: 134 [40704/50000]\tLoss: 0.81\n","Training Epoch: 134 [40960/50000]\tLoss: 0.94\n","Training Epoch: 134 [41216/50000]\tLoss: 0.78\n","Training Epoch: 134 [41472/50000]\tLoss: 0.86\n","Training Epoch: 134 [41728/50000]\tLoss: 0.83\n","Training Epoch: 134 [41984/50000]\tLoss: 0.79\n","Training Epoch: 134 [42240/50000]\tLoss: 0.71\n","Training Epoch: 134 [42496/50000]\tLoss: 0.84\n","Training Epoch: 134 [42752/50000]\tLoss: 0.90\n","Training Epoch: 134 [43008/50000]\tLoss: 0.57\n","Training Epoch: 134 [43264/50000]\tLoss: 0.77\n","Training Epoch: 134 [43520/50000]\tLoss: 0.80\n","Training Epoch: 134 [43776/50000]\tLoss: 0.84\n","Training Epoch: 134 [44032/50000]\tLoss: 0.74\n","Training Epoch: 134 [44288/50000]\tLoss: 0.83\n","Training Epoch: 134 [44544/50000]\tLoss: 0.86\n","Training Epoch: 134 [44800/50000]\tLoss: 0.85\n","Training Epoch: 134 [45056/50000]\tLoss: 0.77\n","Training Epoch: 134 [45312/50000]\tLoss: 0.79\n","Training Epoch: 134 [45568/50000]\tLoss: 0.76\n","Training Epoch: 134 [45824/50000]\tLoss: 0.77\n","Training Epoch: 134 [46080/50000]\tLoss: 0.69\n","Training Epoch: 134 [46336/50000]\tLoss: 0.82\n","Training Epoch: 134 [46592/50000]\tLoss: 0.75\n","Training Epoch: 134 [46848/50000]\tLoss: 0.87\n","Training Epoch: 134 [47104/50000]\tLoss: 0.76\n","Training Epoch: 134 [47360/50000]\tLoss: 0.81\n","Training Epoch: 134 [47616/50000]\tLoss: 0.79\n","Training Epoch: 134 [47872/50000]\tLoss: 0.68\n","Training Epoch: 134 [48128/50000]\tLoss: 0.96\n","Training Epoch: 134 [48384/50000]\tLoss: 0.91\n","Training Epoch: 134 [48640/50000]\tLoss: 0.72\n","Training Epoch: 134 [48896/50000]\tLoss: 0.76\n","Training Epoch: 134 [49152/50000]\tLoss: 0.77\n","Training Epoch: 134 [49408/50000]\tLoss: 0.72\n","Training Epoch: 134 [49664/50000]\tLoss: 0.79\n","Training Epoch: 134 [49920/50000]\tLoss: 0.78\n","Training Epoch: 134 [50000/50000]\tLoss: 0.90\n","Time taken to train epoch 134: 27.23s\n","Testing Network for epoch:  134\n","Evaluation: Evaluation Time: 2.55s, Average loss: 0.0062, Accuracy: 0.6209, Recall: 0.6209, Precision: 0.6240\n","Training Epoch: 135 [256/50000]\tLoss: 0.60\n","Training Epoch: 135 [512/50000]\tLoss: 0.74\n","Training Epoch: 135 [768/50000]\tLoss: 0.70\n","Training Epoch: 135 [1024/50000]\tLoss: 0.83\n","Training Epoch: 135 [1280/50000]\tLoss: 0.75\n","Training Epoch: 135 [1536/50000]\tLoss: 0.74\n","Training Epoch: 135 [1792/50000]\tLoss: 0.88\n","Training Epoch: 135 [2048/50000]\tLoss: 0.75\n","Training Epoch: 135 [2304/50000]\tLoss: 0.69\n","Training Epoch: 135 [2560/50000]\tLoss: 0.70\n","Training Epoch: 135 [2816/50000]\tLoss: 0.60\n","Training Epoch: 135 [3072/50000]\tLoss: 0.73\n","Training Epoch: 135 [3328/50000]\tLoss: 0.83\n","Training Epoch: 135 [3584/50000]\tLoss: 0.84\n","Training Epoch: 135 [3840/50000]\tLoss: 0.76\n","Training Epoch: 135 [4096/50000]\tLoss: 0.83\n","Training Epoch: 135 [4352/50000]\tLoss: 0.60\n","Training Epoch: 135 [4608/50000]\tLoss: 0.77\n","Training Epoch: 135 [4864/50000]\tLoss: 0.65\n","Training Epoch: 135 [5120/50000]\tLoss: 0.79\n","Training Epoch: 135 [5376/50000]\tLoss: 0.79\n","Training Epoch: 135 [5632/50000]\tLoss: 0.70\n","Training Epoch: 135 [5888/50000]\tLoss: 0.80\n","Training Epoch: 135 [6144/50000]\tLoss: 0.73\n","Training Epoch: 135 [6400/50000]\tLoss: 0.78\n","Training Epoch: 135 [6656/50000]\tLoss: 0.74\n","Training Epoch: 135 [6912/50000]\tLoss: 0.87\n","Training Epoch: 135 [7168/50000]\tLoss: 0.78\n","Training Epoch: 135 [7424/50000]\tLoss: 0.75\n","Training Epoch: 135 [7680/50000]\tLoss: 0.71\n","Training Epoch: 135 [7936/50000]\tLoss: 0.83\n","Training Epoch: 135 [8192/50000]\tLoss: 0.80\n","Training Epoch: 135 [8448/50000]\tLoss: 0.67\n","Training Epoch: 135 [8704/50000]\tLoss: 0.94\n","Training Epoch: 135 [8960/50000]\tLoss: 0.84\n","Training Epoch: 135 [9216/50000]\tLoss: 0.75\n","Training Epoch: 135 [9472/50000]\tLoss: 0.84\n","Training Epoch: 135 [9728/50000]\tLoss: 0.75\n","Training Epoch: 135 [9984/50000]\tLoss: 0.81\n","Training Epoch: 135 [10240/50000]\tLoss: 0.89\n","Training Epoch: 135 [10496/50000]\tLoss: 0.85\n","Training Epoch: 135 [10752/50000]\tLoss: 0.70\n","Training Epoch: 135 [11008/50000]\tLoss: 0.64\n","Training Epoch: 135 [11264/50000]\tLoss: 0.62\n","Training Epoch: 135 [11520/50000]\tLoss: 0.86\n","Training Epoch: 135 [11776/50000]\tLoss: 0.72\n","Training Epoch: 135 [12032/50000]\tLoss: 0.81\n","Training Epoch: 135 [12288/50000]\tLoss: 0.79\n","Training Epoch: 135 [12544/50000]\tLoss: 0.67\n","Training Epoch: 135 [12800/50000]\tLoss: 0.68\n","Training Epoch: 135 [13056/50000]\tLoss: 0.72\n","Training Epoch: 135 [13312/50000]\tLoss: 0.75\n","Training Epoch: 135 [13568/50000]\tLoss: 0.67\n","Training Epoch: 135 [13824/50000]\tLoss: 0.83\n","Training Epoch: 135 [14080/50000]\tLoss: 0.78\n","Training Epoch: 135 [14336/50000]\tLoss: 0.88\n","Training Epoch: 135 [14592/50000]\tLoss: 0.77\n","Training Epoch: 135 [14848/50000]\tLoss: 0.66\n","Training Epoch: 135 [15104/50000]\tLoss: 0.78\n","Training Epoch: 135 [15360/50000]\tLoss: 0.77\n","Training Epoch: 135 [15616/50000]\tLoss: 0.89\n","Training Epoch: 135 [15872/50000]\tLoss: 0.78\n","Training Epoch: 135 [16128/50000]\tLoss: 0.77\n","Training Epoch: 135 [16384/50000]\tLoss: 0.80\n","Training Epoch: 135 [16640/50000]\tLoss: 0.78\n","Training Epoch: 135 [16896/50000]\tLoss: 0.75\n","Training Epoch: 135 [17152/50000]\tLoss: 0.86\n","Training Epoch: 135 [17408/50000]\tLoss: 0.83\n","Training Epoch: 135 [17664/50000]\tLoss: 0.79\n","Training Epoch: 135 [17920/50000]\tLoss: 0.81\n","Training Epoch: 135 [18176/50000]\tLoss: 0.77\n","Training Epoch: 135 [18432/50000]\tLoss: 0.85\n","Training Epoch: 135 [18688/50000]\tLoss: 0.62\n","Training Epoch: 135 [18944/50000]\tLoss: 0.70\n","Training Epoch: 135 [19200/50000]\tLoss: 0.92\n","Training Epoch: 135 [19456/50000]\tLoss: 0.78\n","Training Epoch: 135 [19712/50000]\tLoss: 0.89\n","Training Epoch: 135 [19968/50000]\tLoss: 0.84\n","Training Epoch: 135 [20224/50000]\tLoss: 0.72\n","Training Epoch: 135 [20480/50000]\tLoss: 0.63\n","Training Epoch: 135 [20736/50000]\tLoss: 0.79\n","Training Epoch: 135 [20992/50000]\tLoss: 0.88\n","Training Epoch: 135 [21248/50000]\tLoss: 0.93\n","Training Epoch: 135 [21504/50000]\tLoss: 0.76\n","Training Epoch: 135 [21760/50000]\tLoss: 0.93\n","Training Epoch: 135 [22016/50000]\tLoss: 0.73\n","Training Epoch: 135 [22272/50000]\tLoss: 0.70\n","Training Epoch: 135 [22528/50000]\tLoss: 0.74\n","Training Epoch: 135 [22784/50000]\tLoss: 0.89\n","Training Epoch: 135 [23040/50000]\tLoss: 0.69\n","Training Epoch: 135 [23296/50000]\tLoss: 0.80\n","Training Epoch: 135 [23552/50000]\tLoss: 0.83\n","Training Epoch: 135 [23808/50000]\tLoss: 0.76\n","Training Epoch: 135 [24064/50000]\tLoss: 0.82\n","Training Epoch: 135 [24320/50000]\tLoss: 0.62\n","Training Epoch: 135 [24576/50000]\tLoss: 0.81\n","Training Epoch: 135 [24832/50000]\tLoss: 0.81\n","Training Epoch: 135 [25088/50000]\tLoss: 0.78\n","Training Epoch: 135 [25344/50000]\tLoss: 0.71\n","Training Epoch: 135 [25600/50000]\tLoss: 0.80\n","Training Epoch: 135 [25856/50000]\tLoss: 0.78\n","Training Epoch: 135 [26112/50000]\tLoss: 0.73\n","Training Epoch: 135 [26368/50000]\tLoss: 0.72\n","Training Epoch: 135 [26624/50000]\tLoss: 0.70\n","Training Epoch: 135 [26880/50000]\tLoss: 0.76\n","Training Epoch: 135 [27136/50000]\tLoss: 0.83\n","Training Epoch: 135 [27392/50000]\tLoss: 0.75\n","Training Epoch: 135 [27648/50000]\tLoss: 0.89\n","Training Epoch: 135 [27904/50000]\tLoss: 0.72\n","Training Epoch: 135 [28160/50000]\tLoss: 0.73\n","Training Epoch: 135 [28416/50000]\tLoss: 0.81\n","Training Epoch: 135 [28672/50000]\tLoss: 0.90\n","Training Epoch: 135 [28928/50000]\tLoss: 0.79\n","Training Epoch: 135 [29184/50000]\tLoss: 0.75\n","Training Epoch: 135 [29440/50000]\tLoss: 0.85\n","Training Epoch: 135 [29696/50000]\tLoss: 0.76\n","Training Epoch: 135 [29952/50000]\tLoss: 0.71\n","Training Epoch: 135 [30208/50000]\tLoss: 0.76\n","Training Epoch: 135 [30464/50000]\tLoss: 0.75\n","Training Epoch: 135 [30720/50000]\tLoss: 0.91\n","Training Epoch: 135 [30976/50000]\tLoss: 0.81\n","Training Epoch: 135 [31232/50000]\tLoss: 0.78\n","Training Epoch: 135 [31488/50000]\tLoss: 0.83\n","Training Epoch: 135 [31744/50000]\tLoss: 0.84\n","Training Epoch: 135 [32000/50000]\tLoss: 0.72\n","Training Epoch: 135 [32256/50000]\tLoss: 0.79\n","Training Epoch: 135 [32512/50000]\tLoss: 0.64\n","Training Epoch: 135 [32768/50000]\tLoss: 0.71\n","Training Epoch: 135 [33024/50000]\tLoss: 0.79\n","Training Epoch: 135 [33280/50000]\tLoss: 0.78\n","Training Epoch: 135 [33536/50000]\tLoss: 0.81\n","Training Epoch: 135 [33792/50000]\tLoss: 0.81\n","Training Epoch: 135 [34048/50000]\tLoss: 0.79\n","Training Epoch: 135 [34304/50000]\tLoss: 0.78\n","Training Epoch: 135 [34560/50000]\tLoss: 0.69\n","Training Epoch: 135 [34816/50000]\tLoss: 0.83\n","Training Epoch: 135 [35072/50000]\tLoss: 0.92\n","Training Epoch: 135 [35328/50000]\tLoss: 0.70\n","Training Epoch: 135 [35584/50000]\tLoss: 0.75\n","Training Epoch: 135 [35840/50000]\tLoss: 0.81\n","Training Epoch: 135 [36096/50000]\tLoss: 0.68\n","Training Epoch: 135 [36352/50000]\tLoss: 0.75\n","Training Epoch: 135 [36608/50000]\tLoss: 0.77\n","Training Epoch: 135 [36864/50000]\tLoss: 0.83\n","Training Epoch: 135 [37120/50000]\tLoss: 0.71\n","Training Epoch: 135 [37376/50000]\tLoss: 0.86\n","Training Epoch: 135 [37632/50000]\tLoss: 0.81\n","Training Epoch: 135 [37888/50000]\tLoss: 0.72\n","Training Epoch: 135 [38144/50000]\tLoss: 0.79\n","Training Epoch: 135 [38400/50000]\tLoss: 0.81\n","Training Epoch: 135 [38656/50000]\tLoss: 0.74\n","Training Epoch: 135 [38912/50000]\tLoss: 0.72\n","Training Epoch: 135 [39168/50000]\tLoss: 0.93\n","Training Epoch: 135 [39424/50000]\tLoss: 0.77\n","Training Epoch: 135 [39680/50000]\tLoss: 0.83\n","Training Epoch: 135 [39936/50000]\tLoss: 0.83\n","Training Epoch: 135 [40192/50000]\tLoss: 0.73\n","Training Epoch: 135 [40448/50000]\tLoss: 0.80\n","Training Epoch: 135 [40704/50000]\tLoss: 0.87\n","Training Epoch: 135 [40960/50000]\tLoss: 0.70\n","Training Epoch: 135 [41216/50000]\tLoss: 0.63\n","Training Epoch: 135 [41472/50000]\tLoss: 0.83\n","Training Epoch: 135 [41728/50000]\tLoss: 0.69\n","Training Epoch: 135 [41984/50000]\tLoss: 0.72\n","Training Epoch: 135 [42240/50000]\tLoss: 0.87\n","Training Epoch: 135 [42496/50000]\tLoss: 0.73\n","Training Epoch: 135 [42752/50000]\tLoss: 0.67\n","Training Epoch: 135 [43008/50000]\tLoss: 0.69\n","Training Epoch: 135 [43264/50000]\tLoss: 0.81\n","Training Epoch: 135 [43520/50000]\tLoss: 0.79\n","Training Epoch: 135 [43776/50000]\tLoss: 0.85\n","Training Epoch: 135 [44032/50000]\tLoss: 0.82\n","Training Epoch: 135 [44288/50000]\tLoss: 0.76\n","Training Epoch: 135 [44544/50000]\tLoss: 0.61\n","Training Epoch: 135 [44800/50000]\tLoss: 0.93\n","Training Epoch: 135 [45056/50000]\tLoss: 0.72\n","Training Epoch: 135 [45312/50000]\tLoss: 0.82\n","Training Epoch: 135 [45568/50000]\tLoss: 0.70\n","Training Epoch: 135 [45824/50000]\tLoss: 0.82\n","Training Epoch: 135 [46080/50000]\tLoss: 0.76\n","Training Epoch: 135 [46336/50000]\tLoss: 0.77\n","Training Epoch: 135 [46592/50000]\tLoss: 0.88\n","Training Epoch: 135 [46848/50000]\tLoss: 0.88\n","Training Epoch: 135 [47104/50000]\tLoss: 0.79\n","Training Epoch: 135 [47360/50000]\tLoss: 0.85\n","Training Epoch: 135 [47616/50000]\tLoss: 0.78\n","Training Epoch: 135 [47872/50000]\tLoss: 0.74\n","Training Epoch: 135 [48128/50000]\tLoss: 0.81\n","Training Epoch: 135 [48384/50000]\tLoss: 0.66\n","Training Epoch: 135 [48640/50000]\tLoss: 0.87\n","Training Epoch: 135 [48896/50000]\tLoss: 1.08\n","Training Epoch: 135 [49152/50000]\tLoss: 0.77\n","Training Epoch: 135 [49408/50000]\tLoss: 0.72\n","Training Epoch: 135 [49664/50000]\tLoss: 0.72\n","Training Epoch: 135 [49920/50000]\tLoss: 0.83\n","Training Epoch: 135 [50000/50000]\tLoss: 0.93\n","Time taken to train epoch 135: 27.26s\n","Testing Network for epoch:  135\n","Evaluation: Evaluation Time: 2.54s, Average loss: 0.0063, Accuracy: 0.6224, Recall: 0.6224, Precision: 0.6249\n","Training Epoch: 136 [256/50000]\tLoss: 0.77\n","Training Epoch: 136 [512/50000]\tLoss: 0.77\n","Training Epoch: 136 [768/50000]\tLoss: 0.54\n","Training Epoch: 136 [1024/50000]\tLoss: 0.64\n","Training Epoch: 136 [1280/50000]\tLoss: 0.61\n","Training Epoch: 136 [1536/50000]\tLoss: 0.83\n","Training Epoch: 136 [1792/50000]\tLoss: 0.77\n","Training Epoch: 136 [2048/50000]\tLoss: 0.67\n","Training Epoch: 136 [2304/50000]\tLoss: 0.84\n","Training Epoch: 136 [2560/50000]\tLoss: 0.79\n","Training Epoch: 136 [2816/50000]\tLoss: 0.83\n","Training Epoch: 136 [3072/50000]\tLoss: 0.77\n","Training Epoch: 136 [3328/50000]\tLoss: 0.72\n","Training Epoch: 136 [3584/50000]\tLoss: 0.99\n","Training Epoch: 136 [3840/50000]\tLoss: 0.91\n","Training Epoch: 136 [4096/50000]\tLoss: 0.76\n","Training Epoch: 136 [4352/50000]\tLoss: 0.71\n","Training Epoch: 136 [4608/50000]\tLoss: 0.64\n","Training Epoch: 136 [4864/50000]\tLoss: 0.78\n","Training Epoch: 136 [5120/50000]\tLoss: 0.96\n","Training Epoch: 136 [5376/50000]\tLoss: 0.65\n","Training Epoch: 136 [5632/50000]\tLoss: 0.80\n","Training Epoch: 136 [5888/50000]\tLoss: 0.81\n","Training Epoch: 136 [6144/50000]\tLoss: 0.87\n","Training Epoch: 136 [6400/50000]\tLoss: 0.80\n","Training Epoch: 136 [6656/50000]\tLoss: 0.65\n","Training Epoch: 136 [6912/50000]\tLoss: 0.73\n","Training Epoch: 136 [7168/50000]\tLoss: 0.73\n","Training Epoch: 136 [7424/50000]\tLoss: 0.69\n","Training Epoch: 136 [7680/50000]\tLoss: 0.72\n","Training Epoch: 136 [7936/50000]\tLoss: 0.84\n","Training Epoch: 136 [8192/50000]\tLoss: 0.86\n","Training Epoch: 136 [8448/50000]\tLoss: 0.82\n","Training Epoch: 136 [8704/50000]\tLoss: 0.82\n","Training Epoch: 136 [8960/50000]\tLoss: 0.67\n","Training Epoch: 136 [9216/50000]\tLoss: 0.74\n","Training Epoch: 136 [9472/50000]\tLoss: 0.74\n","Training Epoch: 136 [9728/50000]\tLoss: 0.78\n","Training Epoch: 136 [9984/50000]\tLoss: 0.73\n","Training Epoch: 136 [10240/50000]\tLoss: 0.79\n","Training Epoch: 136 [10496/50000]\tLoss: 0.72\n","Training Epoch: 136 [10752/50000]\tLoss: 0.67\n","Training Epoch: 136 [11008/50000]\tLoss: 0.83\n","Training Epoch: 136 [11264/50000]\tLoss: 0.58\n","Training Epoch: 136 [11520/50000]\tLoss: 0.68\n","Training Epoch: 136 [11776/50000]\tLoss: 0.66\n","Training Epoch: 136 [12032/50000]\tLoss: 0.78\n","Training Epoch: 136 [12288/50000]\tLoss: 0.82\n","Training Epoch: 136 [12544/50000]\tLoss: 0.92\n","Training Epoch: 136 [12800/50000]\tLoss: 0.74\n","Training Epoch: 136 [13056/50000]\tLoss: 0.68\n","Training Epoch: 136 [13312/50000]\tLoss: 0.75\n","Training Epoch: 136 [13568/50000]\tLoss: 0.78\n","Training Epoch: 136 [13824/50000]\tLoss: 0.71\n","Training Epoch: 136 [14080/50000]\tLoss: 0.74\n","Training Epoch: 136 [14336/50000]\tLoss: 0.76\n","Training Epoch: 136 [14592/50000]\tLoss: 0.75\n","Training Epoch: 136 [14848/50000]\tLoss: 0.81\n","Training Epoch: 136 [15104/50000]\tLoss: 0.81\n","Training Epoch: 136 [15360/50000]\tLoss: 0.80\n","Training Epoch: 136 [15616/50000]\tLoss: 0.90\n","Training Epoch: 136 [15872/50000]\tLoss: 0.69\n","Training Epoch: 136 [16128/50000]\tLoss: 0.66\n","Training Epoch: 136 [16384/50000]\tLoss: 0.73\n","Training Epoch: 136 [16640/50000]\tLoss: 0.84\n","Training Epoch: 136 [16896/50000]\tLoss: 0.77\n","Training Epoch: 136 [17152/50000]\tLoss: 0.70\n","Training Epoch: 136 [17408/50000]\tLoss: 0.92\n","Training Epoch: 136 [17664/50000]\tLoss: 0.71\n","Training Epoch: 136 [17920/50000]\tLoss: 0.80\n","Training Epoch: 136 [18176/50000]\tLoss: 0.77\n","Training Epoch: 136 [18432/50000]\tLoss: 0.71\n","Training Epoch: 136 [18688/50000]\tLoss: 0.79\n","Training Epoch: 136 [18944/50000]\tLoss: 0.78\n","Training Epoch: 136 [19200/50000]\tLoss: 0.85\n","Training Epoch: 136 [19456/50000]\tLoss: 0.72\n","Training Epoch: 136 [19712/50000]\tLoss: 0.72\n","Training Epoch: 136 [19968/50000]\tLoss: 0.71\n","Training Epoch: 136 [20224/50000]\tLoss: 0.57\n","Training Epoch: 136 [20480/50000]\tLoss: 0.82\n","Training Epoch: 136 [20736/50000]\tLoss: 0.76\n","Training Epoch: 136 [20992/50000]\tLoss: 0.82\n","Training Epoch: 136 [21248/50000]\tLoss: 0.80\n","Training Epoch: 136 [21504/50000]\tLoss: 0.87\n","Training Epoch: 136 [21760/50000]\tLoss: 0.84\n","Training Epoch: 136 [22016/50000]\tLoss: 0.76\n","Training Epoch: 136 [22272/50000]\tLoss: 0.85\n","Training Epoch: 136 [22528/50000]\tLoss: 0.73\n","Training Epoch: 136 [22784/50000]\tLoss: 0.75\n","Training Epoch: 136 [23040/50000]\tLoss: 0.71\n","Training Epoch: 136 [23296/50000]\tLoss: 0.85\n","Training Epoch: 136 [23552/50000]\tLoss: 0.71\n","Training Epoch: 136 [23808/50000]\tLoss: 0.84\n","Training Epoch: 136 [24064/50000]\tLoss: 0.81\n","Training Epoch: 136 [24320/50000]\tLoss: 0.85\n","Training Epoch: 136 [24576/50000]\tLoss: 0.74\n","Training Epoch: 136 [24832/50000]\tLoss: 0.77\n","Training Epoch: 136 [25088/50000]\tLoss: 0.76\n","Training Epoch: 136 [25344/50000]\tLoss: 0.67\n","Training Epoch: 136 [25600/50000]\tLoss: 0.73\n","Training Epoch: 136 [25856/50000]\tLoss: 0.94\n","Training Epoch: 136 [26112/50000]\tLoss: 0.76\n","Training Epoch: 136 [26368/50000]\tLoss: 0.95\n","Training Epoch: 136 [26624/50000]\tLoss: 0.95\n","Training Epoch: 136 [26880/50000]\tLoss: 0.74\n","Training Epoch: 136 [27136/50000]\tLoss: 0.78\n","Training Epoch: 136 [27392/50000]\tLoss: 0.69\n","Training Epoch: 136 [27648/50000]\tLoss: 0.72\n","Training Epoch: 136 [27904/50000]\tLoss: 0.79\n","Training Epoch: 136 [28160/50000]\tLoss: 0.63\n","Training Epoch: 136 [28416/50000]\tLoss: 0.73\n","Training Epoch: 136 [28672/50000]\tLoss: 0.77\n","Training Epoch: 136 [28928/50000]\tLoss: 0.74\n","Training Epoch: 136 [29184/50000]\tLoss: 0.70\n","Training Epoch: 136 [29440/50000]\tLoss: 0.81\n","Training Epoch: 136 [29696/50000]\tLoss: 0.71\n","Training Epoch: 136 [29952/50000]\tLoss: 0.67\n","Training Epoch: 136 [30208/50000]\tLoss: 0.67\n","Training Epoch: 136 [30464/50000]\tLoss: 0.79\n","Training Epoch: 136 [30720/50000]\tLoss: 0.78\n","Training Epoch: 136 [30976/50000]\tLoss: 0.73\n","Training Epoch: 136 [31232/50000]\tLoss: 0.72\n","Training Epoch: 136 [31488/50000]\tLoss: 0.79\n","Training Epoch: 136 [31744/50000]\tLoss: 0.86\n","Training Epoch: 136 [32000/50000]\tLoss: 0.76\n","Training Epoch: 136 [32256/50000]\tLoss: 0.82\n","Training Epoch: 136 [32512/50000]\tLoss: 0.68\n","Training Epoch: 136 [32768/50000]\tLoss: 0.81\n","Training Epoch: 136 [33024/50000]\tLoss: 0.79\n","Training Epoch: 136 [33280/50000]\tLoss: 0.99\n","Training Epoch: 136 [33536/50000]\tLoss: 0.79\n","Training Epoch: 136 [33792/50000]\tLoss: 0.76\n","Training Epoch: 136 [34048/50000]\tLoss: 0.72\n","Training Epoch: 136 [34304/50000]\tLoss: 0.71\n","Training Epoch: 136 [34560/50000]\tLoss: 0.96\n","Training Epoch: 136 [34816/50000]\tLoss: 0.82\n","Training Epoch: 136 [35072/50000]\tLoss: 0.83\n","Training Epoch: 136 [35328/50000]\tLoss: 0.74\n","Training Epoch: 136 [35584/50000]\tLoss: 0.76\n","Training Epoch: 136 [35840/50000]\tLoss: 0.69\n","Training Epoch: 136 [36096/50000]\tLoss: 0.75\n","Training Epoch: 136 [36352/50000]\tLoss: 0.80\n","Training Epoch: 136 [36608/50000]\tLoss: 0.92\n","Training Epoch: 136 [36864/50000]\tLoss: 0.83\n","Training Epoch: 136 [37120/50000]\tLoss: 0.87\n","Training Epoch: 136 [37376/50000]\tLoss: 0.65\n","Training Epoch: 136 [37632/50000]\tLoss: 0.78\n","Training Epoch: 136 [37888/50000]\tLoss: 0.77\n","Training Epoch: 136 [38144/50000]\tLoss: 0.62\n","Training Epoch: 136 [38400/50000]\tLoss: 0.73\n","Training Epoch: 136 [38656/50000]\tLoss: 0.79\n","Training Epoch: 136 [38912/50000]\tLoss: 0.74\n","Training Epoch: 136 [39168/50000]\tLoss: 0.83\n","Training Epoch: 136 [39424/50000]\tLoss: 0.80\n","Training Epoch: 136 [39680/50000]\tLoss: 0.74\n","Training Epoch: 136 [39936/50000]\tLoss: 0.58\n","Training Epoch: 136 [40192/50000]\tLoss: 0.69\n","Training Epoch: 136 [40448/50000]\tLoss: 0.77\n","Training Epoch: 136 [40704/50000]\tLoss: 0.65\n","Training Epoch: 136 [40960/50000]\tLoss: 0.70\n","Training Epoch: 136 [41216/50000]\tLoss: 0.77\n","Training Epoch: 136 [41472/50000]\tLoss: 0.78\n","Training Epoch: 136 [41728/50000]\tLoss: 0.86\n","Training Epoch: 136 [41984/50000]\tLoss: 0.85\n","Training Epoch: 136 [42240/50000]\tLoss: 0.77\n","Training Epoch: 136 [42496/50000]\tLoss: 0.69\n","Training Epoch: 136 [42752/50000]\tLoss: 0.73\n","Training Epoch: 136 [43008/50000]\tLoss: 0.81\n","Training Epoch: 136 [43264/50000]\tLoss: 0.95\n","Training Epoch: 136 [43520/50000]\tLoss: 0.65\n","Training Epoch: 136 [43776/50000]\tLoss: 0.75\n","Training Epoch: 136 [44032/50000]\tLoss: 0.85\n","Training Epoch: 136 [44288/50000]\tLoss: 0.76\n","Training Epoch: 136 [44544/50000]\tLoss: 0.76\n","Training Epoch: 136 [44800/50000]\tLoss: 0.77\n","Training Epoch: 136 [45056/50000]\tLoss: 0.90\n","Training Epoch: 136 [45312/50000]\tLoss: 0.80\n","Training Epoch: 136 [45568/50000]\tLoss: 0.69\n","Training Epoch: 136 [45824/50000]\tLoss: 0.66\n","Training Epoch: 136 [46080/50000]\tLoss: 0.82\n","Training Epoch: 136 [46336/50000]\tLoss: 0.63\n","Training Epoch: 136 [46592/50000]\tLoss: 0.78\n","Training Epoch: 136 [46848/50000]\tLoss: 0.81\n","Training Epoch: 136 [47104/50000]\tLoss: 0.85\n","Training Epoch: 136 [47360/50000]\tLoss: 0.69\n","Training Epoch: 136 [47616/50000]\tLoss: 0.75\n","Training Epoch: 136 [47872/50000]\tLoss: 0.87\n","Training Epoch: 136 [48128/50000]\tLoss: 0.78\n","Training Epoch: 136 [48384/50000]\tLoss: 0.75\n","Training Epoch: 136 [48640/50000]\tLoss: 0.81\n","Training Epoch: 136 [48896/50000]\tLoss: 0.72\n","Training Epoch: 136 [49152/50000]\tLoss: 0.83\n","Training Epoch: 136 [49408/50000]\tLoss: 0.57\n","Training Epoch: 136 [49664/50000]\tLoss: 0.71\n","Training Epoch: 136 [49920/50000]\tLoss: 0.83\n","Training Epoch: 136 [50000/50000]\tLoss: 1.17\n","Time taken to train epoch 136: 27.15s\n","Testing Network for epoch:  136\n","Evaluation: Evaluation Time: 2.57s, Average loss: 0.0061, Accuracy: 0.6239, Recall: 0.6239, Precision: 0.6289\n","Training Epoch: 137 [256/50000]\tLoss: 0.81\n","Training Epoch: 137 [512/50000]\tLoss: 0.71\n","Training Epoch: 137 [768/50000]\tLoss: 0.86\n","Training Epoch: 137 [1024/50000]\tLoss: 0.77\n","Training Epoch: 137 [1280/50000]\tLoss: 0.91\n","Training Epoch: 137 [1536/50000]\tLoss: 0.75\n","Training Epoch: 137 [1792/50000]\tLoss: 0.83\n","Training Epoch: 137 [2048/50000]\tLoss: 0.76\n","Training Epoch: 137 [2304/50000]\tLoss: 0.76\n","Training Epoch: 137 [2560/50000]\tLoss: 0.71\n","Training Epoch: 137 [2816/50000]\tLoss: 0.83\n","Training Epoch: 137 [3072/50000]\tLoss: 0.80\n","Training Epoch: 137 [3328/50000]\tLoss: 0.67\n","Training Epoch: 137 [3584/50000]\tLoss: 0.70\n","Training Epoch: 137 [3840/50000]\tLoss: 0.90\n","Training Epoch: 137 [4096/50000]\tLoss: 0.65\n","Training Epoch: 137 [4352/50000]\tLoss: 0.74\n","Training Epoch: 137 [4608/50000]\tLoss: 0.79\n","Training Epoch: 137 [4864/50000]\tLoss: 0.75\n","Training Epoch: 137 [5120/50000]\tLoss: 0.72\n","Training Epoch: 137 [5376/50000]\tLoss: 0.80\n","Training Epoch: 137 [5632/50000]\tLoss: 0.65\n","Training Epoch: 137 [5888/50000]\tLoss: 0.69\n","Training Epoch: 137 [6144/50000]\tLoss: 0.94\n","Training Epoch: 137 [6400/50000]\tLoss: 0.67\n","Training Epoch: 137 [6656/50000]\tLoss: 0.69\n","Training Epoch: 137 [6912/50000]\tLoss: 0.76\n","Training Epoch: 137 [7168/50000]\tLoss: 0.74\n","Training Epoch: 137 [7424/50000]\tLoss: 0.69\n","Training Epoch: 137 [7680/50000]\tLoss: 0.81\n","Training Epoch: 137 [7936/50000]\tLoss: 0.75\n","Training Epoch: 137 [8192/50000]\tLoss: 0.82\n","Training Epoch: 137 [8448/50000]\tLoss: 0.85\n","Training Epoch: 137 [8704/50000]\tLoss: 0.74\n","Training Epoch: 137 [8960/50000]\tLoss: 0.84\n","Training Epoch: 137 [9216/50000]\tLoss: 0.80\n","Training Epoch: 137 [9472/50000]\tLoss: 0.80\n","Training Epoch: 137 [9728/50000]\tLoss: 0.69\n","Training Epoch: 137 [9984/50000]\tLoss: 0.77\n","Training Epoch: 137 [10240/50000]\tLoss: 0.85\n","Training Epoch: 137 [10496/50000]\tLoss: 0.72\n","Training Epoch: 137 [10752/50000]\tLoss: 0.68\n","Training Epoch: 137 [11008/50000]\tLoss: 0.72\n","Training Epoch: 137 [11264/50000]\tLoss: 0.81\n","Training Epoch: 137 [11520/50000]\tLoss: 0.77\n","Training Epoch: 137 [11776/50000]\tLoss: 0.80\n","Training Epoch: 137 [12032/50000]\tLoss: 0.74\n","Training Epoch: 137 [12288/50000]\tLoss: 0.78\n","Training Epoch: 137 [12544/50000]\tLoss: 0.71\n","Training Epoch: 137 [12800/50000]\tLoss: 0.76\n","Training Epoch: 137 [13056/50000]\tLoss: 0.71\n","Training Epoch: 137 [13312/50000]\tLoss: 0.75\n","Training Epoch: 137 [13568/50000]\tLoss: 0.87\n","Training Epoch: 137 [13824/50000]\tLoss: 0.71\n","Training Epoch: 137 [14080/50000]\tLoss: 0.65\n","Training Epoch: 137 [14336/50000]\tLoss: 0.79\n","Training Epoch: 137 [14592/50000]\tLoss: 0.64\n","Training Epoch: 137 [14848/50000]\tLoss: 0.68\n","Training Epoch: 137 [15104/50000]\tLoss: 0.71\n","Training Epoch: 137 [15360/50000]\tLoss: 0.77\n","Training Epoch: 137 [15616/50000]\tLoss: 0.62\n","Training Epoch: 137 [15872/50000]\tLoss: 0.76\n","Training Epoch: 137 [16128/50000]\tLoss: 0.67\n","Training Epoch: 137 [16384/50000]\tLoss: 0.73\n","Training Epoch: 137 [16640/50000]\tLoss: 0.73\n","Training Epoch: 137 [16896/50000]\tLoss: 0.84\n","Training Epoch: 137 [17152/50000]\tLoss: 0.66\n","Training Epoch: 137 [17408/50000]\tLoss: 0.69\n","Training Epoch: 137 [17664/50000]\tLoss: 0.88\n","Training Epoch: 137 [17920/50000]\tLoss: 0.73\n","Training Epoch: 137 [18176/50000]\tLoss: 0.78\n","Training Epoch: 137 [18432/50000]\tLoss: 0.68\n","Training Epoch: 137 [18688/50000]\tLoss: 0.82\n","Training Epoch: 137 [18944/50000]\tLoss: 0.81\n","Training Epoch: 137 [19200/50000]\tLoss: 0.75\n","Training Epoch: 137 [19456/50000]\tLoss: 0.73\n","Training Epoch: 137 [19712/50000]\tLoss: 0.78\n","Training Epoch: 137 [19968/50000]\tLoss: 0.80\n","Training Epoch: 137 [20224/50000]\tLoss: 0.67\n","Training Epoch: 137 [20480/50000]\tLoss: 0.78\n","Training Epoch: 137 [20736/50000]\tLoss: 0.76\n","Training Epoch: 137 [20992/50000]\tLoss: 0.82\n","Training Epoch: 137 [21248/50000]\tLoss: 0.75\n","Training Epoch: 137 [21504/50000]\tLoss: 0.69\n","Training Epoch: 137 [21760/50000]\tLoss: 0.70\n","Training Epoch: 137 [22016/50000]\tLoss: 0.62\n","Training Epoch: 137 [22272/50000]\tLoss: 0.74\n","Training Epoch: 137 [22528/50000]\tLoss: 0.82\n","Training Epoch: 137 [22784/50000]\tLoss: 0.74\n","Training Epoch: 137 [23040/50000]\tLoss: 0.66\n","Training Epoch: 137 [23296/50000]\tLoss: 0.68\n","Training Epoch: 137 [23552/50000]\tLoss: 0.83\n","Training Epoch: 137 [23808/50000]\tLoss: 0.78\n","Training Epoch: 137 [24064/50000]\tLoss: 0.67\n","Training Epoch: 137 [24320/50000]\tLoss: 0.67\n","Training Epoch: 137 [24576/50000]\tLoss: 0.72\n","Training Epoch: 137 [24832/50000]\tLoss: 0.85\n","Training Epoch: 137 [25088/50000]\tLoss: 0.74\n","Training Epoch: 137 [25344/50000]\tLoss: 0.80\n","Training Epoch: 137 [25600/50000]\tLoss: 0.93\n","Training Epoch: 137 [25856/50000]\tLoss: 0.94\n","Training Epoch: 137 [26112/50000]\tLoss: 0.83\n","Training Epoch: 137 [26368/50000]\tLoss: 0.64\n","Training Epoch: 137 [26624/50000]\tLoss: 0.71\n","Training Epoch: 137 [26880/50000]\tLoss: 0.93\n","Training Epoch: 137 [27136/50000]\tLoss: 0.82\n","Training Epoch: 137 [27392/50000]\tLoss: 0.69\n","Training Epoch: 137 [27648/50000]\tLoss: 0.74\n","Training Epoch: 137 [27904/50000]\tLoss: 0.69\n","Training Epoch: 137 [28160/50000]\tLoss: 0.88\n","Training Epoch: 137 [28416/50000]\tLoss: 0.76\n","Training Epoch: 137 [28672/50000]\tLoss: 0.73\n","Training Epoch: 137 [28928/50000]\tLoss: 0.76\n","Training Epoch: 137 [29184/50000]\tLoss: 0.88\n","Training Epoch: 137 [29440/50000]\tLoss: 0.73\n","Training Epoch: 137 [29696/50000]\tLoss: 0.85\n","Training Epoch: 137 [29952/50000]\tLoss: 0.75\n","Training Epoch: 137 [30208/50000]\tLoss: 0.70\n","Training Epoch: 137 [30464/50000]\tLoss: 0.79\n","Training Epoch: 137 [30720/50000]\tLoss: 0.59\n","Training Epoch: 137 [30976/50000]\tLoss: 0.82\n","Training Epoch: 137 [31232/50000]\tLoss: 0.93\n","Training Epoch: 137 [31488/50000]\tLoss: 0.87\n","Training Epoch: 137 [31744/50000]\tLoss: 0.94\n","Training Epoch: 137 [32000/50000]\tLoss: 0.63\n","Training Epoch: 137 [32256/50000]\tLoss: 0.83\n","Training Epoch: 137 [32512/50000]\tLoss: 0.83\n","Training Epoch: 137 [32768/50000]\tLoss: 0.66\n","Training Epoch: 137 [33024/50000]\tLoss: 0.72\n","Training Epoch: 137 [33280/50000]\tLoss: 0.67\n","Training Epoch: 137 [33536/50000]\tLoss: 0.80\n","Training Epoch: 137 [33792/50000]\tLoss: 0.79\n","Training Epoch: 137 [34048/50000]\tLoss: 0.73\n","Training Epoch: 137 [34304/50000]\tLoss: 0.72\n","Training Epoch: 137 [34560/50000]\tLoss: 0.90\n","Training Epoch: 137 [34816/50000]\tLoss: 0.81\n","Training Epoch: 137 [35072/50000]\tLoss: 0.80\n","Training Epoch: 137 [35328/50000]\tLoss: 0.61\n","Training Epoch: 137 [35584/50000]\tLoss: 0.80\n","Training Epoch: 137 [35840/50000]\tLoss: 0.72\n","Training Epoch: 137 [36096/50000]\tLoss: 0.70\n","Training Epoch: 137 [36352/50000]\tLoss: 0.67\n","Training Epoch: 137 [36608/50000]\tLoss: 0.84\n","Training Epoch: 137 [36864/50000]\tLoss: 0.98\n","Training Epoch: 137 [37120/50000]\tLoss: 0.80\n","Training Epoch: 137 [37376/50000]\tLoss: 0.86\n","Training Epoch: 137 [37632/50000]\tLoss: 0.79\n","Training Epoch: 137 [37888/50000]\tLoss: 0.90\n","Training Epoch: 137 [38144/50000]\tLoss: 0.76\n","Training Epoch: 137 [38400/50000]\tLoss: 0.67\n","Training Epoch: 137 [38656/50000]\tLoss: 0.70\n","Training Epoch: 137 [38912/50000]\tLoss: 0.72\n","Training Epoch: 137 [39168/50000]\tLoss: 0.77\n","Training Epoch: 137 [39424/50000]\tLoss: 0.67\n","Training Epoch: 137 [39680/50000]\tLoss: 0.69\n","Training Epoch: 137 [39936/50000]\tLoss: 0.75\n","Training Epoch: 137 [40192/50000]\tLoss: 0.62\n","Training Epoch: 137 [40448/50000]\tLoss: 0.75\n","Training Epoch: 137 [40704/50000]\tLoss: 0.67\n","Training Epoch: 137 [40960/50000]\tLoss: 0.77\n","Training Epoch: 137 [41216/50000]\tLoss: 0.86\n","Training Epoch: 137 [41472/50000]\tLoss: 0.80\n","Training Epoch: 137 [41728/50000]\tLoss: 0.80\n","Training Epoch: 137 [41984/50000]\tLoss: 0.73\n","Training Epoch: 137 [42240/50000]\tLoss: 0.72\n","Training Epoch: 137 [42496/50000]\tLoss: 0.84\n","Training Epoch: 137 [42752/50000]\tLoss: 0.83\n","Training Epoch: 137 [43008/50000]\tLoss: 0.87\n","Training Epoch: 137 [43264/50000]\tLoss: 0.80\n","Training Epoch: 137 [43520/50000]\tLoss: 0.68\n","Training Epoch: 137 [43776/50000]\tLoss: 0.82\n","Training Epoch: 137 [44032/50000]\tLoss: 0.81\n","Training Epoch: 137 [44288/50000]\tLoss: 0.73\n","Training Epoch: 137 [44544/50000]\tLoss: 0.77\n","Training Epoch: 137 [44800/50000]\tLoss: 0.84\n","Training Epoch: 137 [45056/50000]\tLoss: 0.76\n","Training Epoch: 137 [45312/50000]\tLoss: 0.71\n","Training Epoch: 137 [45568/50000]\tLoss: 0.72\n","Training Epoch: 137 [45824/50000]\tLoss: 0.70\n","Training Epoch: 137 [46080/50000]\tLoss: 0.79\n","Training Epoch: 137 [46336/50000]\tLoss: 0.80\n","Training Epoch: 137 [46592/50000]\tLoss: 0.64\n","Training Epoch: 137 [46848/50000]\tLoss: 0.89\n","Training Epoch: 137 [47104/50000]\tLoss: 0.77\n","Training Epoch: 137 [47360/50000]\tLoss: 0.67\n","Training Epoch: 137 [47616/50000]\tLoss: 0.77\n","Training Epoch: 137 [47872/50000]\tLoss: 0.77\n","Training Epoch: 137 [48128/50000]\tLoss: 0.82\n","Training Epoch: 137 [48384/50000]\tLoss: 0.72\n","Training Epoch: 137 [48640/50000]\tLoss: 0.66\n","Training Epoch: 137 [48896/50000]\tLoss: 0.85\n","Training Epoch: 137 [49152/50000]\tLoss: 0.86\n","Training Epoch: 137 [49408/50000]\tLoss: 0.78\n","Training Epoch: 137 [49664/50000]\tLoss: 0.84\n","Training Epoch: 137 [49920/50000]\tLoss: 0.86\n","Training Epoch: 137 [50000/50000]\tLoss: 0.78\n","Time taken to train epoch 137: 27.34s\n","Testing Network for epoch:  137\n","Evaluation: Evaluation Time: 2.58s, Average loss: 0.0063, Accuracy: 0.6240, Recall: 0.6240, Precision: 0.6256\n","Training Epoch: 138 [256/50000]\tLoss: 0.66\n","Training Epoch: 138 [512/50000]\tLoss: 0.74\n","Training Epoch: 138 [768/50000]\tLoss: 0.77\n","Training Epoch: 138 [1024/50000]\tLoss: 0.85\n","Training Epoch: 138 [1280/50000]\tLoss: 0.75\n","Training Epoch: 138 [1536/50000]\tLoss: 0.74\n","Training Epoch: 138 [1792/50000]\tLoss: 0.70\n","Training Epoch: 138 [2048/50000]\tLoss: 0.77\n","Training Epoch: 138 [2304/50000]\tLoss: 0.64\n","Training Epoch: 138 [2560/50000]\tLoss: 0.80\n","Training Epoch: 138 [2816/50000]\tLoss: 0.62\n","Training Epoch: 138 [3072/50000]\tLoss: 0.84\n","Training Epoch: 138 [3328/50000]\tLoss: 0.62\n","Training Epoch: 138 [3584/50000]\tLoss: 0.74\n","Training Epoch: 138 [3840/50000]\tLoss: 0.71\n","Training Epoch: 138 [4096/50000]\tLoss: 0.70\n","Training Epoch: 138 [4352/50000]\tLoss: 0.71\n","Training Epoch: 138 [4608/50000]\tLoss: 0.78\n","Training Epoch: 138 [4864/50000]\tLoss: 0.71\n","Training Epoch: 138 [5120/50000]\tLoss: 0.87\n","Training Epoch: 138 [5376/50000]\tLoss: 0.85\n","Training Epoch: 138 [5632/50000]\tLoss: 0.73\n","Training Epoch: 138 [5888/50000]\tLoss: 0.75\n","Training Epoch: 138 [6144/50000]\tLoss: 0.76\n","Training Epoch: 138 [6400/50000]\tLoss: 0.78\n","Training Epoch: 138 [6656/50000]\tLoss: 0.86\n","Training Epoch: 138 [6912/50000]\tLoss: 0.85\n","Training Epoch: 138 [7168/50000]\tLoss: 0.65\n","Training Epoch: 138 [7424/50000]\tLoss: 0.79\n","Training Epoch: 138 [7680/50000]\tLoss: 0.77\n","Training Epoch: 138 [7936/50000]\tLoss: 0.69\n","Training Epoch: 138 [8192/50000]\tLoss: 0.82\n","Training Epoch: 138 [8448/50000]\tLoss: 0.72\n","Training Epoch: 138 [8704/50000]\tLoss: 0.76\n","Training Epoch: 138 [8960/50000]\tLoss: 0.78\n","Training Epoch: 138 [9216/50000]\tLoss: 0.75\n","Training Epoch: 138 [9472/50000]\tLoss: 0.80\n","Training Epoch: 138 [9728/50000]\tLoss: 0.85\n","Training Epoch: 138 [9984/50000]\tLoss: 0.74\n","Training Epoch: 138 [10240/50000]\tLoss: 0.86\n","Training Epoch: 138 [10496/50000]\tLoss: 0.75\n","Training Epoch: 138 [10752/50000]\tLoss: 0.71\n","Training Epoch: 138 [11008/50000]\tLoss: 0.67\n","Training Epoch: 138 [11264/50000]\tLoss: 0.67\n","Training Epoch: 138 [11520/50000]\tLoss: 0.69\n","Training Epoch: 138 [11776/50000]\tLoss: 0.74\n","Training Epoch: 138 [12032/50000]\tLoss: 0.77\n","Training Epoch: 138 [12288/50000]\tLoss: 0.72\n","Training Epoch: 138 [12544/50000]\tLoss: 0.72\n","Training Epoch: 138 [12800/50000]\tLoss: 0.81\n","Training Epoch: 138 [13056/50000]\tLoss: 0.85\n","Training Epoch: 138 [13312/50000]\tLoss: 0.83\n","Training Epoch: 138 [13568/50000]\tLoss: 0.83\n","Training Epoch: 138 [13824/50000]\tLoss: 0.73\n","Training Epoch: 138 [14080/50000]\tLoss: 0.78\n","Training Epoch: 138 [14336/50000]\tLoss: 0.80\n","Training Epoch: 138 [14592/50000]\tLoss: 0.65\n","Training Epoch: 138 [14848/50000]\tLoss: 0.77\n","Training Epoch: 138 [15104/50000]\tLoss: 0.75\n","Training Epoch: 138 [15360/50000]\tLoss: 0.74\n","Training Epoch: 138 [15616/50000]\tLoss: 0.82\n","Training Epoch: 138 [15872/50000]\tLoss: 0.76\n","Training Epoch: 138 [16128/50000]\tLoss: 0.82\n","Training Epoch: 138 [16384/50000]\tLoss: 0.83\n","Training Epoch: 138 [16640/50000]\tLoss: 0.66\n","Training Epoch: 138 [16896/50000]\tLoss: 0.68\n","Training Epoch: 138 [17152/50000]\tLoss: 0.81\n","Training Epoch: 138 [17408/50000]\tLoss: 0.74\n","Training Epoch: 138 [17664/50000]\tLoss: 0.73\n","Training Epoch: 138 [17920/50000]\tLoss: 0.78\n","Training Epoch: 138 [18176/50000]\tLoss: 0.81\n","Training Epoch: 138 [18432/50000]\tLoss: 0.62\n","Training Epoch: 138 [18688/50000]\tLoss: 0.74\n","Training Epoch: 138 [18944/50000]\tLoss: 0.87\n","Training Epoch: 138 [19200/50000]\tLoss: 0.89\n","Training Epoch: 138 [19456/50000]\tLoss: 0.78\n","Training Epoch: 138 [19712/50000]\tLoss: 0.78\n","Training Epoch: 138 [19968/50000]\tLoss: 0.75\n","Training Epoch: 138 [20224/50000]\tLoss: 0.83\n","Training Epoch: 138 [20480/50000]\tLoss: 0.70\n","Training Epoch: 138 [20736/50000]\tLoss: 0.76\n","Training Epoch: 138 [20992/50000]\tLoss: 0.72\n","Training Epoch: 138 [21248/50000]\tLoss: 0.87\n","Training Epoch: 138 [21504/50000]\tLoss: 0.80\n","Training Epoch: 138 [21760/50000]\tLoss: 0.67\n","Training Epoch: 138 [22016/50000]\tLoss: 0.69\n","Training Epoch: 138 [22272/50000]\tLoss: 0.71\n","Training Epoch: 138 [22528/50000]\tLoss: 0.72\n","Training Epoch: 138 [22784/50000]\tLoss: 0.78\n","Training Epoch: 138 [23040/50000]\tLoss: 0.90\n","Training Epoch: 138 [23296/50000]\tLoss: 0.68\n","Training Epoch: 138 [23552/50000]\tLoss: 0.74\n","Training Epoch: 138 [23808/50000]\tLoss: 0.76\n","Training Epoch: 138 [24064/50000]\tLoss: 0.69\n","Training Epoch: 138 [24320/50000]\tLoss: 0.82\n","Training Epoch: 138 [24576/50000]\tLoss: 0.75\n","Training Epoch: 138 [24832/50000]\tLoss: 0.79\n","Training Epoch: 138 [25088/50000]\tLoss: 0.74\n","Training Epoch: 138 [25344/50000]\tLoss: 0.84\n","Training Epoch: 138 [25600/50000]\tLoss: 0.69\n","Training Epoch: 138 [25856/50000]\tLoss: 0.69\n","Training Epoch: 138 [26112/50000]\tLoss: 0.70\n","Training Epoch: 138 [26368/50000]\tLoss: 0.78\n","Training Epoch: 138 [26624/50000]\tLoss: 0.94\n","Training Epoch: 138 [26880/50000]\tLoss: 0.71\n","Training Epoch: 138 [27136/50000]\tLoss: 0.90\n","Training Epoch: 138 [27392/50000]\tLoss: 0.85\n","Training Epoch: 138 [27648/50000]\tLoss: 0.81\n","Training Epoch: 138 [27904/50000]\tLoss: 0.79\n","Training Epoch: 138 [28160/50000]\tLoss: 0.91\n","Training Epoch: 138 [28416/50000]\tLoss: 0.77\n","Training Epoch: 138 [28672/50000]\tLoss: 0.75\n","Training Epoch: 138 [28928/50000]\tLoss: 0.68\n","Training Epoch: 138 [29184/50000]\tLoss: 0.80\n","Training Epoch: 138 [29440/50000]\tLoss: 0.70\n","Training Epoch: 138 [29696/50000]\tLoss: 0.86\n","Training Epoch: 138 [29952/50000]\tLoss: 0.65\n","Training Epoch: 138 [30208/50000]\tLoss: 0.76\n","Training Epoch: 138 [30464/50000]\tLoss: 0.69\n","Training Epoch: 138 [30720/50000]\tLoss: 0.76\n","Training Epoch: 138 [30976/50000]\tLoss: 0.78\n","Training Epoch: 138 [31232/50000]\tLoss: 0.79\n","Training Epoch: 138 [31488/50000]\tLoss: 0.82\n","Training Epoch: 138 [31744/50000]\tLoss: 0.69\n","Training Epoch: 138 [32000/50000]\tLoss: 0.73\n","Training Epoch: 138 [32256/50000]\tLoss: 0.89\n","Training Epoch: 138 [32512/50000]\tLoss: 0.83\n","Training Epoch: 138 [32768/50000]\tLoss: 0.66\n","Training Epoch: 138 [33024/50000]\tLoss: 0.87\n","Training Epoch: 138 [33280/50000]\tLoss: 0.71\n","Training Epoch: 138 [33536/50000]\tLoss: 0.67\n","Training Epoch: 138 [33792/50000]\tLoss: 0.80\n","Training Epoch: 138 [34048/50000]\tLoss: 0.78\n","Training Epoch: 138 [34304/50000]\tLoss: 0.87\n","Training Epoch: 138 [34560/50000]\tLoss: 0.73\n","Training Epoch: 138 [34816/50000]\tLoss: 0.76\n","Training Epoch: 138 [35072/50000]\tLoss: 0.87\n","Training Epoch: 138 [35328/50000]\tLoss: 0.67\n","Training Epoch: 138 [35584/50000]\tLoss: 0.65\n","Training Epoch: 138 [35840/50000]\tLoss: 0.85\n","Training Epoch: 138 [36096/50000]\tLoss: 0.86\n","Training Epoch: 138 [36352/50000]\tLoss: 0.78\n","Training Epoch: 138 [36608/50000]\tLoss: 0.67\n","Training Epoch: 138 [36864/50000]\tLoss: 0.74\n","Training Epoch: 138 [37120/50000]\tLoss: 0.95\n","Training Epoch: 138 [37376/50000]\tLoss: 0.83\n","Training Epoch: 138 [37632/50000]\tLoss: 0.73\n","Training Epoch: 138 [37888/50000]\tLoss: 0.70\n","Training Epoch: 138 [38144/50000]\tLoss: 0.92\n","Training Epoch: 138 [38400/50000]\tLoss: 0.76\n","Training Epoch: 138 [38656/50000]\tLoss: 0.82\n","Training Epoch: 138 [38912/50000]\tLoss: 0.75\n","Training Epoch: 138 [39168/50000]\tLoss: 0.74\n","Training Epoch: 138 [39424/50000]\tLoss: 0.78\n","Training Epoch: 138 [39680/50000]\tLoss: 0.65\n","Training Epoch: 138 [39936/50000]\tLoss: 0.70\n","Training Epoch: 138 [40192/50000]\tLoss: 0.81\n","Training Epoch: 138 [40448/50000]\tLoss: 0.81\n","Training Epoch: 138 [40704/50000]\tLoss: 0.81\n","Training Epoch: 138 [40960/50000]\tLoss: 0.92\n","Training Epoch: 138 [41216/50000]\tLoss: 0.69\n","Training Epoch: 138 [41472/50000]\tLoss: 0.77\n","Training Epoch: 138 [41728/50000]\tLoss: 0.69\n","Training Epoch: 138 [41984/50000]\tLoss: 0.69\n","Training Epoch: 138 [42240/50000]\tLoss: 0.68\n","Training Epoch: 138 [42496/50000]\tLoss: 0.70\n","Training Epoch: 138 [42752/50000]\tLoss: 0.88\n","Training Epoch: 138 [43008/50000]\tLoss: 0.93\n","Training Epoch: 138 [43264/50000]\tLoss: 0.88\n","Training Epoch: 138 [43520/50000]\tLoss: 0.82\n","Training Epoch: 138 [43776/50000]\tLoss: 0.77\n","Training Epoch: 138 [44032/50000]\tLoss: 0.85\n","Training Epoch: 138 [44288/50000]\tLoss: 0.85\n","Training Epoch: 138 [44544/50000]\tLoss: 0.69\n","Training Epoch: 138 [44800/50000]\tLoss: 0.80\n","Training Epoch: 138 [45056/50000]\tLoss: 0.69\n","Training Epoch: 138 [45312/50000]\tLoss: 0.70\n","Training Epoch: 138 [45568/50000]\tLoss: 0.79\n","Training Epoch: 138 [45824/50000]\tLoss: 0.72\n","Training Epoch: 138 [46080/50000]\tLoss: 0.78\n","Training Epoch: 138 [46336/50000]\tLoss: 0.70\n","Training Epoch: 138 [46592/50000]\tLoss: 0.80\n","Training Epoch: 138 [46848/50000]\tLoss: 0.74\n","Training Epoch: 138 [47104/50000]\tLoss: 0.77\n","Training Epoch: 138 [47360/50000]\tLoss: 0.77\n","Training Epoch: 138 [47616/50000]\tLoss: 0.71\n","Training Epoch: 138 [47872/50000]\tLoss: 0.80\n","Training Epoch: 138 [48128/50000]\tLoss: 0.75\n","Training Epoch: 138 [48384/50000]\tLoss: 0.66\n","Training Epoch: 138 [48640/50000]\tLoss: 0.74\n","Training Epoch: 138 [48896/50000]\tLoss: 0.78\n","Training Epoch: 138 [49152/50000]\tLoss: 0.72\n","Training Epoch: 138 [49408/50000]\tLoss: 0.75\n","Training Epoch: 138 [49664/50000]\tLoss: 0.74\n","Training Epoch: 138 [49920/50000]\tLoss: 0.89\n","Training Epoch: 138 [50000/50000]\tLoss: 0.95\n","Time taken to train epoch 138: 27.24s\n","Testing Network for epoch:  138\n","Evaluation: Evaluation Time: 2.59s, Average loss: 0.0062, Accuracy: 0.6207, Recall: 0.6207, Precision: 0.6265\n","Training Epoch: 139 [256/50000]\tLoss: 0.72\n","Training Epoch: 139 [512/50000]\tLoss: 0.73\n","Training Epoch: 139 [768/50000]\tLoss: 0.77\n","Training Epoch: 139 [1024/50000]\tLoss: 0.70\n","Training Epoch: 139 [1280/50000]\tLoss: 0.68\n","Training Epoch: 139 [1536/50000]\tLoss: 0.83\n","Training Epoch: 139 [1792/50000]\tLoss: 0.93\n","Training Epoch: 139 [2048/50000]\tLoss: 0.74\n","Training Epoch: 139 [2304/50000]\tLoss: 0.89\n","Training Epoch: 139 [2560/50000]\tLoss: 0.67\n","Training Epoch: 139 [2816/50000]\tLoss: 0.66\n","Training Epoch: 139 [3072/50000]\tLoss: 0.87\n","Training Epoch: 139 [3328/50000]\tLoss: 0.72\n","Training Epoch: 139 [3584/50000]\tLoss: 0.68\n","Training Epoch: 139 [3840/50000]\tLoss: 0.73\n","Training Epoch: 139 [4096/50000]\tLoss: 0.81\n","Training Epoch: 139 [4352/50000]\tLoss: 0.65\n","Training Epoch: 139 [4608/50000]\tLoss: 0.70\n","Training Epoch: 139 [4864/50000]\tLoss: 0.73\n","Training Epoch: 139 [5120/50000]\tLoss: 0.66\n","Training Epoch: 139 [5376/50000]\tLoss: 0.78\n","Training Epoch: 139 [5632/50000]\tLoss: 0.83\n","Training Epoch: 139 [5888/50000]\tLoss: 0.89\n","Training Epoch: 139 [6144/50000]\tLoss: 0.68\n","Training Epoch: 139 [6400/50000]\tLoss: 1.13\n","Training Epoch: 139 [6656/50000]\tLoss: 0.70\n","Training Epoch: 139 [6912/50000]\tLoss: 0.74\n","Training Epoch: 139 [7168/50000]\tLoss: 0.81\n","Training Epoch: 139 [7424/50000]\tLoss: 0.76\n","Training Epoch: 139 [7680/50000]\tLoss: 0.82\n","Training Epoch: 139 [7936/50000]\tLoss: 0.75\n","Training Epoch: 139 [8192/50000]\tLoss: 0.70\n","Training Epoch: 139 [8448/50000]\tLoss: 0.69\n","Training Epoch: 139 [8704/50000]\tLoss: 0.76\n","Training Epoch: 139 [8960/50000]\tLoss: 0.73\n","Training Epoch: 139 [9216/50000]\tLoss: 0.76\n","Training Epoch: 139 [9472/50000]\tLoss: 0.80\n","Training Epoch: 139 [9728/50000]\tLoss: 0.77\n","Training Epoch: 139 [9984/50000]\tLoss: 0.86\n","Training Epoch: 139 [10240/50000]\tLoss: 0.71\n","Training Epoch: 139 [10496/50000]\tLoss: 0.79\n","Training Epoch: 139 [10752/50000]\tLoss: 0.76\n","Training Epoch: 139 [11008/50000]\tLoss: 0.74\n","Training Epoch: 139 [11264/50000]\tLoss: 0.63\n","Training Epoch: 139 [11520/50000]\tLoss: 0.96\n","Training Epoch: 139 [11776/50000]\tLoss: 0.65\n","Training Epoch: 139 [12032/50000]\tLoss: 0.76\n","Training Epoch: 139 [12288/50000]\tLoss: 0.63\n","Training Epoch: 139 [12544/50000]\tLoss: 0.82\n","Training Epoch: 139 [12800/50000]\tLoss: 0.71\n","Training Epoch: 139 [13056/50000]\tLoss: 0.79\n","Training Epoch: 139 [13312/50000]\tLoss: 0.74\n","Training Epoch: 139 [13568/50000]\tLoss: 0.72\n","Training Epoch: 139 [13824/50000]\tLoss: 0.75\n","Training Epoch: 139 [14080/50000]\tLoss: 0.76\n","Training Epoch: 139 [14336/50000]\tLoss: 0.67\n","Training Epoch: 139 [14592/50000]\tLoss: 0.86\n","Training Epoch: 139 [14848/50000]\tLoss: 0.84\n","Training Epoch: 139 [15104/50000]\tLoss: 0.66\n","Training Epoch: 139 [15360/50000]\tLoss: 0.77\n","Training Epoch: 139 [15616/50000]\tLoss: 0.83\n","Training Epoch: 139 [15872/50000]\tLoss: 0.70\n","Training Epoch: 139 [16128/50000]\tLoss: 0.79\n","Training Epoch: 139 [16384/50000]\tLoss: 0.84\n","Training Epoch: 139 [16640/50000]\tLoss: 0.73\n","Training Epoch: 139 [16896/50000]\tLoss: 0.71\n","Training Epoch: 139 [17152/50000]\tLoss: 0.69\n","Training Epoch: 139 [17408/50000]\tLoss: 0.72\n","Training Epoch: 139 [17664/50000]\tLoss: 0.73\n","Training Epoch: 139 [17920/50000]\tLoss: 0.73\n","Training Epoch: 139 [18176/50000]\tLoss: 0.67\n","Training Epoch: 139 [18432/50000]\tLoss: 0.72\n","Training Epoch: 139 [18688/50000]\tLoss: 0.85\n","Training Epoch: 139 [18944/50000]\tLoss: 0.82\n","Training Epoch: 139 [19200/50000]\tLoss: 0.73\n","Training Epoch: 139 [19456/50000]\tLoss: 0.72\n","Training Epoch: 139 [19712/50000]\tLoss: 0.74\n","Training Epoch: 139 [19968/50000]\tLoss: 0.65\n","Training Epoch: 139 [20224/50000]\tLoss: 0.80\n","Training Epoch: 139 [20480/50000]\tLoss: 0.77\n","Training Epoch: 139 [20736/50000]\tLoss: 0.64\n","Training Epoch: 139 [20992/50000]\tLoss: 0.70\n","Training Epoch: 139 [21248/50000]\tLoss: 0.80\n","Training Epoch: 139 [21504/50000]\tLoss: 0.80\n","Training Epoch: 139 [21760/50000]\tLoss: 0.87\n","Training Epoch: 139 [22016/50000]\tLoss: 0.94\n","Training Epoch: 139 [22272/50000]\tLoss: 0.69\n","Training Epoch: 139 [22528/50000]\tLoss: 0.72\n","Training Epoch: 139 [22784/50000]\tLoss: 0.69\n","Training Epoch: 139 [23040/50000]\tLoss: 0.83\n","Training Epoch: 139 [23296/50000]\tLoss: 0.61\n","Training Epoch: 139 [23552/50000]\tLoss: 0.79\n","Training Epoch: 139 [23808/50000]\tLoss: 0.84\n","Training Epoch: 139 [24064/50000]\tLoss: 0.70\n","Training Epoch: 139 [24320/50000]\tLoss: 0.72\n","Training Epoch: 139 [24576/50000]\tLoss: 0.80\n","Training Epoch: 139 [24832/50000]\tLoss: 0.77\n","Training Epoch: 139 [25088/50000]\tLoss: 0.70\n","Training Epoch: 139 [25344/50000]\tLoss: 0.67\n","Training Epoch: 139 [25600/50000]\tLoss: 0.73\n","Training Epoch: 139 [25856/50000]\tLoss: 0.82\n","Training Epoch: 139 [26112/50000]\tLoss: 0.81\n","Training Epoch: 139 [26368/50000]\tLoss: 0.75\n","Training Epoch: 139 [26624/50000]\tLoss: 0.79\n","Training Epoch: 139 [26880/50000]\tLoss: 0.80\n","Training Epoch: 139 [27136/50000]\tLoss: 0.77\n","Training Epoch: 139 [27392/50000]\tLoss: 0.75\n","Training Epoch: 139 [27648/50000]\tLoss: 0.85\n","Training Epoch: 139 [27904/50000]\tLoss: 0.70\n","Training Epoch: 139 [28160/50000]\tLoss: 0.76\n","Training Epoch: 139 [28416/50000]\tLoss: 0.80\n","Training Epoch: 139 [28672/50000]\tLoss: 0.76\n","Training Epoch: 139 [28928/50000]\tLoss: 0.71\n","Training Epoch: 139 [29184/50000]\tLoss: 0.94\n","Training Epoch: 139 [29440/50000]\tLoss: 0.95\n","Training Epoch: 139 [29696/50000]\tLoss: 0.81\n","Training Epoch: 139 [29952/50000]\tLoss: 0.76\n","Training Epoch: 139 [30208/50000]\tLoss: 0.70\n","Training Epoch: 139 [30464/50000]\tLoss: 0.83\n","Training Epoch: 139 [30720/50000]\tLoss: 0.69\n","Training Epoch: 139 [30976/50000]\tLoss: 0.84\n","Training Epoch: 139 [31232/50000]\tLoss: 0.68\n","Training Epoch: 139 [31488/50000]\tLoss: 0.81\n","Training Epoch: 139 [31744/50000]\tLoss: 0.87\n","Training Epoch: 139 [32000/50000]\tLoss: 0.69\n","Training Epoch: 139 [32256/50000]\tLoss: 0.93\n","Training Epoch: 139 [32512/50000]\tLoss: 0.74\n","Training Epoch: 139 [32768/50000]\tLoss: 0.72\n","Training Epoch: 139 [33024/50000]\tLoss: 0.67\n","Training Epoch: 139 [33280/50000]\tLoss: 0.89\n","Training Epoch: 139 [33536/50000]\tLoss: 0.69\n","Training Epoch: 139 [33792/50000]\tLoss: 0.67\n","Training Epoch: 139 [34048/50000]\tLoss: 0.76\n","Training Epoch: 139 [34304/50000]\tLoss: 0.76\n","Training Epoch: 139 [34560/50000]\tLoss: 0.77\n","Training Epoch: 139 [34816/50000]\tLoss: 0.86\n","Training Epoch: 139 [35072/50000]\tLoss: 0.75\n","Training Epoch: 139 [35328/50000]\tLoss: 0.73\n","Training Epoch: 139 [35584/50000]\tLoss: 0.76\n","Training Epoch: 139 [35840/50000]\tLoss: 0.82\n","Training Epoch: 139 [36096/50000]\tLoss: 0.79\n","Training Epoch: 139 [36352/50000]\tLoss: 0.86\n","Training Epoch: 139 [36608/50000]\tLoss: 0.78\n","Training Epoch: 139 [36864/50000]\tLoss: 0.81\n","Training Epoch: 139 [37120/50000]\tLoss: 0.69\n","Training Epoch: 139 [37376/50000]\tLoss: 0.70\n","Training Epoch: 139 [37632/50000]\tLoss: 0.82\n","Training Epoch: 139 [37888/50000]\tLoss: 0.70\n","Training Epoch: 139 [38144/50000]\tLoss: 0.79\n","Training Epoch: 139 [38400/50000]\tLoss: 0.76\n","Training Epoch: 139 [38656/50000]\tLoss: 0.80\n","Training Epoch: 139 [38912/50000]\tLoss: 0.85\n","Training Epoch: 139 [39168/50000]\tLoss: 0.80\n","Training Epoch: 139 [39424/50000]\tLoss: 0.77\n","Training Epoch: 139 [39680/50000]\tLoss: 0.67\n","Training Epoch: 139 [39936/50000]\tLoss: 0.74\n","Training Epoch: 139 [40192/50000]\tLoss: 0.77\n","Training Epoch: 139 [40448/50000]\tLoss: 0.81\n","Training Epoch: 139 [40704/50000]\tLoss: 0.74\n","Training Epoch: 139 [40960/50000]\tLoss: 0.76\n","Training Epoch: 139 [41216/50000]\tLoss: 0.71\n","Training Epoch: 139 [41472/50000]\tLoss: 0.66\n","Training Epoch: 139 [41728/50000]\tLoss: 0.63\n","Training Epoch: 139 [41984/50000]\tLoss: 0.81\n","Training Epoch: 139 [42240/50000]\tLoss: 0.82\n","Training Epoch: 139 [42496/50000]\tLoss: 0.91\n","Training Epoch: 139 [42752/50000]\tLoss: 0.75\n","Training Epoch: 139 [43008/50000]\tLoss: 0.75\n","Training Epoch: 139 [43264/50000]\tLoss: 0.77\n","Training Epoch: 139 [43520/50000]\tLoss: 0.88\n","Training Epoch: 139 [43776/50000]\tLoss: 0.72\n","Training Epoch: 139 [44032/50000]\tLoss: 0.76\n","Training Epoch: 139 [44288/50000]\tLoss: 0.68\n","Training Epoch: 139 [44544/50000]\tLoss: 0.80\n","Training Epoch: 139 [44800/50000]\tLoss: 0.69\n","Training Epoch: 139 [45056/50000]\tLoss: 0.71\n","Training Epoch: 139 [45312/50000]\tLoss: 0.72\n","Training Epoch: 139 [45568/50000]\tLoss: 0.90\n","Training Epoch: 139 [45824/50000]\tLoss: 0.74\n","Training Epoch: 139 [46080/50000]\tLoss: 0.83\n","Training Epoch: 139 [46336/50000]\tLoss: 0.69\n","Training Epoch: 139 [46592/50000]\tLoss: 0.76\n","Training Epoch: 139 [46848/50000]\tLoss: 0.66\n","Training Epoch: 139 [47104/50000]\tLoss: 0.74\n","Training Epoch: 139 [47360/50000]\tLoss: 0.75\n","Training Epoch: 139 [47616/50000]\tLoss: 0.78\n","Training Epoch: 139 [47872/50000]\tLoss: 0.73\n","Training Epoch: 139 [48128/50000]\tLoss: 0.85\n","Training Epoch: 139 [48384/50000]\tLoss: 0.72\n","Training Epoch: 139 [48640/50000]\tLoss: 0.87\n","Training Epoch: 139 [48896/50000]\tLoss: 0.75\n","Training Epoch: 139 [49152/50000]\tLoss: 0.86\n","Training Epoch: 139 [49408/50000]\tLoss: 0.73\n","Training Epoch: 139 [49664/50000]\tLoss: 0.83\n","Training Epoch: 139 [49920/50000]\tLoss: 0.74\n","Training Epoch: 139 [50000/50000]\tLoss: 0.83\n","Time taken to train epoch 139: 27.16s\n","Testing Network for epoch:  139\n","Evaluation: Evaluation Time: 2.59s, Average loss: 0.0062, Accuracy: 0.6233, Recall: 0.6233, Precision: 0.6269\n","Training Epoch: 140 [256/50000]\tLoss: 0.76\n","Training Epoch: 140 [512/50000]\tLoss: 0.93\n","Training Epoch: 140 [768/50000]\tLoss: 0.77\n","Training Epoch: 140 [1024/50000]\tLoss: 0.73\n","Training Epoch: 140 [1280/50000]\tLoss: 0.62\n","Training Epoch: 140 [1536/50000]\tLoss: 0.72\n","Training Epoch: 140 [1792/50000]\tLoss: 0.72\n","Training Epoch: 140 [2048/50000]\tLoss: 0.71\n","Training Epoch: 140 [2304/50000]\tLoss: 0.89\n","Training Epoch: 140 [2560/50000]\tLoss: 0.67\n","Training Epoch: 140 [2816/50000]\tLoss: 0.84\n","Training Epoch: 140 [3072/50000]\tLoss: 0.86\n","Training Epoch: 140 [3328/50000]\tLoss: 0.76\n","Training Epoch: 140 [3584/50000]\tLoss: 0.71\n","Training Epoch: 140 [3840/50000]\tLoss: 0.75\n","Training Epoch: 140 [4096/50000]\tLoss: 0.77\n","Training Epoch: 140 [4352/50000]\tLoss: 0.77\n","Training Epoch: 140 [4608/50000]\tLoss: 0.76\n","Training Epoch: 140 [4864/50000]\tLoss: 0.81\n","Training Epoch: 140 [5120/50000]\tLoss: 0.74\n","Training Epoch: 140 [5376/50000]\tLoss: 0.77\n","Training Epoch: 140 [5632/50000]\tLoss: 0.80\n","Training Epoch: 140 [5888/50000]\tLoss: 0.62\n","Training Epoch: 140 [6144/50000]\tLoss: 0.64\n","Training Epoch: 140 [6400/50000]\tLoss: 0.69\n","Training Epoch: 140 [6656/50000]\tLoss: 0.82\n","Training Epoch: 140 [6912/50000]\tLoss: 0.74\n","Training Epoch: 140 [7168/50000]\tLoss: 0.69\n","Training Epoch: 140 [7424/50000]\tLoss: 0.56\n","Training Epoch: 140 [7680/50000]\tLoss: 0.71\n","Training Epoch: 140 [7936/50000]\tLoss: 0.81\n","Training Epoch: 140 [8192/50000]\tLoss: 0.94\n","Training Epoch: 140 [8448/50000]\tLoss: 0.77\n","Training Epoch: 140 [8704/50000]\tLoss: 0.67\n","Training Epoch: 140 [8960/50000]\tLoss: 0.72\n","Training Epoch: 140 [9216/50000]\tLoss: 0.60\n","Training Epoch: 140 [9472/50000]\tLoss: 0.68\n","Training Epoch: 140 [9728/50000]\tLoss: 0.73\n","Training Epoch: 140 [9984/50000]\tLoss: 0.78\n","Training Epoch: 140 [10240/50000]\tLoss: 0.83\n","Training Epoch: 140 [10496/50000]\tLoss: 0.64\n","Training Epoch: 140 [10752/50000]\tLoss: 0.77\n","Training Epoch: 140 [11008/50000]\tLoss: 0.88\n","Training Epoch: 140 [11264/50000]\tLoss: 0.73\n","Training Epoch: 140 [11520/50000]\tLoss: 0.72\n","Training Epoch: 140 [11776/50000]\tLoss: 0.75\n","Training Epoch: 140 [12032/50000]\tLoss: 0.81\n","Training Epoch: 140 [12288/50000]\tLoss: 0.78\n","Training Epoch: 140 [12544/50000]\tLoss: 0.80\n","Training Epoch: 140 [12800/50000]\tLoss: 0.72\n","Training Epoch: 140 [13056/50000]\tLoss: 0.70\n","Training Epoch: 140 [13312/50000]\tLoss: 0.66\n","Training Epoch: 140 [13568/50000]\tLoss: 0.78\n","Training Epoch: 140 [13824/50000]\tLoss: 0.69\n","Training Epoch: 140 [14080/50000]\tLoss: 0.86\n","Training Epoch: 140 [14336/50000]\tLoss: 0.69\n","Training Epoch: 140 [14592/50000]\tLoss: 0.65\n","Training Epoch: 140 [14848/50000]\tLoss: 0.88\n","Training Epoch: 140 [15104/50000]\tLoss: 0.74\n","Training Epoch: 140 [15360/50000]\tLoss: 0.70\n","Training Epoch: 140 [15616/50000]\tLoss: 0.72\n","Training Epoch: 140 [15872/50000]\tLoss: 0.68\n","Training Epoch: 140 [16128/50000]\tLoss: 0.72\n","Training Epoch: 140 [16384/50000]\tLoss: 0.82\n","Training Epoch: 140 [16640/50000]\tLoss: 0.80\n","Training Epoch: 140 [16896/50000]\tLoss: 0.64\n","Training Epoch: 140 [17152/50000]\tLoss: 0.86\n","Training Epoch: 140 [17408/50000]\tLoss: 0.64\n","Training Epoch: 140 [17664/50000]\tLoss: 0.85\n","Training Epoch: 140 [17920/50000]\tLoss: 0.82\n","Training Epoch: 140 [18176/50000]\tLoss: 0.66\n","Training Epoch: 140 [18432/50000]\tLoss: 0.88\n","Training Epoch: 140 [18688/50000]\tLoss: 0.74\n","Training Epoch: 140 [18944/50000]\tLoss: 0.81\n","Training Epoch: 140 [19200/50000]\tLoss: 0.68\n","Training Epoch: 140 [19456/50000]\tLoss: 0.71\n","Training Epoch: 140 [19712/50000]\tLoss: 0.73\n","Training Epoch: 140 [19968/50000]\tLoss: 0.72\n","Training Epoch: 140 [20224/50000]\tLoss: 0.90\n","Training Epoch: 140 [20480/50000]\tLoss: 0.91\n","Training Epoch: 140 [20736/50000]\tLoss: 0.74\n","Training Epoch: 140 [20992/50000]\tLoss: 0.77\n","Training Epoch: 140 [21248/50000]\tLoss: 0.95\n","Training Epoch: 140 [21504/50000]\tLoss: 0.74\n","Training Epoch: 140 [21760/50000]\tLoss: 0.84\n","Training Epoch: 140 [22016/50000]\tLoss: 0.88\n","Training Epoch: 140 [22272/50000]\tLoss: 0.68\n","Training Epoch: 140 [22528/50000]\tLoss: 0.85\n","Training Epoch: 140 [22784/50000]\tLoss: 0.80\n","Training Epoch: 140 [23040/50000]\tLoss: 0.69\n","Training Epoch: 140 [23296/50000]\tLoss: 0.73\n","Training Epoch: 140 [23552/50000]\tLoss: 0.79\n","Training Epoch: 140 [23808/50000]\tLoss: 0.66\n","Training Epoch: 140 [24064/50000]\tLoss: 0.80\n","Training Epoch: 140 [24320/50000]\tLoss: 0.81\n","Training Epoch: 140 [24576/50000]\tLoss: 0.66\n","Training Epoch: 140 [24832/50000]\tLoss: 0.72\n","Training Epoch: 140 [25088/50000]\tLoss: 0.68\n","Training Epoch: 140 [25344/50000]\tLoss: 0.71\n","Training Epoch: 140 [25600/50000]\tLoss: 0.74\n","Training Epoch: 140 [25856/50000]\tLoss: 0.79\n","Training Epoch: 140 [26112/50000]\tLoss: 0.74\n","Training Epoch: 140 [26368/50000]\tLoss: 0.78\n","Training Epoch: 140 [26624/50000]\tLoss: 0.72\n","Training Epoch: 140 [26880/50000]\tLoss: 0.73\n","Training Epoch: 140 [27136/50000]\tLoss: 0.69\n","Training Epoch: 140 [27392/50000]\tLoss: 0.81\n","Training Epoch: 140 [27648/50000]\tLoss: 0.75\n","Training Epoch: 140 [27904/50000]\tLoss: 0.66\n","Training Epoch: 140 [28160/50000]\tLoss: 0.72\n","Training Epoch: 140 [28416/50000]\tLoss: 0.87\n","Training Epoch: 140 [28672/50000]\tLoss: 0.78\n","Training Epoch: 140 [28928/50000]\tLoss: 0.73\n","Training Epoch: 140 [29184/50000]\tLoss: 0.63\n","Training Epoch: 140 [29440/50000]\tLoss: 0.76\n","Training Epoch: 140 [29696/50000]\tLoss: 0.75\n","Training Epoch: 140 [29952/50000]\tLoss: 0.73\n","Training Epoch: 140 [30208/50000]\tLoss: 0.80\n","Training Epoch: 140 [30464/50000]\tLoss: 0.68\n","Training Epoch: 140 [30720/50000]\tLoss: 0.73\n","Training Epoch: 140 [30976/50000]\tLoss: 0.76\n","Training Epoch: 140 [31232/50000]\tLoss: 0.68\n","Training Epoch: 140 [31488/50000]\tLoss: 0.85\n","Training Epoch: 140 [31744/50000]\tLoss: 0.78\n","Training Epoch: 140 [32000/50000]\tLoss: 0.82\n","Training Epoch: 140 [32256/50000]\tLoss: 0.84\n","Training Epoch: 140 [32512/50000]\tLoss: 0.70\n","Training Epoch: 140 [32768/50000]\tLoss: 0.56\n","Training Epoch: 140 [33024/50000]\tLoss: 0.89\n","Training Epoch: 140 [33280/50000]\tLoss: 0.82\n","Training Epoch: 140 [33536/50000]\tLoss: 0.64\n","Training Epoch: 140 [33792/50000]\tLoss: 0.85\n","Training Epoch: 140 [34048/50000]\tLoss: 0.74\n","Training Epoch: 140 [34304/50000]\tLoss: 0.70\n","Training Epoch: 140 [34560/50000]\tLoss: 0.73\n","Training Epoch: 140 [34816/50000]\tLoss: 0.73\n","Training Epoch: 140 [35072/50000]\tLoss: 0.75\n","Training Epoch: 140 [35328/50000]\tLoss: 0.74\n","Training Epoch: 140 [35584/50000]\tLoss: 0.83\n","Training Epoch: 140 [35840/50000]\tLoss: 0.81\n","Training Epoch: 140 [36096/50000]\tLoss: 0.77\n","Training Epoch: 140 [36352/50000]\tLoss: 0.72\n","Training Epoch: 140 [36608/50000]\tLoss: 0.76\n","Training Epoch: 140 [36864/50000]\tLoss: 0.71\n","Training Epoch: 140 [37120/50000]\tLoss: 0.85\n","Training Epoch: 140 [37376/50000]\tLoss: 0.85\n","Training Epoch: 140 [37632/50000]\tLoss: 0.73\n","Training Epoch: 140 [37888/50000]\tLoss: 0.74\n","Training Epoch: 140 [38144/50000]\tLoss: 0.79\n","Training Epoch: 140 [38400/50000]\tLoss: 0.85\n","Training Epoch: 140 [38656/50000]\tLoss: 0.61\n","Training Epoch: 140 [38912/50000]\tLoss: 0.81\n","Training Epoch: 140 [39168/50000]\tLoss: 0.80\n","Training Epoch: 140 [39424/50000]\tLoss: 0.82\n","Training Epoch: 140 [39680/50000]\tLoss: 0.78\n","Training Epoch: 140 [39936/50000]\tLoss: 0.85\n","Training Epoch: 140 [40192/50000]\tLoss: 0.78\n","Training Epoch: 140 [40448/50000]\tLoss: 0.86\n","Training Epoch: 140 [40704/50000]\tLoss: 0.76\n","Training Epoch: 140 [40960/50000]\tLoss: 0.78\n","Training Epoch: 140 [41216/50000]\tLoss: 0.71\n","Training Epoch: 140 [41472/50000]\tLoss: 0.84\n","Training Epoch: 140 [41728/50000]\tLoss: 0.80\n","Training Epoch: 140 [41984/50000]\tLoss: 0.81\n","Training Epoch: 140 [42240/50000]\tLoss: 0.64\n","Training Epoch: 140 [42496/50000]\tLoss: 0.70\n","Training Epoch: 140 [42752/50000]\tLoss: 0.75\n","Training Epoch: 140 [43008/50000]\tLoss: 0.74\n","Training Epoch: 140 [43264/50000]\tLoss: 0.66\n","Training Epoch: 140 [43520/50000]\tLoss: 0.84\n","Training Epoch: 140 [43776/50000]\tLoss: 0.78\n","Training Epoch: 140 [44032/50000]\tLoss: 0.67\n","Training Epoch: 140 [44288/50000]\tLoss: 0.76\n","Training Epoch: 140 [44544/50000]\tLoss: 0.71\n","Training Epoch: 140 [44800/50000]\tLoss: 0.84\n","Training Epoch: 140 [45056/50000]\tLoss: 0.84\n","Training Epoch: 140 [45312/50000]\tLoss: 0.92\n","Training Epoch: 140 [45568/50000]\tLoss: 0.81\n","Training Epoch: 140 [45824/50000]\tLoss: 0.71\n","Training Epoch: 140 [46080/50000]\tLoss: 0.66\n","Training Epoch: 140 [46336/50000]\tLoss: 0.70\n","Training Epoch: 140 [46592/50000]\tLoss: 0.78\n","Training Epoch: 140 [46848/50000]\tLoss: 0.77\n","Training Epoch: 140 [47104/50000]\tLoss: 0.97\n","Training Epoch: 140 [47360/50000]\tLoss: 0.77\n","Training Epoch: 140 [47616/50000]\tLoss: 0.66\n","Training Epoch: 140 [47872/50000]\tLoss: 0.68\n","Training Epoch: 140 [48128/50000]\tLoss: 0.72\n","Training Epoch: 140 [48384/50000]\tLoss: 0.77\n","Training Epoch: 140 [48640/50000]\tLoss: 0.78\n","Training Epoch: 140 [48896/50000]\tLoss: 0.87\n","Training Epoch: 140 [49152/50000]\tLoss: 0.86\n","Training Epoch: 140 [49408/50000]\tLoss: 0.79\n","Training Epoch: 140 [49664/50000]\tLoss: 0.78\n","Training Epoch: 140 [49920/50000]\tLoss: 0.95\n","Training Epoch: 140 [50000/50000]\tLoss: 0.72\n","Time taken to train epoch 140: 27.10s\n","Testing Network for epoch:  140\n","Evaluation: Evaluation Time: 2.57s, Average loss: 0.0063, Accuracy: 0.6252, Recall: 0.6252, Precision: 0.6285\n","Training Epoch: 141 [256/50000]\tLoss: 0.89\n","Training Epoch: 141 [512/50000]\tLoss: 0.74\n","Training Epoch: 141 [768/50000]\tLoss: 0.83\n","Training Epoch: 141 [1024/50000]\tLoss: 0.89\n","Training Epoch: 141 [1280/50000]\tLoss: 0.78\n","Training Epoch: 141 [1536/50000]\tLoss: 0.77\n","Training Epoch: 141 [1792/50000]\tLoss: 0.67\n","Training Epoch: 141 [2048/50000]\tLoss: 0.81\n","Training Epoch: 141 [2304/50000]\tLoss: 0.81\n","Training Epoch: 141 [2560/50000]\tLoss: 0.68\n","Training Epoch: 141 [2816/50000]\tLoss: 0.69\n","Training Epoch: 141 [3072/50000]\tLoss: 0.76\n","Training Epoch: 141 [3328/50000]\tLoss: 0.80\n","Training Epoch: 141 [3584/50000]\tLoss: 0.78\n","Training Epoch: 141 [3840/50000]\tLoss: 0.78\n","Training Epoch: 141 [4096/50000]\tLoss: 0.72\n","Training Epoch: 141 [4352/50000]\tLoss: 0.87\n","Training Epoch: 141 [4608/50000]\tLoss: 0.79\n","Training Epoch: 141 [4864/50000]\tLoss: 0.73\n","Training Epoch: 141 [5120/50000]\tLoss: 0.66\n","Training Epoch: 141 [5376/50000]\tLoss: 0.74\n","Training Epoch: 141 [5632/50000]\tLoss: 0.75\n","Training Epoch: 141 [5888/50000]\tLoss: 0.75\n","Training Epoch: 141 [6144/50000]\tLoss: 0.78\n","Training Epoch: 141 [6400/50000]\tLoss: 0.76\n","Training Epoch: 141 [6656/50000]\tLoss: 0.64\n","Training Epoch: 141 [6912/50000]\tLoss: 0.71\n","Training Epoch: 141 [7168/50000]\tLoss: 0.77\n","Training Epoch: 141 [7424/50000]\tLoss: 0.66\n","Training Epoch: 141 [7680/50000]\tLoss: 0.78\n","Training Epoch: 141 [7936/50000]\tLoss: 0.72\n","Training Epoch: 141 [8192/50000]\tLoss: 0.81\n","Training Epoch: 141 [8448/50000]\tLoss: 0.77\n","Training Epoch: 141 [8704/50000]\tLoss: 0.86\n","Training Epoch: 141 [8960/50000]\tLoss: 0.69\n","Training Epoch: 141 [9216/50000]\tLoss: 0.82\n","Training Epoch: 141 [9472/50000]\tLoss: 0.72\n","Training Epoch: 141 [9728/50000]\tLoss: 0.79\n","Training Epoch: 141 [9984/50000]\tLoss: 0.57\n","Training Epoch: 141 [10240/50000]\tLoss: 0.69\n","Training Epoch: 141 [10496/50000]\tLoss: 0.82\n","Training Epoch: 141 [10752/50000]\tLoss: 0.67\n","Training Epoch: 141 [11008/50000]\tLoss: 0.74\n","Training Epoch: 141 [11264/50000]\tLoss: 0.85\n","Training Epoch: 141 [11520/50000]\tLoss: 0.92\n","Training Epoch: 141 [11776/50000]\tLoss: 0.82\n","Training Epoch: 141 [12032/50000]\tLoss: 0.84\n","Training Epoch: 141 [12288/50000]\tLoss: 0.79\n","Training Epoch: 141 [12544/50000]\tLoss: 0.65\n","Training Epoch: 141 [12800/50000]\tLoss: 0.81\n","Training Epoch: 141 [13056/50000]\tLoss: 0.92\n","Training Epoch: 141 [13312/50000]\tLoss: 0.75\n","Training Epoch: 141 [13568/50000]\tLoss: 0.78\n","Training Epoch: 141 [13824/50000]\tLoss: 0.95\n","Training Epoch: 141 [14080/50000]\tLoss: 0.84\n","Training Epoch: 141 [14336/50000]\tLoss: 0.73\n","Training Epoch: 141 [14592/50000]\tLoss: 0.75\n","Training Epoch: 141 [14848/50000]\tLoss: 0.64\n","Training Epoch: 141 [15104/50000]\tLoss: 0.69\n","Training Epoch: 141 [15360/50000]\tLoss: 0.68\n","Training Epoch: 141 [15616/50000]\tLoss: 0.73\n","Training Epoch: 141 [15872/50000]\tLoss: 0.74\n","Training Epoch: 141 [16128/50000]\tLoss: 0.68\n","Training Epoch: 141 [16384/50000]\tLoss: 0.80\n","Training Epoch: 141 [16640/50000]\tLoss: 0.84\n","Training Epoch: 141 [16896/50000]\tLoss: 0.83\n","Training Epoch: 141 [17152/50000]\tLoss: 0.82\n","Training Epoch: 141 [17408/50000]\tLoss: 0.81\n","Training Epoch: 141 [17664/50000]\tLoss: 0.77\n","Training Epoch: 141 [17920/50000]\tLoss: 0.80\n","Training Epoch: 141 [18176/50000]\tLoss: 0.86\n","Training Epoch: 141 [18432/50000]\tLoss: 0.78\n","Training Epoch: 141 [18688/50000]\tLoss: 0.69\n","Training Epoch: 141 [18944/50000]\tLoss: 0.74\n","Training Epoch: 141 [19200/50000]\tLoss: 0.78\n","Training Epoch: 141 [19456/50000]\tLoss: 0.54\n","Training Epoch: 141 [19712/50000]\tLoss: 0.89\n","Training Epoch: 141 [19968/50000]\tLoss: 0.85\n","Training Epoch: 141 [20224/50000]\tLoss: 0.84\n","Training Epoch: 141 [20480/50000]\tLoss: 1.08\n","Training Epoch: 141 [20736/50000]\tLoss: 0.81\n","Training Epoch: 141 [20992/50000]\tLoss: 0.89\n","Training Epoch: 141 [21248/50000]\tLoss: 0.77\n","Training Epoch: 141 [21504/50000]\tLoss: 0.70\n","Training Epoch: 141 [21760/50000]\tLoss: 0.83\n","Training Epoch: 141 [22016/50000]\tLoss: 0.79\n","Training Epoch: 141 [22272/50000]\tLoss: 0.71\n","Training Epoch: 141 [22528/50000]\tLoss: 0.73\n","Training Epoch: 141 [22784/50000]\tLoss: 0.70\n","Training Epoch: 141 [23040/50000]\tLoss: 0.80\n","Training Epoch: 141 [23296/50000]\tLoss: 0.83\n","Training Epoch: 141 [23552/50000]\tLoss: 0.80\n","Training Epoch: 141 [23808/50000]\tLoss: 0.92\n","Training Epoch: 141 [24064/50000]\tLoss: 0.80\n","Training Epoch: 141 [24320/50000]\tLoss: 0.76\n","Training Epoch: 141 [24576/50000]\tLoss: 0.67\n","Training Epoch: 141 [24832/50000]\tLoss: 0.75\n","Training Epoch: 141 [25088/50000]\tLoss: 0.81\n","Training Epoch: 141 [25344/50000]\tLoss: 0.67\n","Training Epoch: 141 [25600/50000]\tLoss: 0.76\n","Training Epoch: 141 [25856/50000]\tLoss: 0.74\n","Training Epoch: 141 [26112/50000]\tLoss: 0.86\n","Training Epoch: 141 [26368/50000]\tLoss: 0.86\n","Training Epoch: 141 [26624/50000]\tLoss: 0.78\n","Training Epoch: 141 [26880/50000]\tLoss: 0.75\n","Training Epoch: 141 [27136/50000]\tLoss: 0.70\n","Training Epoch: 141 [27392/50000]\tLoss: 0.62\n","Training Epoch: 141 [27648/50000]\tLoss: 0.84\n","Training Epoch: 141 [27904/50000]\tLoss: 0.83\n","Training Epoch: 141 [28160/50000]\tLoss: 0.62\n","Training Epoch: 141 [28416/50000]\tLoss: 0.81\n","Training Epoch: 141 [28672/50000]\tLoss: 0.74\n","Training Epoch: 141 [28928/50000]\tLoss: 0.72\n","Training Epoch: 141 [29184/50000]\tLoss: 0.80\n","Training Epoch: 141 [29440/50000]\tLoss: 0.72\n","Training Epoch: 141 [29696/50000]\tLoss: 0.78\n","Training Epoch: 141 [29952/50000]\tLoss: 0.79\n","Training Epoch: 141 [30208/50000]\tLoss: 0.62\n","Training Epoch: 141 [30464/50000]\tLoss: 0.82\n","Training Epoch: 141 [30720/50000]\tLoss: 0.74\n","Training Epoch: 141 [30976/50000]\tLoss: 0.84\n","Training Epoch: 141 [31232/50000]\tLoss: 0.87\n","Training Epoch: 141 [31488/50000]\tLoss: 0.67\n","Training Epoch: 141 [31744/50000]\tLoss: 0.80\n","Training Epoch: 141 [32000/50000]\tLoss: 0.71\n","Training Epoch: 141 [32256/50000]\tLoss: 0.80\n","Training Epoch: 141 [32512/50000]\tLoss: 0.86\n","Training Epoch: 141 [32768/50000]\tLoss: 0.62\n","Training Epoch: 141 [33024/50000]\tLoss: 0.84\n","Training Epoch: 141 [33280/50000]\tLoss: 0.69\n","Training Epoch: 141 [33536/50000]\tLoss: 0.78\n","Training Epoch: 141 [33792/50000]\tLoss: 0.77\n","Training Epoch: 141 [34048/50000]\tLoss: 0.66\n","Training Epoch: 141 [34304/50000]\tLoss: 0.80\n","Training Epoch: 141 [34560/50000]\tLoss: 0.73\n","Training Epoch: 141 [34816/50000]\tLoss: 0.77\n","Training Epoch: 141 [35072/50000]\tLoss: 0.92\n","Training Epoch: 141 [35328/50000]\tLoss: 0.80\n","Training Epoch: 141 [35584/50000]\tLoss: 0.73\n","Training Epoch: 141 [35840/50000]\tLoss: 0.77\n","Training Epoch: 141 [36096/50000]\tLoss: 0.69\n","Training Epoch: 141 [36352/50000]\tLoss: 0.78\n","Training Epoch: 141 [36608/50000]\tLoss: 0.79\n","Training Epoch: 141 [36864/50000]\tLoss: 0.75\n","Training Epoch: 141 [37120/50000]\tLoss: 0.79\n","Training Epoch: 141 [37376/50000]\tLoss: 0.81\n","Training Epoch: 141 [37632/50000]\tLoss: 0.81\n","Training Epoch: 141 [37888/50000]\tLoss: 0.75\n","Training Epoch: 141 [38144/50000]\tLoss: 0.75\n","Training Epoch: 141 [38400/50000]\tLoss: 0.76\n","Training Epoch: 141 [38656/50000]\tLoss: 0.81\n","Training Epoch: 141 [38912/50000]\tLoss: 0.66\n","Training Epoch: 141 [39168/50000]\tLoss: 0.97\n","Training Epoch: 141 [39424/50000]\tLoss: 0.80\n","Training Epoch: 141 [39680/50000]\tLoss: 0.75\n","Training Epoch: 141 [39936/50000]\tLoss: 0.83\n","Training Epoch: 141 [40192/50000]\tLoss: 0.76\n","Training Epoch: 141 [40448/50000]\tLoss: 0.71\n","Training Epoch: 141 [40704/50000]\tLoss: 0.86\n","Training Epoch: 141 [40960/50000]\tLoss: 0.66\n","Training Epoch: 141 [41216/50000]\tLoss: 0.77\n","Training Epoch: 141 [41472/50000]\tLoss: 0.90\n","Training Epoch: 141 [41728/50000]\tLoss: 0.78\n","Training Epoch: 141 [41984/50000]\tLoss: 0.91\n","Training Epoch: 141 [42240/50000]\tLoss: 0.89\n","Training Epoch: 141 [42496/50000]\tLoss: 0.78\n","Training Epoch: 141 [42752/50000]\tLoss: 0.72\n","Training Epoch: 141 [43008/50000]\tLoss: 0.88\n","Training Epoch: 141 [43264/50000]\tLoss: 0.81\n","Training Epoch: 141 [43520/50000]\tLoss: 0.72\n","Training Epoch: 141 [43776/50000]\tLoss: 0.72\n","Training Epoch: 141 [44032/50000]\tLoss: 0.80\n","Training Epoch: 141 [44288/50000]\tLoss: 0.70\n","Training Epoch: 141 [44544/50000]\tLoss: 0.70\n","Training Epoch: 141 [44800/50000]\tLoss: 0.57\n","Training Epoch: 141 [45056/50000]\tLoss: 0.73\n","Training Epoch: 141 [45312/50000]\tLoss: 0.94\n","Training Epoch: 141 [45568/50000]\tLoss: 0.83\n","Training Epoch: 141 [45824/50000]\tLoss: 0.73\n","Training Epoch: 141 [46080/50000]\tLoss: 0.81\n","Training Epoch: 141 [46336/50000]\tLoss: 0.79\n","Training Epoch: 141 [46592/50000]\tLoss: 0.71\n","Training Epoch: 141 [46848/50000]\tLoss: 0.70\n","Training Epoch: 141 [47104/50000]\tLoss: 0.65\n","Training Epoch: 141 [47360/50000]\tLoss: 0.74\n","Training Epoch: 141 [47616/50000]\tLoss: 0.81\n","Training Epoch: 141 [47872/50000]\tLoss: 0.82\n","Training Epoch: 141 [48128/50000]\tLoss: 0.70\n","Training Epoch: 141 [48384/50000]\tLoss: 0.81\n","Training Epoch: 141 [48640/50000]\tLoss: 0.72\n","Training Epoch: 141 [48896/50000]\tLoss: 0.69\n","Training Epoch: 141 [49152/50000]\tLoss: 0.86\n","Training Epoch: 141 [49408/50000]\tLoss: 0.64\n","Training Epoch: 141 [49664/50000]\tLoss: 0.70\n","Training Epoch: 141 [49920/50000]\tLoss: 0.67\n","Training Epoch: 141 [50000/50000]\tLoss: 0.82\n","Time taken to train epoch 141: 27.30s\n","Testing Network for epoch:  141\n","Evaluation: Evaluation Time: 2.57s, Average loss: 0.0063, Accuracy: 0.6244, Recall: 0.6244, Precision: 0.6282\n","Training Epoch: 142 [256/50000]\tLoss: 0.79\n","Training Epoch: 142 [512/50000]\tLoss: 0.72\n","Training Epoch: 142 [768/50000]\tLoss: 0.85\n","Training Epoch: 142 [1024/50000]\tLoss: 0.85\n","Training Epoch: 142 [1280/50000]\tLoss: 0.62\n","Training Epoch: 142 [1536/50000]\tLoss: 0.75\n","Training Epoch: 142 [1792/50000]\tLoss: 0.71\n","Training Epoch: 142 [2048/50000]\tLoss: 0.79\n","Training Epoch: 142 [2304/50000]\tLoss: 0.61\n","Training Epoch: 142 [2560/50000]\tLoss: 0.72\n","Training Epoch: 142 [2816/50000]\tLoss: 0.71\n","Training Epoch: 142 [3072/50000]\tLoss: 0.83\n","Training Epoch: 142 [3328/50000]\tLoss: 0.68\n","Training Epoch: 142 [3584/50000]\tLoss: 0.77\n","Training Epoch: 142 [3840/50000]\tLoss: 0.77\n","Training Epoch: 142 [4096/50000]\tLoss: 0.89\n","Training Epoch: 142 [4352/50000]\tLoss: 0.68\n","Training Epoch: 142 [4608/50000]\tLoss: 0.76\n","Training Epoch: 142 [4864/50000]\tLoss: 0.72\n","Training Epoch: 142 [5120/50000]\tLoss: 0.88\n","Training Epoch: 142 [5376/50000]\tLoss: 0.67\n","Training Epoch: 142 [5632/50000]\tLoss: 0.74\n","Training Epoch: 142 [5888/50000]\tLoss: 0.59\n","Training Epoch: 142 [6144/50000]\tLoss: 0.72\n","Training Epoch: 142 [6400/50000]\tLoss: 0.69\n","Training Epoch: 142 [6656/50000]\tLoss: 0.76\n","Training Epoch: 142 [6912/50000]\tLoss: 0.80\n","Training Epoch: 142 [7168/50000]\tLoss: 0.65\n","Training Epoch: 142 [7424/50000]\tLoss: 0.77\n","Training Epoch: 142 [7680/50000]\tLoss: 0.63\n","Training Epoch: 142 [7936/50000]\tLoss: 0.78\n","Training Epoch: 142 [8192/50000]\tLoss: 0.80\n","Training Epoch: 142 [8448/50000]\tLoss: 0.80\n","Training Epoch: 142 [8704/50000]\tLoss: 0.73\n","Training Epoch: 142 [8960/50000]\tLoss: 0.77\n","Training Epoch: 142 [9216/50000]\tLoss: 0.70\n","Training Epoch: 142 [9472/50000]\tLoss: 0.65\n","Training Epoch: 142 [9728/50000]\tLoss: 0.78\n","Training Epoch: 142 [9984/50000]\tLoss: 0.69\n","Training Epoch: 142 [10240/50000]\tLoss: 0.68\n","Training Epoch: 142 [10496/50000]\tLoss: 0.75\n","Training Epoch: 142 [10752/50000]\tLoss: 0.85\n","Training Epoch: 142 [11008/50000]\tLoss: 0.81\n","Training Epoch: 142 [11264/50000]\tLoss: 0.88\n","Training Epoch: 142 [11520/50000]\tLoss: 0.71\n","Training Epoch: 142 [11776/50000]\tLoss: 0.77\n","Training Epoch: 142 [12032/50000]\tLoss: 0.72\n","Training Epoch: 142 [12288/50000]\tLoss: 0.82\n","Training Epoch: 142 [12544/50000]\tLoss: 0.88\n","Training Epoch: 142 [12800/50000]\tLoss: 0.72\n","Training Epoch: 142 [13056/50000]\tLoss: 0.72\n","Training Epoch: 142 [13312/50000]\tLoss: 0.73\n","Training Epoch: 142 [13568/50000]\tLoss: 0.66\n","Training Epoch: 142 [13824/50000]\tLoss: 0.71\n","Training Epoch: 142 [14080/50000]\tLoss: 0.86\n","Training Epoch: 142 [14336/50000]\tLoss: 0.72\n","Training Epoch: 142 [14592/50000]\tLoss: 0.78\n","Training Epoch: 142 [14848/50000]\tLoss: 0.74\n","Training Epoch: 142 [15104/50000]\tLoss: 0.63\n","Training Epoch: 142 [15360/50000]\tLoss: 0.91\n","Training Epoch: 142 [15616/50000]\tLoss: 0.74\n","Training Epoch: 142 [15872/50000]\tLoss: 0.63\n","Training Epoch: 142 [16128/50000]\tLoss: 0.93\n","Training Epoch: 142 [16384/50000]\tLoss: 0.70\n","Training Epoch: 142 [16640/50000]\tLoss: 0.78\n","Training Epoch: 142 [16896/50000]\tLoss: 0.76\n","Training Epoch: 142 [17152/50000]\tLoss: 0.77\n","Training Epoch: 142 [17408/50000]\tLoss: 0.79\n","Training Epoch: 142 [17664/50000]\tLoss: 0.71\n","Training Epoch: 142 [17920/50000]\tLoss: 0.64\n","Training Epoch: 142 [18176/50000]\tLoss: 0.84\n","Training Epoch: 142 [18432/50000]\tLoss: 0.89\n","Training Epoch: 142 [18688/50000]\tLoss: 0.78\n","Training Epoch: 142 [18944/50000]\tLoss: 0.72\n","Training Epoch: 142 [19200/50000]\tLoss: 0.63\n","Training Epoch: 142 [19456/50000]\tLoss: 0.82\n","Training Epoch: 142 [19712/50000]\tLoss: 0.89\n","Training Epoch: 142 [19968/50000]\tLoss: 0.67\n","Training Epoch: 142 [20224/50000]\tLoss: 0.81\n","Training Epoch: 142 [20480/50000]\tLoss: 0.80\n","Training Epoch: 142 [20736/50000]\tLoss: 0.78\n","Training Epoch: 142 [20992/50000]\tLoss: 0.90\n","Training Epoch: 142 [21248/50000]\tLoss: 0.69\n","Training Epoch: 142 [21504/50000]\tLoss: 0.74\n","Training Epoch: 142 [21760/50000]\tLoss: 0.74\n","Training Epoch: 142 [22016/50000]\tLoss: 0.76\n","Training Epoch: 142 [22272/50000]\tLoss: 0.68\n","Training Epoch: 142 [22528/50000]\tLoss: 0.83\n","Training Epoch: 142 [22784/50000]\tLoss: 0.85\n","Training Epoch: 142 [23040/50000]\tLoss: 0.68\n","Training Epoch: 142 [23296/50000]\tLoss: 0.79\n","Training Epoch: 142 [23552/50000]\tLoss: 0.69\n","Training Epoch: 142 [23808/50000]\tLoss: 0.71\n","Training Epoch: 142 [24064/50000]\tLoss: 0.73\n","Training Epoch: 142 [24320/50000]\tLoss: 0.87\n","Training Epoch: 142 [24576/50000]\tLoss: 0.76\n","Training Epoch: 142 [24832/50000]\tLoss: 0.70\n","Training Epoch: 142 [25088/50000]\tLoss: 0.65\n","Training Epoch: 142 [25344/50000]\tLoss: 0.92\n","Training Epoch: 142 [25600/50000]\tLoss: 0.79\n","Training Epoch: 142 [25856/50000]\tLoss: 0.71\n","Training Epoch: 142 [26112/50000]\tLoss: 0.71\n","Training Epoch: 142 [26368/50000]\tLoss: 0.81\n","Training Epoch: 142 [26624/50000]\tLoss: 0.76\n","Training Epoch: 142 [26880/50000]\tLoss: 0.68\n","Training Epoch: 142 [27136/50000]\tLoss: 0.69\n","Training Epoch: 142 [27392/50000]\tLoss: 0.73\n","Training Epoch: 142 [27648/50000]\tLoss: 1.00\n","Training Epoch: 142 [27904/50000]\tLoss: 0.80\n","Training Epoch: 142 [28160/50000]\tLoss: 0.83\n","Training Epoch: 142 [28416/50000]\tLoss: 0.88\n","Training Epoch: 142 [28672/50000]\tLoss: 0.71\n","Training Epoch: 142 [28928/50000]\tLoss: 0.82\n","Training Epoch: 142 [29184/50000]\tLoss: 0.64\n","Training Epoch: 142 [29440/50000]\tLoss: 0.67\n","Training Epoch: 142 [29696/50000]\tLoss: 0.73\n","Training Epoch: 142 [29952/50000]\tLoss: 0.90\n","Training Epoch: 142 [30208/50000]\tLoss: 0.75\n","Training Epoch: 142 [30464/50000]\tLoss: 0.73\n","Training Epoch: 142 [30720/50000]\tLoss: 0.68\n","Training Epoch: 142 [30976/50000]\tLoss: 0.66\n","Training Epoch: 142 [31232/50000]\tLoss: 0.89\n","Training Epoch: 142 [31488/50000]\tLoss: 0.73\n","Training Epoch: 142 [31744/50000]\tLoss: 0.74\n","Training Epoch: 142 [32000/50000]\tLoss: 0.74\n","Training Epoch: 142 [32256/50000]\tLoss: 0.71\n","Training Epoch: 142 [32512/50000]\tLoss: 0.83\n","Training Epoch: 142 [32768/50000]\tLoss: 0.69\n","Training Epoch: 142 [33024/50000]\tLoss: 0.84\n","Training Epoch: 142 [33280/50000]\tLoss: 0.84\n","Training Epoch: 142 [33536/50000]\tLoss: 0.88\n","Training Epoch: 142 [33792/50000]\tLoss: 0.87\n","Training Epoch: 142 [34048/50000]\tLoss: 0.78\n","Training Epoch: 142 [34304/50000]\tLoss: 0.89\n","Training Epoch: 142 [34560/50000]\tLoss: 0.84\n","Training Epoch: 142 [34816/50000]\tLoss: 0.72\n","Training Epoch: 142 [35072/50000]\tLoss: 0.76\n","Training Epoch: 142 [35328/50000]\tLoss: 0.78\n","Training Epoch: 142 [35584/50000]\tLoss: 0.78\n","Training Epoch: 142 [35840/50000]\tLoss: 0.70\n","Training Epoch: 142 [36096/50000]\tLoss: 0.80\n","Training Epoch: 142 [36352/50000]\tLoss: 0.78\n","Training Epoch: 142 [36608/50000]\tLoss: 0.73\n","Training Epoch: 142 [36864/50000]\tLoss: 0.62\n","Training Epoch: 142 [37120/50000]\tLoss: 0.81\n","Training Epoch: 142 [37376/50000]\tLoss: 0.85\n","Training Epoch: 142 [37632/50000]\tLoss: 0.80\n","Training Epoch: 142 [37888/50000]\tLoss: 0.87\n","Training Epoch: 142 [38144/50000]\tLoss: 0.74\n","Training Epoch: 142 [38400/50000]\tLoss: 0.70\n","Training Epoch: 142 [38656/50000]\tLoss: 0.80\n","Training Epoch: 142 [38912/50000]\tLoss: 0.89\n","Training Epoch: 142 [39168/50000]\tLoss: 0.88\n","Training Epoch: 142 [39424/50000]\tLoss: 0.72\n","Training Epoch: 142 [39680/50000]\tLoss: 0.70\n","Training Epoch: 142 [39936/50000]\tLoss: 0.74\n","Training Epoch: 142 [40192/50000]\tLoss: 0.74\n","Training Epoch: 142 [40448/50000]\tLoss: 0.76\n","Training Epoch: 142 [40704/50000]\tLoss: 0.75\n","Training Epoch: 142 [40960/50000]\tLoss: 0.73\n","Training Epoch: 142 [41216/50000]\tLoss: 0.80\n","Training Epoch: 142 [41472/50000]\tLoss: 0.77\n","Training Epoch: 142 [41728/50000]\tLoss: 0.67\n","Training Epoch: 142 [41984/50000]\tLoss: 0.85\n","Training Epoch: 142 [42240/50000]\tLoss: 0.90\n","Training Epoch: 142 [42496/50000]\tLoss: 0.91\n","Training Epoch: 142 [42752/50000]\tLoss: 0.76\n","Training Epoch: 142 [43008/50000]\tLoss: 0.77\n","Training Epoch: 142 [43264/50000]\tLoss: 0.68\n","Training Epoch: 142 [43520/50000]\tLoss: 0.90\n","Training Epoch: 142 [43776/50000]\tLoss: 0.88\n","Training Epoch: 142 [44032/50000]\tLoss: 0.74\n","Training Epoch: 142 [44288/50000]\tLoss: 0.78\n","Training Epoch: 142 [44544/50000]\tLoss: 0.63\n","Training Epoch: 142 [44800/50000]\tLoss: 0.76\n","Training Epoch: 142 [45056/50000]\tLoss: 0.75\n","Training Epoch: 142 [45312/50000]\tLoss: 0.65\n","Training Epoch: 142 [45568/50000]\tLoss: 0.82\n","Training Epoch: 142 [45824/50000]\tLoss: 0.63\n","Training Epoch: 142 [46080/50000]\tLoss: 0.90\n","Training Epoch: 142 [46336/50000]\tLoss: 0.74\n","Training Epoch: 142 [46592/50000]\tLoss: 0.83\n","Training Epoch: 142 [46848/50000]\tLoss: 0.66\n","Training Epoch: 142 [47104/50000]\tLoss: 0.75\n","Training Epoch: 142 [47360/50000]\tLoss: 0.76\n","Training Epoch: 142 [47616/50000]\tLoss: 0.82\n","Training Epoch: 142 [47872/50000]\tLoss: 0.67\n","Training Epoch: 142 [48128/50000]\tLoss: 0.80\n","Training Epoch: 142 [48384/50000]\tLoss: 0.66\n","Training Epoch: 142 [48640/50000]\tLoss: 0.77\n","Training Epoch: 142 [48896/50000]\tLoss: 0.67\n","Training Epoch: 142 [49152/50000]\tLoss: 0.72\n","Training Epoch: 142 [49408/50000]\tLoss: 0.91\n","Training Epoch: 142 [49664/50000]\tLoss: 0.77\n","Training Epoch: 142 [49920/50000]\tLoss: 0.84\n","Training Epoch: 142 [50000/50000]\tLoss: 0.73\n","Time taken to train epoch 142: 27.18s\n","Testing Network for epoch:  142\n","Evaluation: Evaluation Time: 2.58s, Average loss: 0.0064, Accuracy: 0.6206, Recall: 0.6206, Precision: 0.6246\n","Training Epoch: 143 [256/50000]\tLoss: 0.84\n","Training Epoch: 143 [512/50000]\tLoss: 0.76\n","Training Epoch: 143 [768/50000]\tLoss: 0.77\n","Training Epoch: 143 [1024/50000]\tLoss: 0.82\n","Training Epoch: 143 [1280/50000]\tLoss: 0.82\n","Training Epoch: 143 [1536/50000]\tLoss: 0.82\n","Training Epoch: 143 [1792/50000]\tLoss: 0.82\n","Training Epoch: 143 [2048/50000]\tLoss: 0.86\n","Training Epoch: 143 [2304/50000]\tLoss: 0.61\n","Training Epoch: 143 [2560/50000]\tLoss: 0.74\n","Training Epoch: 143 [2816/50000]\tLoss: 0.77\n","Training Epoch: 143 [3072/50000]\tLoss: 0.75\n","Training Epoch: 143 [3328/50000]\tLoss: 0.71\n","Training Epoch: 143 [3584/50000]\tLoss: 0.72\n","Training Epoch: 143 [3840/50000]\tLoss: 0.64\n","Training Epoch: 143 [4096/50000]\tLoss: 0.73\n","Training Epoch: 143 [4352/50000]\tLoss: 0.79\n","Training Epoch: 143 [4608/50000]\tLoss: 0.78\n","Training Epoch: 143 [4864/50000]\tLoss: 0.78\n","Training Epoch: 143 [5120/50000]\tLoss: 0.72\n","Training Epoch: 143 [5376/50000]\tLoss: 0.71\n","Training Epoch: 143 [5632/50000]\tLoss: 0.70\n","Training Epoch: 143 [5888/50000]\tLoss: 0.59\n","Training Epoch: 143 [6144/50000]\tLoss: 0.76\n","Training Epoch: 143 [6400/50000]\tLoss: 0.67\n","Training Epoch: 143 [6656/50000]\tLoss: 0.91\n","Training Epoch: 143 [6912/50000]\tLoss: 0.72\n","Training Epoch: 143 [7168/50000]\tLoss: 0.79\n","Training Epoch: 143 [7424/50000]\tLoss: 0.70\n","Training Epoch: 143 [7680/50000]\tLoss: 0.74\n","Training Epoch: 143 [7936/50000]\tLoss: 0.87\n","Training Epoch: 143 [8192/50000]\tLoss: 0.65\n","Training Epoch: 143 [8448/50000]\tLoss: 0.98\n","Training Epoch: 143 [8704/50000]\tLoss: 0.66\n","Training Epoch: 143 [8960/50000]\tLoss: 0.73\n","Training Epoch: 143 [9216/50000]\tLoss: 0.60\n","Training Epoch: 143 [9472/50000]\tLoss: 0.72\n","Training Epoch: 143 [9728/50000]\tLoss: 0.73\n","Training Epoch: 143 [9984/50000]\tLoss: 0.70\n","Training Epoch: 143 [10240/50000]\tLoss: 0.75\n","Training Epoch: 143 [10496/50000]\tLoss: 0.76\n","Training Epoch: 143 [10752/50000]\tLoss: 0.67\n","Training Epoch: 143 [11008/50000]\tLoss: 0.79\n","Training Epoch: 143 [11264/50000]\tLoss: 0.68\n","Training Epoch: 143 [11520/50000]\tLoss: 0.76\n","Training Epoch: 143 [11776/50000]\tLoss: 0.78\n","Training Epoch: 143 [12032/50000]\tLoss: 0.72\n","Training Epoch: 143 [12288/50000]\tLoss: 0.85\n","Training Epoch: 143 [12544/50000]\tLoss: 0.61\n","Training Epoch: 143 [12800/50000]\tLoss: 0.69\n","Training Epoch: 143 [13056/50000]\tLoss: 0.75\n","Training Epoch: 143 [13312/50000]\tLoss: 0.72\n","Training Epoch: 143 [13568/50000]\tLoss: 0.64\n","Training Epoch: 143 [13824/50000]\tLoss: 0.80\n","Training Epoch: 143 [14080/50000]\tLoss: 0.70\n","Training Epoch: 143 [14336/50000]\tLoss: 0.73\n","Training Epoch: 143 [14592/50000]\tLoss: 0.79\n","Training Epoch: 143 [14848/50000]\tLoss: 0.79\n","Training Epoch: 143 [15104/50000]\tLoss: 0.80\n","Training Epoch: 143 [15360/50000]\tLoss: 0.72\n","Training Epoch: 143 [15616/50000]\tLoss: 0.75\n","Training Epoch: 143 [15872/50000]\tLoss: 0.80\n","Training Epoch: 143 [16128/50000]\tLoss: 0.76\n","Training Epoch: 143 [16384/50000]\tLoss: 0.72\n","Training Epoch: 143 [16640/50000]\tLoss: 0.92\n","Training Epoch: 143 [16896/50000]\tLoss: 0.82\n","Training Epoch: 143 [17152/50000]\tLoss: 0.83\n","Training Epoch: 143 [17408/50000]\tLoss: 0.83\n","Training Epoch: 143 [17664/50000]\tLoss: 0.67\n","Training Epoch: 143 [17920/50000]\tLoss: 0.73\n","Training Epoch: 143 [18176/50000]\tLoss: 0.70\n","Training Epoch: 143 [18432/50000]\tLoss: 0.78\n","Training Epoch: 143 [18688/50000]\tLoss: 0.62\n","Training Epoch: 143 [18944/50000]\tLoss: 0.72\n","Training Epoch: 143 [19200/50000]\tLoss: 0.63\n","Training Epoch: 143 [19456/50000]\tLoss: 0.74\n","Training Epoch: 143 [19712/50000]\tLoss: 0.84\n","Training Epoch: 143 [19968/50000]\tLoss: 0.78\n","Training Epoch: 143 [20224/50000]\tLoss: 0.85\n","Training Epoch: 143 [20480/50000]\tLoss: 0.62\n","Training Epoch: 143 [20736/50000]\tLoss: 0.79\n","Training Epoch: 143 [20992/50000]\tLoss: 0.80\n","Training Epoch: 143 [21248/50000]\tLoss: 0.72\n","Training Epoch: 143 [21504/50000]\tLoss: 0.90\n","Training Epoch: 143 [21760/50000]\tLoss: 0.80\n","Training Epoch: 143 [22016/50000]\tLoss: 0.76\n","Training Epoch: 143 [22272/50000]\tLoss: 0.84\n","Training Epoch: 143 [22528/50000]\tLoss: 0.75\n","Training Epoch: 143 [22784/50000]\tLoss: 0.93\n","Training Epoch: 143 [23040/50000]\tLoss: 0.70\n","Training Epoch: 143 [23296/50000]\tLoss: 0.79\n","Training Epoch: 143 [23552/50000]\tLoss: 0.77\n","Training Epoch: 143 [23808/50000]\tLoss: 0.79\n","Training Epoch: 143 [24064/50000]\tLoss: 0.68\n","Training Epoch: 143 [24320/50000]\tLoss: 0.72\n","Training Epoch: 143 [24576/50000]\tLoss: 0.70\n","Training Epoch: 143 [24832/50000]\tLoss: 0.74\n","Training Epoch: 143 [25088/50000]\tLoss: 0.73\n","Training Epoch: 143 [25344/50000]\tLoss: 0.88\n","Training Epoch: 143 [25600/50000]\tLoss: 0.76\n","Training Epoch: 143 [25856/50000]\tLoss: 0.82\n","Training Epoch: 143 [26112/50000]\tLoss: 0.87\n","Training Epoch: 143 [26368/50000]\tLoss: 0.63\n","Training Epoch: 143 [26624/50000]\tLoss: 0.75\n","Training Epoch: 143 [26880/50000]\tLoss: 0.66\n","Training Epoch: 143 [27136/50000]\tLoss: 0.81\n","Training Epoch: 143 [27392/50000]\tLoss: 0.75\n","Training Epoch: 143 [27648/50000]\tLoss: 0.71\n","Training Epoch: 143 [27904/50000]\tLoss: 0.76\n","Training Epoch: 143 [28160/50000]\tLoss: 0.65\n","Training Epoch: 143 [28416/50000]\tLoss: 0.76\n","Training Epoch: 143 [28672/50000]\tLoss: 0.70\n","Training Epoch: 143 [28928/50000]\tLoss: 0.71\n","Training Epoch: 143 [29184/50000]\tLoss: 0.71\n","Training Epoch: 143 [29440/50000]\tLoss: 0.70\n","Training Epoch: 143 [29696/50000]\tLoss: 0.69\n","Training Epoch: 143 [29952/50000]\tLoss: 0.67\n","Training Epoch: 143 [30208/50000]\tLoss: 0.65\n","Training Epoch: 143 [30464/50000]\tLoss: 0.79\n","Training Epoch: 143 [30720/50000]\tLoss: 0.67\n","Training Epoch: 143 [30976/50000]\tLoss: 0.80\n","Training Epoch: 143 [31232/50000]\tLoss: 0.75\n","Training Epoch: 143 [31488/50000]\tLoss: 0.79\n","Training Epoch: 143 [31744/50000]\tLoss: 0.70\n","Training Epoch: 143 [32000/50000]\tLoss: 0.78\n","Training Epoch: 143 [32256/50000]\tLoss: 0.74\n","Training Epoch: 143 [32512/50000]\tLoss: 0.70\n","Training Epoch: 143 [32768/50000]\tLoss: 0.81\n","Training Epoch: 143 [33024/50000]\tLoss: 0.61\n","Training Epoch: 143 [33280/50000]\tLoss: 0.82\n","Training Epoch: 143 [33536/50000]\tLoss: 0.73\n","Training Epoch: 143 [33792/50000]\tLoss: 0.74\n","Training Epoch: 143 [34048/50000]\tLoss: 0.75\n","Training Epoch: 143 [34304/50000]\tLoss: 0.62\n","Training Epoch: 143 [34560/50000]\tLoss: 0.71\n","Training Epoch: 143 [34816/50000]\tLoss: 0.71\n","Training Epoch: 143 [35072/50000]\tLoss: 0.75\n","Training Epoch: 143 [35328/50000]\tLoss: 0.82\n","Training Epoch: 143 [35584/50000]\tLoss: 1.01\n","Training Epoch: 143 [35840/50000]\tLoss: 0.73\n","Training Epoch: 143 [36096/50000]\tLoss: 0.84\n","Training Epoch: 143 [36352/50000]\tLoss: 0.84\n","Training Epoch: 143 [36608/50000]\tLoss: 0.73\n","Training Epoch: 143 [36864/50000]\tLoss: 0.72\n","Training Epoch: 143 [37120/50000]\tLoss: 0.79\n","Training Epoch: 143 [37376/50000]\tLoss: 0.80\n","Training Epoch: 143 [37632/50000]\tLoss: 0.75\n","Training Epoch: 143 [37888/50000]\tLoss: 0.82\n","Training Epoch: 143 [38144/50000]\tLoss: 0.79\n","Training Epoch: 143 [38400/50000]\tLoss: 0.65\n","Training Epoch: 143 [38656/50000]\tLoss: 0.74\n","Training Epoch: 143 [38912/50000]\tLoss: 0.68\n","Training Epoch: 143 [39168/50000]\tLoss: 0.92\n","Training Epoch: 143 [39424/50000]\tLoss: 0.86\n","Training Epoch: 143 [39680/50000]\tLoss: 0.77\n","Training Epoch: 143 [39936/50000]\tLoss: 0.80\n","Training Epoch: 143 [40192/50000]\tLoss: 0.79\n","Training Epoch: 143 [40448/50000]\tLoss: 0.75\n","Training Epoch: 143 [40704/50000]\tLoss: 0.67\n","Training Epoch: 143 [40960/50000]\tLoss: 0.87\n","Training Epoch: 143 [41216/50000]\tLoss: 0.75\n","Training Epoch: 143 [41472/50000]\tLoss: 0.68\n","Training Epoch: 143 [41728/50000]\tLoss: 0.79\n","Training Epoch: 143 [41984/50000]\tLoss: 0.80\n","Training Epoch: 143 [42240/50000]\tLoss: 0.79\n","Training Epoch: 143 [42496/50000]\tLoss: 0.71\n","Training Epoch: 143 [42752/50000]\tLoss: 0.86\n","Training Epoch: 143 [43008/50000]\tLoss: 0.81\n","Training Epoch: 143 [43264/50000]\tLoss: 0.92\n","Training Epoch: 143 [43520/50000]\tLoss: 0.75\n","Training Epoch: 143 [43776/50000]\tLoss: 0.79\n","Training Epoch: 143 [44032/50000]\tLoss: 0.72\n","Training Epoch: 143 [44288/50000]\tLoss: 0.81\n","Training Epoch: 143 [44544/50000]\tLoss: 0.68\n","Training Epoch: 143 [44800/50000]\tLoss: 0.89\n","Training Epoch: 143 [45056/50000]\tLoss: 0.86\n","Training Epoch: 143 [45312/50000]\tLoss: 0.80\n","Training Epoch: 143 [45568/50000]\tLoss: 0.83\n","Training Epoch: 143 [45824/50000]\tLoss: 0.64\n","Training Epoch: 143 [46080/50000]\tLoss: 0.78\n","Training Epoch: 143 [46336/50000]\tLoss: 0.78\n","Training Epoch: 143 [46592/50000]\tLoss: 0.69\n","Training Epoch: 143 [46848/50000]\tLoss: 0.66\n","Training Epoch: 143 [47104/50000]\tLoss: 0.79\n","Training Epoch: 143 [47360/50000]\tLoss: 0.76\n","Training Epoch: 143 [47616/50000]\tLoss: 0.77\n","Training Epoch: 143 [47872/50000]\tLoss: 0.79\n","Training Epoch: 143 [48128/50000]\tLoss: 0.83\n","Training Epoch: 143 [48384/50000]\tLoss: 0.78\n","Training Epoch: 143 [48640/50000]\tLoss: 0.71\n","Training Epoch: 143 [48896/50000]\tLoss: 0.72\n","Training Epoch: 143 [49152/50000]\tLoss: 0.79\n","Training Epoch: 143 [49408/50000]\tLoss: 0.78\n","Training Epoch: 143 [49664/50000]\tLoss: 0.65\n","Training Epoch: 143 [49920/50000]\tLoss: 0.83\n","Training Epoch: 143 [50000/50000]\tLoss: 0.79\n","Time taken to train epoch 143: 27.34s\n","Testing Network for epoch:  143\n","Evaluation: Evaluation Time: 2.62s, Average loss: 0.0062, Accuracy: 0.6235, Recall: 0.6235, Precision: 0.6283\n","Training Epoch: 144 [256/50000]\tLoss: 0.74\n","Training Epoch: 144 [512/50000]\tLoss: 0.73\n","Training Epoch: 144 [768/50000]\tLoss: 0.82\n","Training Epoch: 144 [1024/50000]\tLoss: 0.60\n","Training Epoch: 144 [1280/50000]\tLoss: 0.73\n","Training Epoch: 144 [1536/50000]\tLoss: 0.81\n","Training Epoch: 144 [1792/50000]\tLoss: 0.71\n","Training Epoch: 144 [2048/50000]\tLoss: 0.69\n","Training Epoch: 144 [2304/50000]\tLoss: 0.76\n","Training Epoch: 144 [2560/50000]\tLoss: 0.69\n","Training Epoch: 144 [2816/50000]\tLoss: 0.72\n","Training Epoch: 144 [3072/50000]\tLoss: 0.94\n","Training Epoch: 144 [3328/50000]\tLoss: 0.72\n","Training Epoch: 144 [3584/50000]\tLoss: 0.80\n","Training Epoch: 144 [3840/50000]\tLoss: 0.69\n","Training Epoch: 144 [4096/50000]\tLoss: 0.75\n","Training Epoch: 144 [4352/50000]\tLoss: 0.71\n","Training Epoch: 144 [4608/50000]\tLoss: 0.78\n","Training Epoch: 144 [4864/50000]\tLoss: 0.72\n","Training Epoch: 144 [5120/50000]\tLoss: 0.74\n","Training Epoch: 144 [5376/50000]\tLoss: 0.68\n","Training Epoch: 144 [5632/50000]\tLoss: 0.80\n","Training Epoch: 144 [5888/50000]\tLoss: 0.84\n","Training Epoch: 144 [6144/50000]\tLoss: 0.83\n","Training Epoch: 144 [6400/50000]\tLoss: 0.70\n","Training Epoch: 144 [6656/50000]\tLoss: 0.76\n","Training Epoch: 144 [6912/50000]\tLoss: 0.71\n","Training Epoch: 144 [7168/50000]\tLoss: 0.73\n","Training Epoch: 144 [7424/50000]\tLoss: 0.69\n","Training Epoch: 144 [7680/50000]\tLoss: 0.68\n","Training Epoch: 144 [7936/50000]\tLoss: 0.85\n","Training Epoch: 144 [8192/50000]\tLoss: 0.75\n","Training Epoch: 144 [8448/50000]\tLoss: 0.77\n","Training Epoch: 144 [8704/50000]\tLoss: 0.75\n","Training Epoch: 144 [8960/50000]\tLoss: 0.74\n","Training Epoch: 144 [9216/50000]\tLoss: 0.70\n","Training Epoch: 144 [9472/50000]\tLoss: 0.77\n","Training Epoch: 144 [9728/50000]\tLoss: 0.72\n","Training Epoch: 144 [9984/50000]\tLoss: 0.78\n","Training Epoch: 144 [10240/50000]\tLoss: 0.91\n","Training Epoch: 144 [10496/50000]\tLoss: 0.65\n","Training Epoch: 144 [10752/50000]\tLoss: 0.93\n","Training Epoch: 144 [11008/50000]\tLoss: 0.66\n","Training Epoch: 144 [11264/50000]\tLoss: 0.78\n","Training Epoch: 144 [11520/50000]\tLoss: 0.93\n","Training Epoch: 144 [11776/50000]\tLoss: 0.78\n","Training Epoch: 144 [12032/50000]\tLoss: 0.88\n","Training Epoch: 144 [12288/50000]\tLoss: 0.67\n","Training Epoch: 144 [12544/50000]\tLoss: 0.59\n","Training Epoch: 144 [12800/50000]\tLoss: 0.91\n","Training Epoch: 144 [13056/50000]\tLoss: 0.78\n","Training Epoch: 144 [13312/50000]\tLoss: 0.81\n","Training Epoch: 144 [13568/50000]\tLoss: 0.76\n","Training Epoch: 144 [13824/50000]\tLoss: 0.80\n","Training Epoch: 144 [14080/50000]\tLoss: 0.71\n","Training Epoch: 144 [14336/50000]\tLoss: 0.75\n","Training Epoch: 144 [14592/50000]\tLoss: 0.74\n","Training Epoch: 144 [14848/50000]\tLoss: 0.80\n","Training Epoch: 144 [15104/50000]\tLoss: 0.64\n","Training Epoch: 144 [15360/50000]\tLoss: 0.62\n","Training Epoch: 144 [15616/50000]\tLoss: 0.76\n","Training Epoch: 144 [15872/50000]\tLoss: 0.83\n","Training Epoch: 144 [16128/50000]\tLoss: 0.69\n","Training Epoch: 144 [16384/50000]\tLoss: 0.70\n","Training Epoch: 144 [16640/50000]\tLoss: 0.72\n","Training Epoch: 144 [16896/50000]\tLoss: 0.98\n","Training Epoch: 144 [17152/50000]\tLoss: 0.73\n","Training Epoch: 144 [17408/50000]\tLoss: 0.75\n","Training Epoch: 144 [17664/50000]\tLoss: 0.74\n","Training Epoch: 144 [17920/50000]\tLoss: 0.78\n","Training Epoch: 144 [18176/50000]\tLoss: 0.69\n","Training Epoch: 144 [18432/50000]\tLoss: 0.89\n","Training Epoch: 144 [18688/50000]\tLoss: 0.76\n","Training Epoch: 144 [18944/50000]\tLoss: 0.74\n","Training Epoch: 144 [19200/50000]\tLoss: 0.90\n","Training Epoch: 144 [19456/50000]\tLoss: 0.72\n","Training Epoch: 144 [19712/50000]\tLoss: 0.76\n","Training Epoch: 144 [19968/50000]\tLoss: 0.81\n","Training Epoch: 144 [20224/50000]\tLoss: 0.75\n","Training Epoch: 144 [20480/50000]\tLoss: 0.68\n","Training Epoch: 144 [20736/50000]\tLoss: 0.75\n","Training Epoch: 144 [20992/50000]\tLoss: 0.72\n","Training Epoch: 144 [21248/50000]\tLoss: 0.70\n","Training Epoch: 144 [21504/50000]\tLoss: 0.76\n","Training Epoch: 144 [21760/50000]\tLoss: 0.70\n","Training Epoch: 144 [22016/50000]\tLoss: 0.85\n","Training Epoch: 144 [22272/50000]\tLoss: 0.81\n","Training Epoch: 144 [22528/50000]\tLoss: 0.76\n","Training Epoch: 144 [22784/50000]\tLoss: 0.75\n","Training Epoch: 144 [23040/50000]\tLoss: 0.84\n","Training Epoch: 144 [23296/50000]\tLoss: 0.73\n","Training Epoch: 144 [23552/50000]\tLoss: 0.75\n","Training Epoch: 144 [23808/50000]\tLoss: 0.73\n","Training Epoch: 144 [24064/50000]\tLoss: 0.74\n","Training Epoch: 144 [24320/50000]\tLoss: 0.78\n","Training Epoch: 144 [24576/50000]\tLoss: 0.80\n","Training Epoch: 144 [24832/50000]\tLoss: 0.68\n","Training Epoch: 144 [25088/50000]\tLoss: 0.76\n","Training Epoch: 144 [25344/50000]\tLoss: 0.72\n","Training Epoch: 144 [25600/50000]\tLoss: 0.89\n","Training Epoch: 144 [25856/50000]\tLoss: 0.68\n","Training Epoch: 144 [26112/50000]\tLoss: 0.74\n","Training Epoch: 144 [26368/50000]\tLoss: 0.72\n","Training Epoch: 144 [26624/50000]\tLoss: 0.67\n","Training Epoch: 144 [26880/50000]\tLoss: 0.62\n","Training Epoch: 144 [27136/50000]\tLoss: 0.77\n","Training Epoch: 144 [27392/50000]\tLoss: 0.78\n","Training Epoch: 144 [27648/50000]\tLoss: 0.65\n","Training Epoch: 144 [27904/50000]\tLoss: 0.69\n","Training Epoch: 144 [28160/50000]\tLoss: 0.77\n","Training Epoch: 144 [28416/50000]\tLoss: 0.70\n","Training Epoch: 144 [28672/50000]\tLoss: 0.77\n","Training Epoch: 144 [28928/50000]\tLoss: 0.92\n","Training Epoch: 144 [29184/50000]\tLoss: 0.78\n","Training Epoch: 144 [29440/50000]\tLoss: 0.85\n","Training Epoch: 144 [29696/50000]\tLoss: 0.75\n","Training Epoch: 144 [29952/50000]\tLoss: 0.92\n","Training Epoch: 144 [30208/50000]\tLoss: 0.81\n","Training Epoch: 144 [30464/50000]\tLoss: 0.74\n","Training Epoch: 144 [30720/50000]\tLoss: 0.74\n","Training Epoch: 144 [30976/50000]\tLoss: 0.79\n","Training Epoch: 144 [31232/50000]\tLoss: 0.83\n","Training Epoch: 144 [31488/50000]\tLoss: 0.69\n","Training Epoch: 144 [31744/50000]\tLoss: 0.92\n","Training Epoch: 144 [32000/50000]\tLoss: 0.79\n","Training Epoch: 144 [32256/50000]\tLoss: 0.74\n","Training Epoch: 144 [32512/50000]\tLoss: 0.84\n","Training Epoch: 144 [32768/50000]\tLoss: 0.73\n","Training Epoch: 144 [33024/50000]\tLoss: 0.80\n","Training Epoch: 144 [33280/50000]\tLoss: 0.79\n","Training Epoch: 144 [33536/50000]\tLoss: 0.57\n","Training Epoch: 144 [33792/50000]\tLoss: 0.68\n","Training Epoch: 144 [34048/50000]\tLoss: 0.69\n","Training Epoch: 144 [34304/50000]\tLoss: 0.84\n","Training Epoch: 144 [34560/50000]\tLoss: 0.74\n","Training Epoch: 144 [34816/50000]\tLoss: 0.83\n","Training Epoch: 144 [35072/50000]\tLoss: 0.90\n","Training Epoch: 144 [35328/50000]\tLoss: 0.77\n","Training Epoch: 144 [35584/50000]\tLoss: 0.79\n","Training Epoch: 144 [35840/50000]\tLoss: 0.80\n","Training Epoch: 144 [36096/50000]\tLoss: 0.79\n","Training Epoch: 144 [36352/50000]\tLoss: 0.69\n","Training Epoch: 144 [36608/50000]\tLoss: 0.88\n","Training Epoch: 144 [36864/50000]\tLoss: 0.91\n","Training Epoch: 144 [37120/50000]\tLoss: 0.74\n","Training Epoch: 144 [37376/50000]\tLoss: 0.76\n","Training Epoch: 144 [37632/50000]\tLoss: 0.73\n","Training Epoch: 144 [37888/50000]\tLoss: 0.65\n","Training Epoch: 144 [38144/50000]\tLoss: 0.84\n","Training Epoch: 144 [38400/50000]\tLoss: 0.70\n","Training Epoch: 144 [38656/50000]\tLoss: 0.82\n","Training Epoch: 144 [38912/50000]\tLoss: 0.69\n","Training Epoch: 144 [39168/50000]\tLoss: 0.80\n","Training Epoch: 144 [39424/50000]\tLoss: 0.82\n","Training Epoch: 144 [39680/50000]\tLoss: 0.80\n","Training Epoch: 144 [39936/50000]\tLoss: 0.75\n","Training Epoch: 144 [40192/50000]\tLoss: 0.70\n","Training Epoch: 144 [40448/50000]\tLoss: 0.78\n","Training Epoch: 144 [40704/50000]\tLoss: 0.82\n","Training Epoch: 144 [40960/50000]\tLoss: 0.84\n","Training Epoch: 144 [41216/50000]\tLoss: 0.73\n","Training Epoch: 144 [41472/50000]\tLoss: 0.79\n","Training Epoch: 144 [41728/50000]\tLoss: 0.78\n","Training Epoch: 144 [41984/50000]\tLoss: 0.78\n","Training Epoch: 144 [42240/50000]\tLoss: 0.69\n","Training Epoch: 144 [42496/50000]\tLoss: 0.77\n","Training Epoch: 144 [42752/50000]\tLoss: 0.70\n","Training Epoch: 144 [43008/50000]\tLoss: 0.76\n","Training Epoch: 144 [43264/50000]\tLoss: 0.76\n","Training Epoch: 144 [43520/50000]\tLoss: 0.73\n","Training Epoch: 144 [43776/50000]\tLoss: 0.80\n","Training Epoch: 144 [44032/50000]\tLoss: 0.73\n","Training Epoch: 144 [44288/50000]\tLoss: 0.79\n","Training Epoch: 144 [44544/50000]\tLoss: 0.70\n","Training Epoch: 144 [44800/50000]\tLoss: 0.69\n","Training Epoch: 144 [45056/50000]\tLoss: 0.93\n","Training Epoch: 144 [45312/50000]\tLoss: 0.68\n","Training Epoch: 144 [45568/50000]\tLoss: 0.65\n","Training Epoch: 144 [45824/50000]\tLoss: 0.88\n","Training Epoch: 144 [46080/50000]\tLoss: 0.87\n","Training Epoch: 144 [46336/50000]\tLoss: 0.83\n","Training Epoch: 144 [46592/50000]\tLoss: 0.77\n","Training Epoch: 144 [46848/50000]\tLoss: 0.61\n","Training Epoch: 144 [47104/50000]\tLoss: 0.76\n","Training Epoch: 144 [47360/50000]\tLoss: 0.70\n","Training Epoch: 144 [47616/50000]\tLoss: 0.77\n","Training Epoch: 144 [47872/50000]\tLoss: 0.69\n","Training Epoch: 144 [48128/50000]\tLoss: 0.65\n","Training Epoch: 144 [48384/50000]\tLoss: 0.70\n","Training Epoch: 144 [48640/50000]\tLoss: 0.84\n","Training Epoch: 144 [48896/50000]\tLoss: 0.72\n","Training Epoch: 144 [49152/50000]\tLoss: 0.66\n","Training Epoch: 144 [49408/50000]\tLoss: 0.68\n","Training Epoch: 144 [49664/50000]\tLoss: 0.82\n","Training Epoch: 144 [49920/50000]\tLoss: 0.61\n","Training Epoch: 144 [50000/50000]\tLoss: 0.93\n","Time taken to train epoch 144: 27.08s\n","Testing Network for epoch:  144\n","Evaluation: Evaluation Time: 2.60s, Average loss: 0.0062, Accuracy: 0.6206, Recall: 0.6206, Precision: 0.6256\n","Training Epoch: 145 [256/50000]\tLoss: 0.86\n","Training Epoch: 145 [512/50000]\tLoss: 0.84\n","Training Epoch: 145 [768/50000]\tLoss: 0.79\n","Training Epoch: 145 [1024/50000]\tLoss: 0.71\n","Training Epoch: 145 [1280/50000]\tLoss: 0.65\n","Training Epoch: 145 [1536/50000]\tLoss: 0.84\n","Training Epoch: 145 [1792/50000]\tLoss: 0.69\n","Training Epoch: 145 [2048/50000]\tLoss: 0.82\n","Training Epoch: 145 [2304/50000]\tLoss: 0.69\n","Training Epoch: 145 [2560/50000]\tLoss: 0.72\n","Training Epoch: 145 [2816/50000]\tLoss: 0.79\n","Training Epoch: 145 [3072/50000]\tLoss: 0.57\n","Training Epoch: 145 [3328/50000]\tLoss: 0.60\n","Training Epoch: 145 [3584/50000]\tLoss: 0.78\n","Training Epoch: 145 [3840/50000]\tLoss: 0.66\n","Training Epoch: 145 [4096/50000]\tLoss: 0.78\n","Training Epoch: 145 [4352/50000]\tLoss: 0.69\n","Training Epoch: 145 [4608/50000]\tLoss: 0.81\n","Training Epoch: 145 [4864/50000]\tLoss: 0.80\n","Training Epoch: 145 [5120/50000]\tLoss: 0.64\n","Training Epoch: 145 [5376/50000]\tLoss: 0.88\n","Training Epoch: 145 [5632/50000]\tLoss: 0.75\n","Training Epoch: 145 [5888/50000]\tLoss: 0.69\n","Training Epoch: 145 [6144/50000]\tLoss: 0.70\n","Training Epoch: 145 [6400/50000]\tLoss: 0.80\n","Training Epoch: 145 [6656/50000]\tLoss: 0.80\n","Training Epoch: 145 [6912/50000]\tLoss: 0.87\n","Training Epoch: 145 [7168/50000]\tLoss: 0.66\n","Training Epoch: 145 [7424/50000]\tLoss: 0.72\n","Training Epoch: 145 [7680/50000]\tLoss: 0.77\n","Training Epoch: 145 [7936/50000]\tLoss: 0.67\n","Training Epoch: 145 [8192/50000]\tLoss: 0.86\n","Training Epoch: 145 [8448/50000]\tLoss: 0.75\n","Training Epoch: 145 [8704/50000]\tLoss: 0.77\n","Training Epoch: 145 [8960/50000]\tLoss: 0.84\n","Training Epoch: 145 [9216/50000]\tLoss: 0.69\n","Training Epoch: 145 [9472/50000]\tLoss: 0.79\n","Training Epoch: 145 [9728/50000]\tLoss: 0.73\n","Training Epoch: 145 [9984/50000]\tLoss: 0.79\n","Training Epoch: 145 [10240/50000]\tLoss: 0.78\n","Training Epoch: 145 [10496/50000]\tLoss: 0.77\n","Training Epoch: 145 [10752/50000]\tLoss: 0.78\n","Training Epoch: 145 [11008/50000]\tLoss: 0.72\n","Training Epoch: 145 [11264/50000]\tLoss: 0.76\n","Training Epoch: 145 [11520/50000]\tLoss: 0.72\n","Training Epoch: 145 [11776/50000]\tLoss: 0.89\n","Training Epoch: 145 [12032/50000]\tLoss: 0.78\n","Training Epoch: 145 [12288/50000]\tLoss: 0.90\n","Training Epoch: 145 [12544/50000]\tLoss: 0.76\n","Training Epoch: 145 [12800/50000]\tLoss: 0.68\n","Training Epoch: 145 [13056/50000]\tLoss: 0.73\n","Training Epoch: 145 [13312/50000]\tLoss: 0.77\n","Training Epoch: 145 [13568/50000]\tLoss: 0.56\n","Training Epoch: 145 [13824/50000]\tLoss: 0.87\n","Training Epoch: 145 [14080/50000]\tLoss: 0.75\n","Training Epoch: 145 [14336/50000]\tLoss: 0.71\n","Training Epoch: 145 [14592/50000]\tLoss: 0.89\n","Training Epoch: 145 [14848/50000]\tLoss: 0.93\n","Training Epoch: 145 [15104/50000]\tLoss: 0.69\n","Training Epoch: 145 [15360/50000]\tLoss: 0.69\n","Training Epoch: 145 [15616/50000]\tLoss: 0.69\n","Training Epoch: 145 [15872/50000]\tLoss: 0.64\n","Training Epoch: 145 [16128/50000]\tLoss: 0.62\n","Training Epoch: 145 [16384/50000]\tLoss: 0.70\n","Training Epoch: 145 [16640/50000]\tLoss: 0.77\n","Training Epoch: 145 [16896/50000]\tLoss: 0.65\n","Training Epoch: 145 [17152/50000]\tLoss: 0.75\n","Training Epoch: 145 [17408/50000]\tLoss: 0.75\n","Training Epoch: 145 [17664/50000]\tLoss: 0.68\n","Training Epoch: 145 [17920/50000]\tLoss: 0.72\n","Training Epoch: 145 [18176/50000]\tLoss: 0.80\n","Training Epoch: 145 [18432/50000]\tLoss: 0.81\n","Training Epoch: 145 [18688/50000]\tLoss: 0.72\n","Training Epoch: 145 [18944/50000]\tLoss: 0.81\n","Training Epoch: 145 [19200/50000]\tLoss: 0.80\n","Training Epoch: 145 [19456/50000]\tLoss: 0.74\n","Training Epoch: 145 [19712/50000]\tLoss: 0.91\n","Training Epoch: 145 [19968/50000]\tLoss: 0.68\n","Training Epoch: 145 [20224/50000]\tLoss: 0.72\n","Training Epoch: 145 [20480/50000]\tLoss: 0.62\n","Training Epoch: 145 [20736/50000]\tLoss: 0.87\n","Training Epoch: 145 [20992/50000]\tLoss: 0.69\n","Training Epoch: 145 [21248/50000]\tLoss: 0.87\n","Training Epoch: 145 [21504/50000]\tLoss: 0.89\n","Training Epoch: 145 [21760/50000]\tLoss: 0.70\n","Training Epoch: 145 [22016/50000]\tLoss: 0.81\n","Training Epoch: 145 [22272/50000]\tLoss: 0.83\n","Training Epoch: 145 [22528/50000]\tLoss: 0.72\n","Training Epoch: 145 [22784/50000]\tLoss: 0.71\n","Training Epoch: 145 [23040/50000]\tLoss: 0.87\n","Training Epoch: 145 [23296/50000]\tLoss: 0.71\n","Training Epoch: 145 [23552/50000]\tLoss: 0.75\n","Training Epoch: 145 [23808/50000]\tLoss: 0.77\n","Training Epoch: 145 [24064/50000]\tLoss: 0.80\n","Training Epoch: 145 [24320/50000]\tLoss: 0.80\n","Training Epoch: 145 [24576/50000]\tLoss: 0.88\n","Training Epoch: 145 [24832/50000]\tLoss: 0.74\n","Training Epoch: 145 [25088/50000]\tLoss: 0.75\n","Training Epoch: 145 [25344/50000]\tLoss: 0.77\n","Training Epoch: 145 [25600/50000]\tLoss: 0.81\n","Training Epoch: 145 [25856/50000]\tLoss: 0.73\n","Training Epoch: 145 [26112/50000]\tLoss: 0.86\n","Training Epoch: 145 [26368/50000]\tLoss: 0.74\n","Training Epoch: 145 [26624/50000]\tLoss: 0.70\n","Training Epoch: 145 [26880/50000]\tLoss: 0.86\n","Training Epoch: 145 [27136/50000]\tLoss: 0.69\n","Training Epoch: 145 [27392/50000]\tLoss: 0.72\n","Training Epoch: 145 [27648/50000]\tLoss: 0.73\n","Training Epoch: 145 [27904/50000]\tLoss: 0.66\n","Training Epoch: 145 [28160/50000]\tLoss: 0.86\n","Training Epoch: 145 [28416/50000]\tLoss: 0.74\n","Training Epoch: 145 [28672/50000]\tLoss: 0.76\n","Training Epoch: 145 [28928/50000]\tLoss: 0.59\n","Training Epoch: 145 [29184/50000]\tLoss: 0.70\n","Training Epoch: 145 [29440/50000]\tLoss: 0.62\n","Training Epoch: 145 [29696/50000]\tLoss: 0.82\n","Training Epoch: 145 [29952/50000]\tLoss: 0.78\n","Training Epoch: 145 [30208/50000]\tLoss: 0.88\n","Training Epoch: 145 [30464/50000]\tLoss: 0.78\n","Training Epoch: 145 [30720/50000]\tLoss: 0.69\n","Training Epoch: 145 [30976/50000]\tLoss: 0.71\n","Training Epoch: 145 [31232/50000]\tLoss: 0.85\n","Training Epoch: 145 [31488/50000]\tLoss: 0.71\n","Training Epoch: 145 [31744/50000]\tLoss: 0.86\n","Training Epoch: 145 [32000/50000]\tLoss: 0.72\n","Training Epoch: 145 [32256/50000]\tLoss: 0.76\n","Training Epoch: 145 [32512/50000]\tLoss: 0.71\n","Training Epoch: 145 [32768/50000]\tLoss: 0.62\n","Training Epoch: 145 [33024/50000]\tLoss: 0.63\n","Training Epoch: 145 [33280/50000]\tLoss: 0.77\n","Training Epoch: 145 [33536/50000]\tLoss: 0.70\n","Training Epoch: 145 [33792/50000]\tLoss: 0.96\n","Training Epoch: 145 [34048/50000]\tLoss: 0.76\n","Training Epoch: 145 [34304/50000]\tLoss: 0.78\n","Training Epoch: 145 [34560/50000]\tLoss: 0.56\n","Training Epoch: 145 [34816/50000]\tLoss: 0.78\n","Training Epoch: 145 [35072/50000]\tLoss: 0.71\n","Training Epoch: 145 [35328/50000]\tLoss: 0.79\n","Training Epoch: 145 [35584/50000]\tLoss: 0.76\n","Training Epoch: 145 [35840/50000]\tLoss: 0.80\n","Training Epoch: 145 [36096/50000]\tLoss: 0.75\n","Training Epoch: 145 [36352/50000]\tLoss: 0.66\n","Training Epoch: 145 [36608/50000]\tLoss: 0.85\n","Training Epoch: 145 [36864/50000]\tLoss: 0.71\n","Training Epoch: 145 [37120/50000]\tLoss: 0.82\n","Training Epoch: 145 [37376/50000]\tLoss: 0.72\n","Training Epoch: 145 [37632/50000]\tLoss: 0.75\n","Training Epoch: 145 [37888/50000]\tLoss: 0.71\n","Training Epoch: 145 [38144/50000]\tLoss: 0.71\n","Training Epoch: 145 [38400/50000]\tLoss: 0.64\n","Training Epoch: 145 [38656/50000]\tLoss: 0.69\n","Training Epoch: 145 [38912/50000]\tLoss: 0.87\n","Training Epoch: 145 [39168/50000]\tLoss: 0.84\n","Training Epoch: 145 [39424/50000]\tLoss: 0.79\n","Training Epoch: 145 [39680/50000]\tLoss: 0.66\n","Training Epoch: 145 [39936/50000]\tLoss: 0.74\n","Training Epoch: 145 [40192/50000]\tLoss: 0.89\n","Training Epoch: 145 [40448/50000]\tLoss: 0.73\n","Training Epoch: 145 [40704/50000]\tLoss: 0.75\n","Training Epoch: 145 [40960/50000]\tLoss: 0.62\n","Training Epoch: 145 [41216/50000]\tLoss: 0.76\n","Training Epoch: 145 [41472/50000]\tLoss: 0.73\n","Training Epoch: 145 [41728/50000]\tLoss: 0.76\n","Training Epoch: 145 [41984/50000]\tLoss: 0.77\n","Training Epoch: 145 [42240/50000]\tLoss: 0.81\n","Training Epoch: 145 [42496/50000]\tLoss: 0.90\n","Training Epoch: 145 [42752/50000]\tLoss: 0.76\n","Training Epoch: 145 [43008/50000]\tLoss: 0.78\n","Training Epoch: 145 [43264/50000]\tLoss: 0.73\n","Training Epoch: 145 [43520/50000]\tLoss: 0.79\n","Training Epoch: 145 [43776/50000]\tLoss: 0.91\n","Training Epoch: 145 [44032/50000]\tLoss: 0.75\n","Training Epoch: 145 [44288/50000]\tLoss: 0.86\n","Training Epoch: 145 [44544/50000]\tLoss: 0.63\n","Training Epoch: 145 [44800/50000]\tLoss: 0.71\n","Training Epoch: 145 [45056/50000]\tLoss: 0.73\n","Training Epoch: 145 [45312/50000]\tLoss: 0.73\n","Training Epoch: 145 [45568/50000]\tLoss: 0.73\n","Training Epoch: 145 [45824/50000]\tLoss: 0.70\n","Training Epoch: 145 [46080/50000]\tLoss: 0.81\n","Training Epoch: 145 [46336/50000]\tLoss: 0.76\n","Training Epoch: 145 [46592/50000]\tLoss: 0.74\n","Training Epoch: 145 [46848/50000]\tLoss: 0.81\n","Training Epoch: 145 [47104/50000]\tLoss: 0.65\n","Training Epoch: 145 [47360/50000]\tLoss: 0.67\n","Training Epoch: 145 [47616/50000]\tLoss: 0.69\n","Training Epoch: 145 [47872/50000]\tLoss: 0.93\n","Training Epoch: 145 [48128/50000]\tLoss: 0.81\n","Training Epoch: 145 [48384/50000]\tLoss: 0.72\n","Training Epoch: 145 [48640/50000]\tLoss: 0.71\n","Training Epoch: 145 [48896/50000]\tLoss: 0.91\n","Training Epoch: 145 [49152/50000]\tLoss: 0.81\n","Training Epoch: 145 [49408/50000]\tLoss: 0.79\n","Training Epoch: 145 [49664/50000]\tLoss: 0.81\n","Training Epoch: 145 [49920/50000]\tLoss: 0.75\n","Training Epoch: 145 [50000/50000]\tLoss: 0.64\n","Time taken to train epoch 145: 27.31s\n","Testing Network for epoch:  145\n","Evaluation: Evaluation Time: 2.61s, Average loss: 0.0063, Accuracy: 0.6226, Recall: 0.6226, Precision: 0.6277\n","Training Epoch: 146 [256/50000]\tLoss: 0.83\n","Training Epoch: 146 [512/50000]\tLoss: 0.66\n","Training Epoch: 146 [768/50000]\tLoss: 0.73\n","Training Epoch: 146 [1024/50000]\tLoss: 0.68\n","Training Epoch: 146 [1280/50000]\tLoss: 0.69\n","Training Epoch: 146 [1536/50000]\tLoss: 0.82\n","Training Epoch: 146 [1792/50000]\tLoss: 0.63\n","Training Epoch: 146 [2048/50000]\tLoss: 0.83\n","Training Epoch: 146 [2304/50000]\tLoss: 0.66\n","Training Epoch: 146 [2560/50000]\tLoss: 0.70\n","Training Epoch: 146 [2816/50000]\tLoss: 0.74\n","Training Epoch: 146 [3072/50000]\tLoss: 0.71\n","Training Epoch: 146 [3328/50000]\tLoss: 0.74\n","Training Epoch: 146 [3584/50000]\tLoss: 0.76\n","Training Epoch: 146 [3840/50000]\tLoss: 0.74\n","Training Epoch: 146 [4096/50000]\tLoss: 0.79\n","Training Epoch: 146 [4352/50000]\tLoss: 0.66\n","Training Epoch: 146 [4608/50000]\tLoss: 0.66\n","Training Epoch: 146 [4864/50000]\tLoss: 0.76\n","Training Epoch: 146 [5120/50000]\tLoss: 0.82\n","Training Epoch: 146 [5376/50000]\tLoss: 0.73\n","Training Epoch: 146 [5632/50000]\tLoss: 0.71\n","Training Epoch: 146 [5888/50000]\tLoss: 0.78\n","Training Epoch: 146 [6144/50000]\tLoss: 0.82\n","Training Epoch: 146 [6400/50000]\tLoss: 0.85\n","Training Epoch: 146 [6656/50000]\tLoss: 0.74\n","Training Epoch: 146 [6912/50000]\tLoss: 0.78\n","Training Epoch: 146 [7168/50000]\tLoss: 0.79\n","Training Epoch: 146 [7424/50000]\tLoss: 0.89\n","Training Epoch: 146 [7680/50000]\tLoss: 0.77\n","Training Epoch: 146 [7936/50000]\tLoss: 0.76\n","Training Epoch: 146 [8192/50000]\tLoss: 0.59\n","Training Epoch: 146 [8448/50000]\tLoss: 0.79\n","Training Epoch: 146 [8704/50000]\tLoss: 0.82\n","Training Epoch: 146 [8960/50000]\tLoss: 0.66\n","Training Epoch: 146 [9216/50000]\tLoss: 0.69\n","Training Epoch: 146 [9472/50000]\tLoss: 0.72\n","Training Epoch: 146 [9728/50000]\tLoss: 0.75\n","Training Epoch: 146 [9984/50000]\tLoss: 0.78\n","Training Epoch: 146 [10240/50000]\tLoss: 0.85\n","Training Epoch: 146 [10496/50000]\tLoss: 0.77\n","Training Epoch: 146 [10752/50000]\tLoss: 0.74\n","Training Epoch: 146 [11008/50000]\tLoss: 0.64\n","Training Epoch: 146 [11264/50000]\tLoss: 0.83\n","Training Epoch: 146 [11520/50000]\tLoss: 0.79\n","Training Epoch: 146 [11776/50000]\tLoss: 0.67\n","Training Epoch: 146 [12032/50000]\tLoss: 0.71\n","Training Epoch: 146 [12288/50000]\tLoss: 0.67\n","Training Epoch: 146 [12544/50000]\tLoss: 0.84\n","Training Epoch: 146 [12800/50000]\tLoss: 0.87\n","Training Epoch: 146 [13056/50000]\tLoss: 0.70\n","Training Epoch: 146 [13312/50000]\tLoss: 0.72\n","Training Epoch: 146 [13568/50000]\tLoss: 0.70\n","Training Epoch: 146 [13824/50000]\tLoss: 0.61\n","Training Epoch: 146 [14080/50000]\tLoss: 0.67\n","Training Epoch: 146 [14336/50000]\tLoss: 0.79\n","Training Epoch: 146 [14592/50000]\tLoss: 0.66\n","Training Epoch: 146 [14848/50000]\tLoss: 0.76\n","Training Epoch: 146 [15104/50000]\tLoss: 0.70\n","Training Epoch: 146 [15360/50000]\tLoss: 0.70\n","Training Epoch: 146 [15616/50000]\tLoss: 0.80\n","Training Epoch: 146 [15872/50000]\tLoss: 0.88\n","Training Epoch: 146 [16128/50000]\tLoss: 0.77\n","Training Epoch: 146 [16384/50000]\tLoss: 0.65\n","Training Epoch: 146 [16640/50000]\tLoss: 0.62\n","Training Epoch: 146 [16896/50000]\tLoss: 0.73\n","Training Epoch: 146 [17152/50000]\tLoss: 0.75\n","Training Epoch: 146 [17408/50000]\tLoss: 0.71\n","Training Epoch: 146 [17664/50000]\tLoss: 0.80\n","Training Epoch: 146 [17920/50000]\tLoss: 0.85\n","Training Epoch: 146 [18176/50000]\tLoss: 0.77\n","Training Epoch: 146 [18432/50000]\tLoss: 0.72\n","Training Epoch: 146 [18688/50000]\tLoss: 0.71\n","Training Epoch: 146 [18944/50000]\tLoss: 0.85\n","Training Epoch: 146 [19200/50000]\tLoss: 0.71\n","Training Epoch: 146 [19456/50000]\tLoss: 0.75\n","Training Epoch: 146 [19712/50000]\tLoss: 0.77\n","Training Epoch: 146 [19968/50000]\tLoss: 0.73\n","Training Epoch: 146 [20224/50000]\tLoss: 0.79\n","Training Epoch: 146 [20480/50000]\tLoss: 0.77\n","Training Epoch: 146 [20736/50000]\tLoss: 0.75\n","Training Epoch: 146 [20992/50000]\tLoss: 0.76\n","Training Epoch: 146 [21248/50000]\tLoss: 0.62\n","Training Epoch: 146 [21504/50000]\tLoss: 0.74\n","Training Epoch: 146 [21760/50000]\tLoss: 0.76\n","Training Epoch: 146 [22016/50000]\tLoss: 0.85\n","Training Epoch: 146 [22272/50000]\tLoss: 0.78\n","Training Epoch: 146 [22528/50000]\tLoss: 0.76\n","Training Epoch: 146 [22784/50000]\tLoss: 0.74\n","Training Epoch: 146 [23040/50000]\tLoss: 0.71\n","Training Epoch: 146 [23296/50000]\tLoss: 0.71\n","Training Epoch: 146 [23552/50000]\tLoss: 0.75\n","Training Epoch: 146 [23808/50000]\tLoss: 0.83\n","Training Epoch: 146 [24064/50000]\tLoss: 0.75\n","Training Epoch: 146 [24320/50000]\tLoss: 0.71\n","Training Epoch: 146 [24576/50000]\tLoss: 0.65\n","Training Epoch: 146 [24832/50000]\tLoss: 0.84\n","Training Epoch: 146 [25088/50000]\tLoss: 0.75\n","Training Epoch: 146 [25344/50000]\tLoss: 0.76\n","Training Epoch: 146 [25600/50000]\tLoss: 0.72\n","Training Epoch: 146 [25856/50000]\tLoss: 0.78\n","Training Epoch: 146 [26112/50000]\tLoss: 0.74\n","Training Epoch: 146 [26368/50000]\tLoss: 0.71\n","Training Epoch: 146 [26624/50000]\tLoss: 0.71\n","Training Epoch: 146 [26880/50000]\tLoss: 0.66\n","Training Epoch: 146 [27136/50000]\tLoss: 0.75\n","Training Epoch: 146 [27392/50000]\tLoss: 0.74\n","Training Epoch: 146 [27648/50000]\tLoss: 0.86\n","Training Epoch: 146 [27904/50000]\tLoss: 0.68\n","Training Epoch: 146 [28160/50000]\tLoss: 0.68\n","Training Epoch: 146 [28416/50000]\tLoss: 0.79\n","Training Epoch: 146 [28672/50000]\tLoss: 0.78\n","Training Epoch: 146 [28928/50000]\tLoss: 0.82\n","Training Epoch: 146 [29184/50000]\tLoss: 0.79\n","Training Epoch: 146 [29440/50000]\tLoss: 0.77\n","Training Epoch: 146 [29696/50000]\tLoss: 0.82\n","Training Epoch: 146 [29952/50000]\tLoss: 0.70\n","Training Epoch: 146 [30208/50000]\tLoss: 0.67\n","Training Epoch: 146 [30464/50000]\tLoss: 0.63\n","Training Epoch: 146 [30720/50000]\tLoss: 0.77\n","Training Epoch: 146 [30976/50000]\tLoss: 0.79\n","Training Epoch: 146 [31232/50000]\tLoss: 0.76\n","Training Epoch: 146 [31488/50000]\tLoss: 0.68\n","Training Epoch: 146 [31744/50000]\tLoss: 0.82\n","Training Epoch: 146 [32000/50000]\tLoss: 0.69\n","Training Epoch: 146 [32256/50000]\tLoss: 1.01\n","Training Epoch: 146 [32512/50000]\tLoss: 0.76\n","Training Epoch: 146 [32768/50000]\tLoss: 0.66\n","Training Epoch: 146 [33024/50000]\tLoss: 0.75\n","Training Epoch: 146 [33280/50000]\tLoss: 0.79\n","Training Epoch: 146 [33536/50000]\tLoss: 0.80\n","Training Epoch: 146 [33792/50000]\tLoss: 0.78\n","Training Epoch: 146 [34048/50000]\tLoss: 0.81\n","Training Epoch: 146 [34304/50000]\tLoss: 0.80\n","Training Epoch: 146 [34560/50000]\tLoss: 0.75\n","Training Epoch: 146 [34816/50000]\tLoss: 0.60\n","Training Epoch: 146 [35072/50000]\tLoss: 0.78\n","Training Epoch: 146 [35328/50000]\tLoss: 0.69\n","Training Epoch: 146 [35584/50000]\tLoss: 0.65\n","Training Epoch: 146 [35840/50000]\tLoss: 0.62\n","Training Epoch: 146 [36096/50000]\tLoss: 0.78\n","Training Epoch: 146 [36352/50000]\tLoss: 0.75\n","Training Epoch: 146 [36608/50000]\tLoss: 0.81\n","Training Epoch: 146 [36864/50000]\tLoss: 0.92\n","Training Epoch: 146 [37120/50000]\tLoss: 0.72\n","Training Epoch: 146 [37376/50000]\tLoss: 0.94\n","Training Epoch: 146 [37632/50000]\tLoss: 0.68\n","Training Epoch: 146 [37888/50000]\tLoss: 0.66\n","Training Epoch: 146 [38144/50000]\tLoss: 0.80\n","Training Epoch: 146 [38400/50000]\tLoss: 0.84\n","Training Epoch: 146 [38656/50000]\tLoss: 0.64\n","Training Epoch: 146 [38912/50000]\tLoss: 0.79\n","Training Epoch: 146 [39168/50000]\tLoss: 0.83\n","Training Epoch: 146 [39424/50000]\tLoss: 0.70\n","Training Epoch: 146 [39680/50000]\tLoss: 0.78\n","Training Epoch: 146 [39936/50000]\tLoss: 0.86\n","Training Epoch: 146 [40192/50000]\tLoss: 0.65\n","Training Epoch: 146 [40448/50000]\tLoss: 0.90\n","Training Epoch: 146 [40704/50000]\tLoss: 0.92\n","Training Epoch: 146 [40960/50000]\tLoss: 0.64\n","Training Epoch: 146 [41216/50000]\tLoss: 0.68\n","Training Epoch: 146 [41472/50000]\tLoss: 0.71\n","Training Epoch: 146 [41728/50000]\tLoss: 0.87\n","Training Epoch: 146 [41984/50000]\tLoss: 0.77\n","Training Epoch: 146 [42240/50000]\tLoss: 0.74\n","Training Epoch: 146 [42496/50000]\tLoss: 0.89\n","Training Epoch: 146 [42752/50000]\tLoss: 0.84\n","Training Epoch: 146 [43008/50000]\tLoss: 0.69\n","Training Epoch: 146 [43264/50000]\tLoss: 0.81\n","Training Epoch: 146 [43520/50000]\tLoss: 0.68\n","Training Epoch: 146 [43776/50000]\tLoss: 0.72\n","Training Epoch: 146 [44032/50000]\tLoss: 0.76\n","Training Epoch: 146 [44288/50000]\tLoss: 0.74\n","Training Epoch: 146 [44544/50000]\tLoss: 0.75\n","Training Epoch: 146 [44800/50000]\tLoss: 0.92\n","Training Epoch: 146 [45056/50000]\tLoss: 0.75\n","Training Epoch: 146 [45312/50000]\tLoss: 0.74\n","Training Epoch: 146 [45568/50000]\tLoss: 0.70\n","Training Epoch: 146 [45824/50000]\tLoss: 0.74\n","Training Epoch: 146 [46080/50000]\tLoss: 0.84\n","Training Epoch: 146 [46336/50000]\tLoss: 0.74\n","Training Epoch: 146 [46592/50000]\tLoss: 0.60\n","Training Epoch: 146 [46848/50000]\tLoss: 0.72\n","Training Epoch: 146 [47104/50000]\tLoss: 0.76\n","Training Epoch: 146 [47360/50000]\tLoss: 0.74\n","Training Epoch: 146 [47616/50000]\tLoss: 0.75\n","Training Epoch: 146 [47872/50000]\tLoss: 0.81\n","Training Epoch: 146 [48128/50000]\tLoss: 0.70\n","Training Epoch: 146 [48384/50000]\tLoss: 0.67\n","Training Epoch: 146 [48640/50000]\tLoss: 0.83\n","Training Epoch: 146 [48896/50000]\tLoss: 0.78\n","Training Epoch: 146 [49152/50000]\tLoss: 0.76\n","Training Epoch: 146 [49408/50000]\tLoss: 0.81\n","Training Epoch: 146 [49664/50000]\tLoss: 0.77\n","Training Epoch: 146 [49920/50000]\tLoss: 0.75\n","Training Epoch: 146 [50000/50000]\tLoss: 0.93\n","Time taken to train epoch 146: 27.29s\n","Testing Network for epoch:  146\n","Evaluation: Evaluation Time: 2.66s, Average loss: 0.0063, Accuracy: 0.6223, Recall: 0.6223, Precision: 0.6282\n","Training Epoch: 147 [256/50000]\tLoss: 0.75\n","Training Epoch: 147 [512/50000]\tLoss: 0.68\n","Training Epoch: 147 [768/50000]\tLoss: 0.63\n","Training Epoch: 147 [1024/50000]\tLoss: 0.80\n","Training Epoch: 147 [1280/50000]\tLoss: 0.70\n","Training Epoch: 147 [1536/50000]\tLoss: 0.83\n","Training Epoch: 147 [1792/50000]\tLoss: 0.73\n","Training Epoch: 147 [2048/50000]\tLoss: 0.72\n","Training Epoch: 147 [2304/50000]\tLoss: 0.68\n","Training Epoch: 147 [2560/50000]\tLoss: 0.77\n","Training Epoch: 147 [2816/50000]\tLoss: 0.64\n","Training Epoch: 147 [3072/50000]\tLoss: 0.54\n","Training Epoch: 147 [3328/50000]\tLoss: 0.68\n","Training Epoch: 147 [3584/50000]\tLoss: 0.65\n","Training Epoch: 147 [3840/50000]\tLoss: 0.68\n","Training Epoch: 147 [4096/50000]\tLoss: 0.73\n","Training Epoch: 147 [4352/50000]\tLoss: 0.66\n","Training Epoch: 147 [4608/50000]\tLoss: 0.76\n","Training Epoch: 147 [4864/50000]\tLoss: 0.74\n","Training Epoch: 147 [5120/50000]\tLoss: 0.80\n","Training Epoch: 147 [5376/50000]\tLoss: 0.78\n","Training Epoch: 147 [5632/50000]\tLoss: 0.66\n","Training Epoch: 147 [5888/50000]\tLoss: 0.80\n","Training Epoch: 147 [6144/50000]\tLoss: 0.58\n","Training Epoch: 147 [6400/50000]\tLoss: 0.85\n","Training Epoch: 147 [6656/50000]\tLoss: 0.70\n","Training Epoch: 147 [6912/50000]\tLoss: 0.85\n","Training Epoch: 147 [7168/50000]\tLoss: 0.62\n","Training Epoch: 147 [7424/50000]\tLoss: 0.64\n","Training Epoch: 147 [7680/50000]\tLoss: 0.81\n","Training Epoch: 147 [7936/50000]\tLoss: 0.81\n","Training Epoch: 147 [8192/50000]\tLoss: 0.79\n","Training Epoch: 147 [8448/50000]\tLoss: 0.64\n","Training Epoch: 147 [8704/50000]\tLoss: 0.78\n","Training Epoch: 147 [8960/50000]\tLoss: 0.63\n","Training Epoch: 147 [9216/50000]\tLoss: 0.72\n","Training Epoch: 147 [9472/50000]\tLoss: 0.70\n","Training Epoch: 147 [9728/50000]\tLoss: 0.65\n","Training Epoch: 147 [9984/50000]\tLoss: 0.71\n","Training Epoch: 147 [10240/50000]\tLoss: 0.80\n","Training Epoch: 147 [10496/50000]\tLoss: 0.77\n","Training Epoch: 147 [10752/50000]\tLoss: 0.71\n","Training Epoch: 147 [11008/50000]\tLoss: 0.71\n","Training Epoch: 147 [11264/50000]\tLoss: 0.68\n","Training Epoch: 147 [11520/50000]\tLoss: 0.69\n","Training Epoch: 147 [11776/50000]\tLoss: 0.70\n","Training Epoch: 147 [12032/50000]\tLoss: 0.71\n","Training Epoch: 147 [12288/50000]\tLoss: 0.66\n","Training Epoch: 147 [12544/50000]\tLoss: 0.73\n","Training Epoch: 147 [12800/50000]\tLoss: 0.67\n","Training Epoch: 147 [13056/50000]\tLoss: 0.68\n","Training Epoch: 147 [13312/50000]\tLoss: 0.82\n","Training Epoch: 147 [13568/50000]\tLoss: 0.74\n","Training Epoch: 147 [13824/50000]\tLoss: 0.70\n","Training Epoch: 147 [14080/50000]\tLoss: 0.83\n","Training Epoch: 147 [14336/50000]\tLoss: 0.79\n","Training Epoch: 147 [14592/50000]\tLoss: 0.68\n","Training Epoch: 147 [14848/50000]\tLoss: 0.73\n","Training Epoch: 147 [15104/50000]\tLoss: 0.73\n","Training Epoch: 147 [15360/50000]\tLoss: 0.70\n","Training Epoch: 147 [15616/50000]\tLoss: 0.70\n","Training Epoch: 147 [15872/50000]\tLoss: 0.80\n","Training Epoch: 147 [16128/50000]\tLoss: 0.65\n","Training Epoch: 147 [16384/50000]\tLoss: 0.62\n","Training Epoch: 147 [16640/50000]\tLoss: 0.61\n","Training Epoch: 147 [16896/50000]\tLoss: 0.62\n","Training Epoch: 147 [17152/50000]\tLoss: 0.99\n","Training Epoch: 147 [17408/50000]\tLoss: 0.75\n","Training Epoch: 147 [17664/50000]\tLoss: 0.76\n","Training Epoch: 147 [17920/50000]\tLoss: 0.67\n","Training Epoch: 147 [18176/50000]\tLoss: 0.66\n","Training Epoch: 147 [18432/50000]\tLoss: 0.63\n","Training Epoch: 147 [18688/50000]\tLoss: 0.65\n","Training Epoch: 147 [18944/50000]\tLoss: 0.87\n","Training Epoch: 147 [19200/50000]\tLoss: 0.78\n","Training Epoch: 147 [19456/50000]\tLoss: 0.78\n","Training Epoch: 147 [19712/50000]\tLoss: 0.86\n","Training Epoch: 147 [19968/50000]\tLoss: 0.78\n","Training Epoch: 147 [20224/50000]\tLoss: 0.68\n","Training Epoch: 147 [20480/50000]\tLoss: 0.63\n","Training Epoch: 147 [20736/50000]\tLoss: 0.76\n","Training Epoch: 147 [20992/50000]\tLoss: 0.76\n","Training Epoch: 147 [21248/50000]\tLoss: 0.76\n","Training Epoch: 147 [21504/50000]\tLoss: 0.70\n","Training Epoch: 147 [21760/50000]\tLoss: 0.76\n","Training Epoch: 147 [22016/50000]\tLoss: 0.81\n","Training Epoch: 147 [22272/50000]\tLoss: 0.71\n","Training Epoch: 147 [22528/50000]\tLoss: 0.79\n","Training Epoch: 147 [22784/50000]\tLoss: 0.74\n","Training Epoch: 147 [23040/50000]\tLoss: 0.79\n","Training Epoch: 147 [23296/50000]\tLoss: 0.72\n","Training Epoch: 147 [23552/50000]\tLoss: 0.93\n","Training Epoch: 147 [23808/50000]\tLoss: 0.73\n","Training Epoch: 147 [24064/50000]\tLoss: 0.75\n","Training Epoch: 147 [24320/50000]\tLoss: 0.84\n","Training Epoch: 147 [24576/50000]\tLoss: 0.86\n","Training Epoch: 147 [24832/50000]\tLoss: 0.67\n","Training Epoch: 147 [25088/50000]\tLoss: 0.67\n","Training Epoch: 147 [25344/50000]\tLoss: 0.77\n","Training Epoch: 147 [25600/50000]\tLoss: 0.64\n","Training Epoch: 147 [25856/50000]\tLoss: 0.77\n","Training Epoch: 147 [26112/50000]\tLoss: 0.87\n","Training Epoch: 147 [26368/50000]\tLoss: 0.73\n","Training Epoch: 147 [26624/50000]\tLoss: 0.78\n","Training Epoch: 147 [26880/50000]\tLoss: 0.89\n","Training Epoch: 147 [27136/50000]\tLoss: 0.87\n","Training Epoch: 147 [27392/50000]\tLoss: 0.80\n","Training Epoch: 147 [27648/50000]\tLoss: 0.61\n","Training Epoch: 147 [27904/50000]\tLoss: 0.76\n","Training Epoch: 147 [28160/50000]\tLoss: 0.79\n","Training Epoch: 147 [28416/50000]\tLoss: 0.77\n","Training Epoch: 147 [28672/50000]\tLoss: 0.72\n","Training Epoch: 147 [28928/50000]\tLoss: 0.74\n","Training Epoch: 147 [29184/50000]\tLoss: 0.82\n","Training Epoch: 147 [29440/50000]\tLoss: 0.91\n","Training Epoch: 147 [29696/50000]\tLoss: 0.88\n","Training Epoch: 147 [29952/50000]\tLoss: 0.63\n","Training Epoch: 147 [30208/50000]\tLoss: 0.63\n","Training Epoch: 147 [30464/50000]\tLoss: 0.63\n","Training Epoch: 147 [30720/50000]\tLoss: 0.70\n","Training Epoch: 147 [30976/50000]\tLoss: 0.84\n","Training Epoch: 147 [31232/50000]\tLoss: 0.80\n","Training Epoch: 147 [31488/50000]\tLoss: 0.81\n","Training Epoch: 147 [31744/50000]\tLoss: 0.67\n","Training Epoch: 147 [32000/50000]\tLoss: 0.82\n","Training Epoch: 147 [32256/50000]\tLoss: 0.69\n","Training Epoch: 147 [32512/50000]\tLoss: 0.85\n","Training Epoch: 147 [32768/50000]\tLoss: 0.72\n","Training Epoch: 147 [33024/50000]\tLoss: 0.81\n","Training Epoch: 147 [33280/50000]\tLoss: 0.84\n","Training Epoch: 147 [33536/50000]\tLoss: 0.83\n","Training Epoch: 147 [33792/50000]\tLoss: 0.68\n","Training Epoch: 147 [34048/50000]\tLoss: 0.63\n","Training Epoch: 147 [34304/50000]\tLoss: 0.66\n","Training Epoch: 147 [34560/50000]\tLoss: 0.92\n","Training Epoch: 147 [34816/50000]\tLoss: 0.79\n","Training Epoch: 147 [35072/50000]\tLoss: 0.69\n","Training Epoch: 147 [35328/50000]\tLoss: 0.80\n","Training Epoch: 147 [35584/50000]\tLoss: 0.65\n","Training Epoch: 147 [35840/50000]\tLoss: 0.83\n","Training Epoch: 147 [36096/50000]\tLoss: 0.71\n","Training Epoch: 147 [36352/50000]\tLoss: 0.74\n","Training Epoch: 147 [36608/50000]\tLoss: 0.73\n","Training Epoch: 147 [36864/50000]\tLoss: 0.66\n","Training Epoch: 147 [37120/50000]\tLoss: 0.67\n","Training Epoch: 147 [37376/50000]\tLoss: 0.81\n","Training Epoch: 147 [37632/50000]\tLoss: 0.80\n","Training Epoch: 147 [37888/50000]\tLoss: 0.71\n","Training Epoch: 147 [38144/50000]\tLoss: 0.84\n","Training Epoch: 147 [38400/50000]\tLoss: 0.71\n","Training Epoch: 147 [38656/50000]\tLoss: 0.82\n","Training Epoch: 147 [38912/50000]\tLoss: 0.71\n","Training Epoch: 147 [39168/50000]\tLoss: 0.70\n","Training Epoch: 147 [39424/50000]\tLoss: 0.82\n","Training Epoch: 147 [39680/50000]\tLoss: 0.72\n","Training Epoch: 147 [39936/50000]\tLoss: 0.95\n","Training Epoch: 147 [40192/50000]\tLoss: 0.79\n","Training Epoch: 147 [40448/50000]\tLoss: 0.76\n","Training Epoch: 147 [40704/50000]\tLoss: 0.69\n","Training Epoch: 147 [40960/50000]\tLoss: 0.78\n","Training Epoch: 147 [41216/50000]\tLoss: 0.76\n","Training Epoch: 147 [41472/50000]\tLoss: 0.77\n","Training Epoch: 147 [41728/50000]\tLoss: 0.73\n","Training Epoch: 147 [41984/50000]\tLoss: 0.72\n","Training Epoch: 147 [42240/50000]\tLoss: 0.76\n","Training Epoch: 147 [42496/50000]\tLoss: 0.85\n","Training Epoch: 147 [42752/50000]\tLoss: 0.61\n","Training Epoch: 147 [43008/50000]\tLoss: 0.77\n","Training Epoch: 147 [43264/50000]\tLoss: 0.76\n","Training Epoch: 147 [43520/50000]\tLoss: 0.80\n","Training Epoch: 147 [43776/50000]\tLoss: 0.85\n","Training Epoch: 147 [44032/50000]\tLoss: 0.89\n","Training Epoch: 147 [44288/50000]\tLoss: 0.82\n","Training Epoch: 147 [44544/50000]\tLoss: 0.73\n","Training Epoch: 147 [44800/50000]\tLoss: 0.75\n","Training Epoch: 147 [45056/50000]\tLoss: 0.83\n","Training Epoch: 147 [45312/50000]\tLoss: 0.86\n","Training Epoch: 147 [45568/50000]\tLoss: 0.68\n","Training Epoch: 147 [45824/50000]\tLoss: 0.83\n","Training Epoch: 147 [46080/50000]\tLoss: 0.68\n","Training Epoch: 147 [46336/50000]\tLoss: 0.73\n","Training Epoch: 147 [46592/50000]\tLoss: 0.75\n","Training Epoch: 147 [46848/50000]\tLoss: 0.63\n","Training Epoch: 147 [47104/50000]\tLoss: 0.79\n","Training Epoch: 147 [47360/50000]\tLoss: 0.82\n","Training Epoch: 147 [47616/50000]\tLoss: 0.85\n","Training Epoch: 147 [47872/50000]\tLoss: 0.70\n","Training Epoch: 147 [48128/50000]\tLoss: 0.78\n","Training Epoch: 147 [48384/50000]\tLoss: 0.68\n","Training Epoch: 147 [48640/50000]\tLoss: 0.78\n","Training Epoch: 147 [48896/50000]\tLoss: 0.76\n","Training Epoch: 147 [49152/50000]\tLoss: 0.86\n","Training Epoch: 147 [49408/50000]\tLoss: 0.92\n","Training Epoch: 147 [49664/50000]\tLoss: 0.80\n","Training Epoch: 147 [49920/50000]\tLoss: 0.77\n","Training Epoch: 147 [50000/50000]\tLoss: 0.86\n","Time taken to train epoch 147: 27.16s\n","Testing Network for epoch:  147\n","Evaluation: Evaluation Time: 2.59s, Average loss: 0.0063, Accuracy: 0.6218, Recall: 0.6218, Precision: 0.6252\n","Training Epoch: 148 [256/50000]\tLoss: 0.66\n","Training Epoch: 148 [512/50000]\tLoss: 0.65\n","Training Epoch: 148 [768/50000]\tLoss: 0.71\n","Training Epoch: 148 [1024/50000]\tLoss: 0.81\n","Training Epoch: 148 [1280/50000]\tLoss: 0.65\n","Training Epoch: 148 [1536/50000]\tLoss: 0.69\n","Training Epoch: 148 [1792/50000]\tLoss: 0.76\n","Training Epoch: 148 [2048/50000]\tLoss: 0.75\n","Training Epoch: 148 [2304/50000]\tLoss: 0.76\n","Training Epoch: 148 [2560/50000]\tLoss: 0.79\n","Training Epoch: 148 [2816/50000]\tLoss: 0.81\n","Training Epoch: 148 [3072/50000]\tLoss: 0.83\n","Training Epoch: 148 [3328/50000]\tLoss: 0.73\n","Training Epoch: 148 [3584/50000]\tLoss: 0.76\n","Training Epoch: 148 [3840/50000]\tLoss: 0.66\n","Training Epoch: 148 [4096/50000]\tLoss: 0.79\n","Training Epoch: 148 [4352/50000]\tLoss: 0.65\n","Training Epoch: 148 [4608/50000]\tLoss: 0.84\n","Training Epoch: 148 [4864/50000]\tLoss: 0.83\n","Training Epoch: 148 [5120/50000]\tLoss: 0.65\n","Training Epoch: 148 [5376/50000]\tLoss: 0.63\n","Training Epoch: 148 [5632/50000]\tLoss: 0.87\n","Training Epoch: 148 [5888/50000]\tLoss: 1.00\n","Training Epoch: 148 [6144/50000]\tLoss: 0.71\n","Training Epoch: 148 [6400/50000]\tLoss: 0.69\n","Training Epoch: 148 [6656/50000]\tLoss: 0.65\n","Training Epoch: 148 [6912/50000]\tLoss: 0.64\n","Training Epoch: 148 [7168/50000]\tLoss: 0.67\n","Training Epoch: 148 [7424/50000]\tLoss: 0.77\n","Training Epoch: 148 [7680/50000]\tLoss: 0.68\n","Training Epoch: 148 [7936/50000]\tLoss: 0.76\n","Training Epoch: 148 [8192/50000]\tLoss: 0.73\n","Training Epoch: 148 [8448/50000]\tLoss: 0.72\n","Training Epoch: 148 [8704/50000]\tLoss: 0.82\n","Training Epoch: 148 [8960/50000]\tLoss: 0.89\n","Training Epoch: 148 [9216/50000]\tLoss: 0.88\n","Training Epoch: 148 [9472/50000]\tLoss: 0.72\n","Training Epoch: 148 [9728/50000]\tLoss: 0.73\n","Training Epoch: 148 [9984/50000]\tLoss: 0.74\n","Training Epoch: 148 [10240/50000]\tLoss: 0.71\n","Training Epoch: 148 [10496/50000]\tLoss: 0.72\n","Training Epoch: 148 [10752/50000]\tLoss: 0.72\n","Training Epoch: 148 [11008/50000]\tLoss: 0.73\n","Training Epoch: 148 [11264/50000]\tLoss: 0.74\n","Training Epoch: 148 [11520/50000]\tLoss: 0.89\n","Training Epoch: 148 [11776/50000]\tLoss: 0.75\n","Training Epoch: 148 [12032/50000]\tLoss: 0.86\n","Training Epoch: 148 [12288/50000]\tLoss: 0.83\n","Training Epoch: 148 [12544/50000]\tLoss: 0.72\n","Training Epoch: 148 [12800/50000]\tLoss: 0.75\n","Training Epoch: 148 [13056/50000]\tLoss: 0.70\n","Training Epoch: 148 [13312/50000]\tLoss: 0.74\n","Training Epoch: 148 [13568/50000]\tLoss: 0.74\n","Training Epoch: 148 [13824/50000]\tLoss: 0.74\n","Training Epoch: 148 [14080/50000]\tLoss: 0.70\n","Training Epoch: 148 [14336/50000]\tLoss: 0.84\n","Training Epoch: 148 [14592/50000]\tLoss: 0.66\n","Training Epoch: 148 [14848/50000]\tLoss: 0.84\n","Training Epoch: 148 [15104/50000]\tLoss: 0.84\n","Training Epoch: 148 [15360/50000]\tLoss: 0.80\n","Training Epoch: 148 [15616/50000]\tLoss: 0.75\n","Training Epoch: 148 [15872/50000]\tLoss: 0.76\n","Training Epoch: 148 [16128/50000]\tLoss: 0.73\n","Training Epoch: 148 [16384/50000]\tLoss: 0.68\n","Training Epoch: 148 [16640/50000]\tLoss: 0.80\n","Training Epoch: 148 [16896/50000]\tLoss: 0.69\n","Training Epoch: 148 [17152/50000]\tLoss: 0.71\n","Training Epoch: 148 [17408/50000]\tLoss: 0.67\n","Training Epoch: 148 [17664/50000]\tLoss: 0.78\n","Training Epoch: 148 [17920/50000]\tLoss: 0.65\n","Training Epoch: 148 [18176/50000]\tLoss: 0.68\n","Training Epoch: 148 [18432/50000]\tLoss: 0.80\n","Training Epoch: 148 [18688/50000]\tLoss: 0.70\n","Training Epoch: 148 [18944/50000]\tLoss: 0.72\n","Training Epoch: 148 [19200/50000]\tLoss: 0.71\n","Training Epoch: 148 [19456/50000]\tLoss: 0.76\n","Training Epoch: 148 [19712/50000]\tLoss: 0.77\n","Training Epoch: 148 [19968/50000]\tLoss: 0.84\n","Training Epoch: 148 [20224/50000]\tLoss: 0.84\n","Training Epoch: 148 [20480/50000]\tLoss: 0.82\n","Training Epoch: 148 [20736/50000]\tLoss: 0.66\n","Training Epoch: 148 [20992/50000]\tLoss: 0.79\n","Training Epoch: 148 [21248/50000]\tLoss: 0.69\n","Training Epoch: 148 [21504/50000]\tLoss: 0.64\n","Training Epoch: 148 [21760/50000]\tLoss: 0.68\n","Training Epoch: 148 [22016/50000]\tLoss: 0.84\n","Training Epoch: 148 [22272/50000]\tLoss: 0.76\n","Training Epoch: 148 [22528/50000]\tLoss: 0.70\n","Training Epoch: 148 [22784/50000]\tLoss: 0.71\n","Training Epoch: 148 [23040/50000]\tLoss: 0.77\n","Training Epoch: 148 [23296/50000]\tLoss: 0.82\n","Training Epoch: 148 [23552/50000]\tLoss: 0.70\n","Training Epoch: 148 [23808/50000]\tLoss: 0.65\n","Training Epoch: 148 [24064/50000]\tLoss: 0.65\n","Training Epoch: 148 [24320/50000]\tLoss: 0.75\n","Training Epoch: 148 [24576/50000]\tLoss: 0.83\n","Training Epoch: 148 [24832/50000]\tLoss: 0.80\n","Training Epoch: 148 [25088/50000]\tLoss: 0.66\n","Training Epoch: 148 [25344/50000]\tLoss: 0.78\n","Training Epoch: 148 [25600/50000]\tLoss: 0.77\n","Training Epoch: 148 [25856/50000]\tLoss: 0.72\n","Training Epoch: 148 [26112/50000]\tLoss: 0.65\n","Training Epoch: 148 [26368/50000]\tLoss: 0.73\n","Training Epoch: 148 [26624/50000]\tLoss: 0.75\n","Training Epoch: 148 [26880/50000]\tLoss: 0.82\n","Training Epoch: 148 [27136/50000]\tLoss: 0.86\n","Training Epoch: 148 [27392/50000]\tLoss: 0.69\n","Training Epoch: 148 [27648/50000]\tLoss: 0.77\n","Training Epoch: 148 [27904/50000]\tLoss: 0.74\n","Training Epoch: 148 [28160/50000]\tLoss: 0.63\n","Training Epoch: 148 [28416/50000]\tLoss: 0.81\n","Training Epoch: 148 [28672/50000]\tLoss: 0.77\n","Training Epoch: 148 [28928/50000]\tLoss: 0.76\n","Training Epoch: 148 [29184/50000]\tLoss: 0.73\n","Training Epoch: 148 [29440/50000]\tLoss: 0.73\n","Training Epoch: 148 [29696/50000]\tLoss: 0.85\n","Training Epoch: 148 [29952/50000]\tLoss: 0.76\n","Training Epoch: 148 [30208/50000]\tLoss: 0.70\n","Training Epoch: 148 [30464/50000]\tLoss: 0.81\n","Training Epoch: 148 [30720/50000]\tLoss: 0.60\n","Training Epoch: 148 [30976/50000]\tLoss: 0.69\n","Training Epoch: 148 [31232/50000]\tLoss: 0.66\n","Training Epoch: 148 [31488/50000]\tLoss: 0.85\n","Training Epoch: 148 [31744/50000]\tLoss: 0.65\n","Training Epoch: 148 [32000/50000]\tLoss: 0.65\n","Training Epoch: 148 [32256/50000]\tLoss: 0.58\n","Training Epoch: 148 [32512/50000]\tLoss: 0.82\n","Training Epoch: 148 [32768/50000]\tLoss: 0.71\n","Training Epoch: 148 [33024/50000]\tLoss: 0.68\n","Training Epoch: 148 [33280/50000]\tLoss: 0.73\n","Training Epoch: 148 [33536/50000]\tLoss: 0.72\n","Training Epoch: 148 [33792/50000]\tLoss: 0.82\n","Training Epoch: 148 [34048/50000]\tLoss: 0.69\n","Training Epoch: 148 [34304/50000]\tLoss: 0.86\n","Training Epoch: 148 [34560/50000]\tLoss: 0.72\n","Training Epoch: 148 [34816/50000]\tLoss: 0.87\n","Training Epoch: 148 [35072/50000]\tLoss: 0.69\n","Training Epoch: 148 [35328/50000]\tLoss: 0.81\n","Training Epoch: 148 [35584/50000]\tLoss: 0.81\n","Training Epoch: 148 [35840/50000]\tLoss: 0.77\n","Training Epoch: 148 [36096/50000]\tLoss: 0.76\n","Training Epoch: 148 [36352/50000]\tLoss: 0.71\n","Training Epoch: 148 [36608/50000]\tLoss: 0.78\n","Training Epoch: 148 [36864/50000]\tLoss: 0.77\n","Training Epoch: 148 [37120/50000]\tLoss: 0.82\n","Training Epoch: 148 [37376/50000]\tLoss: 0.73\n","Training Epoch: 148 [37632/50000]\tLoss: 0.88\n","Training Epoch: 148 [37888/50000]\tLoss: 0.68\n","Training Epoch: 148 [38144/50000]\tLoss: 0.75\n","Training Epoch: 148 [38400/50000]\tLoss: 0.80\n","Training Epoch: 148 [38656/50000]\tLoss: 0.74\n","Training Epoch: 148 [38912/50000]\tLoss: 0.65\n","Training Epoch: 148 [39168/50000]\tLoss: 0.74\n","Training Epoch: 148 [39424/50000]\tLoss: 0.70\n","Training Epoch: 148 [39680/50000]\tLoss: 0.68\n","Training Epoch: 148 [39936/50000]\tLoss: 0.74\n","Training Epoch: 148 [40192/50000]\tLoss: 0.74\n","Training Epoch: 148 [40448/50000]\tLoss: 0.75\n","Training Epoch: 148 [40704/50000]\tLoss: 0.77\n","Training Epoch: 148 [40960/50000]\tLoss: 0.70\n","Training Epoch: 148 [41216/50000]\tLoss: 0.75\n","Training Epoch: 148 [41472/50000]\tLoss: 0.74\n","Training Epoch: 148 [41728/50000]\tLoss: 0.69\n","Training Epoch: 148 [41984/50000]\tLoss: 0.82\n","Training Epoch: 148 [42240/50000]\tLoss: 0.81\n","Training Epoch: 148 [42496/50000]\tLoss: 0.72\n","Training Epoch: 148 [42752/50000]\tLoss: 0.93\n","Training Epoch: 148 [43008/50000]\tLoss: 0.78\n","Training Epoch: 148 [43264/50000]\tLoss: 0.75\n","Training Epoch: 148 [43520/50000]\tLoss: 0.75\n","Training Epoch: 148 [43776/50000]\tLoss: 0.87\n","Training Epoch: 148 [44032/50000]\tLoss: 0.70\n","Training Epoch: 148 [44288/50000]\tLoss: 0.64\n","Training Epoch: 148 [44544/50000]\tLoss: 0.84\n","Training Epoch: 148 [44800/50000]\tLoss: 0.79\n","Training Epoch: 148 [45056/50000]\tLoss: 0.67\n","Training Epoch: 148 [45312/50000]\tLoss: 0.80\n","Training Epoch: 148 [45568/50000]\tLoss: 0.63\n","Training Epoch: 148 [45824/50000]\tLoss: 0.78\n","Training Epoch: 148 [46080/50000]\tLoss: 0.91\n","Training Epoch: 148 [46336/50000]\tLoss: 0.74\n","Training Epoch: 148 [46592/50000]\tLoss: 0.62\n","Training Epoch: 148 [46848/50000]\tLoss: 0.65\n","Training Epoch: 148 [47104/50000]\tLoss: 0.74\n","Training Epoch: 148 [47360/50000]\tLoss: 0.73\n","Training Epoch: 148 [47616/50000]\tLoss: 0.78\n","Training Epoch: 148 [47872/50000]\tLoss: 0.74\n","Training Epoch: 148 [48128/50000]\tLoss: 0.67\n","Training Epoch: 148 [48384/50000]\tLoss: 0.83\n","Training Epoch: 148 [48640/50000]\tLoss: 0.62\n","Training Epoch: 148 [48896/50000]\tLoss: 0.72\n","Training Epoch: 148 [49152/50000]\tLoss: 0.74\n","Training Epoch: 148 [49408/50000]\tLoss: 0.65\n","Training Epoch: 148 [49664/50000]\tLoss: 0.73\n","Training Epoch: 148 [49920/50000]\tLoss: 0.66\n","Training Epoch: 148 [50000/50000]\tLoss: 0.79\n","Time taken to train epoch 148: 27.20s\n","Testing Network for epoch:  148\n","Evaluation: Evaluation Time: 2.59s, Average loss: 0.0063, Accuracy: 0.6192, Recall: 0.6192, Precision: 0.6269\n","Training Epoch: 149 [256/50000]\tLoss: 0.81\n","Training Epoch: 149 [512/50000]\tLoss: 0.76\n","Training Epoch: 149 [768/50000]\tLoss: 0.81\n","Training Epoch: 149 [1024/50000]\tLoss: 0.72\n","Training Epoch: 149 [1280/50000]\tLoss: 0.69\n","Training Epoch: 149 [1536/50000]\tLoss: 0.91\n","Training Epoch: 149 [1792/50000]\tLoss: 0.65\n","Training Epoch: 149 [2048/50000]\tLoss: 0.59\n","Training Epoch: 149 [2304/50000]\tLoss: 0.76\n","Training Epoch: 149 [2560/50000]\tLoss: 0.76\n","Training Epoch: 149 [2816/50000]\tLoss: 0.69\n","Training Epoch: 149 [3072/50000]\tLoss: 0.68\n","Training Epoch: 149 [3328/50000]\tLoss: 0.80\n","Training Epoch: 149 [3584/50000]\tLoss: 0.65\n","Training Epoch: 149 [3840/50000]\tLoss: 0.77\n","Training Epoch: 149 [4096/50000]\tLoss: 0.71\n","Training Epoch: 149 [4352/50000]\tLoss: 0.61\n","Training Epoch: 149 [4608/50000]\tLoss: 0.80\n","Training Epoch: 149 [4864/50000]\tLoss: 0.69\n","Training Epoch: 149 [5120/50000]\tLoss: 0.80\n","Training Epoch: 149 [5376/50000]\tLoss: 0.68\n","Training Epoch: 149 [5632/50000]\tLoss: 0.92\n","Training Epoch: 149 [5888/50000]\tLoss: 0.58\n","Training Epoch: 149 [6144/50000]\tLoss: 0.77\n","Training Epoch: 149 [6400/50000]\tLoss: 0.86\n","Training Epoch: 149 [6656/50000]\tLoss: 0.69\n","Training Epoch: 149 [6912/50000]\tLoss: 0.80\n","Training Epoch: 149 [7168/50000]\tLoss: 0.68\n","Training Epoch: 149 [7424/50000]\tLoss: 0.66\n","Training Epoch: 149 [7680/50000]\tLoss: 0.71\n","Training Epoch: 149 [7936/50000]\tLoss: 0.65\n","Training Epoch: 149 [8192/50000]\tLoss: 0.66\n","Training Epoch: 149 [8448/50000]\tLoss: 0.85\n","Training Epoch: 149 [8704/50000]\tLoss: 0.72\n","Training Epoch: 149 [8960/50000]\tLoss: 0.67\n","Training Epoch: 149 [9216/50000]\tLoss: 0.74\n","Training Epoch: 149 [9472/50000]\tLoss: 0.86\n","Training Epoch: 149 [9728/50000]\tLoss: 0.67\n","Training Epoch: 149 [9984/50000]\tLoss: 0.88\n","Training Epoch: 149 [10240/50000]\tLoss: 0.78\n","Training Epoch: 149 [10496/50000]\tLoss: 0.77\n","Training Epoch: 149 [10752/50000]\tLoss: 0.75\n","Training Epoch: 149 [11008/50000]\tLoss: 0.84\n","Training Epoch: 149 [11264/50000]\tLoss: 0.66\n","Training Epoch: 149 [11520/50000]\tLoss: 0.88\n","Training Epoch: 149 [11776/50000]\tLoss: 0.72\n","Training Epoch: 149 [12032/50000]\tLoss: 0.71\n","Training Epoch: 149 [12288/50000]\tLoss: 0.78\n","Training Epoch: 149 [12544/50000]\tLoss: 0.82\n","Training Epoch: 149 [12800/50000]\tLoss: 0.70\n","Training Epoch: 149 [13056/50000]\tLoss: 0.74\n","Training Epoch: 149 [13312/50000]\tLoss: 0.76\n","Training Epoch: 149 [13568/50000]\tLoss: 0.72\n","Training Epoch: 149 [13824/50000]\tLoss: 0.72\n","Training Epoch: 149 [14080/50000]\tLoss: 0.66\n","Training Epoch: 149 [14336/50000]\tLoss: 0.78\n","Training Epoch: 149 [14592/50000]\tLoss: 0.85\n","Training Epoch: 149 [14848/50000]\tLoss: 0.78\n","Training Epoch: 149 [15104/50000]\tLoss: 0.68\n","Training Epoch: 149 [15360/50000]\tLoss: 0.73\n","Training Epoch: 149 [15616/50000]\tLoss: 0.77\n","Training Epoch: 149 [15872/50000]\tLoss: 0.65\n","Training Epoch: 149 [16128/50000]\tLoss: 0.82\n","Training Epoch: 149 [16384/50000]\tLoss: 0.74\n","Training Epoch: 149 [16640/50000]\tLoss: 0.72\n","Training Epoch: 149 [16896/50000]\tLoss: 0.67\n","Training Epoch: 149 [17152/50000]\tLoss: 0.68\n","Training Epoch: 149 [17408/50000]\tLoss: 0.75\n","Training Epoch: 149 [17664/50000]\tLoss: 0.76\n","Training Epoch: 149 [17920/50000]\tLoss: 0.79\n","Training Epoch: 149 [18176/50000]\tLoss: 0.86\n","Training Epoch: 149 [18432/50000]\tLoss: 0.61\n","Training Epoch: 149 [18688/50000]\tLoss: 0.62\n","Training Epoch: 149 [18944/50000]\tLoss: 0.67\n","Training Epoch: 149 [19200/50000]\tLoss: 0.77\n","Training Epoch: 149 [19456/50000]\tLoss: 0.72\n","Training Epoch: 149 [19712/50000]\tLoss: 0.66\n","Training Epoch: 149 [19968/50000]\tLoss: 0.75\n","Training Epoch: 149 [20224/50000]\tLoss: 0.80\n","Training Epoch: 149 [20480/50000]\tLoss: 0.71\n","Training Epoch: 149 [20736/50000]\tLoss: 0.80\n","Training Epoch: 149 [20992/50000]\tLoss: 0.64\n","Training Epoch: 149 [21248/50000]\tLoss: 0.69\n","Training Epoch: 149 [21504/50000]\tLoss: 0.69\n","Training Epoch: 149 [21760/50000]\tLoss: 0.82\n","Training Epoch: 149 [22016/50000]\tLoss: 0.61\n","Training Epoch: 149 [22272/50000]\tLoss: 0.71\n","Training Epoch: 149 [22528/50000]\tLoss: 0.74\n","Training Epoch: 149 [22784/50000]\tLoss: 0.79\n","Training Epoch: 149 [23040/50000]\tLoss: 0.73\n","Training Epoch: 149 [23296/50000]\tLoss: 0.73\n","Training Epoch: 149 [23552/50000]\tLoss: 0.86\n","Training Epoch: 149 [23808/50000]\tLoss: 0.77\n","Training Epoch: 149 [24064/50000]\tLoss: 0.79\n","Training Epoch: 149 [24320/50000]\tLoss: 0.87\n","Training Epoch: 149 [24576/50000]\tLoss: 0.73\n","Training Epoch: 149 [24832/50000]\tLoss: 0.78\n","Training Epoch: 149 [25088/50000]\tLoss: 0.74\n","Training Epoch: 149 [25344/50000]\tLoss: 0.84\n","Training Epoch: 149 [25600/50000]\tLoss: 0.76\n","Training Epoch: 149 [25856/50000]\tLoss: 0.64\n","Training Epoch: 149 [26112/50000]\tLoss: 0.89\n","Training Epoch: 149 [26368/50000]\tLoss: 0.82\n","Training Epoch: 149 [26624/50000]\tLoss: 0.82\n","Training Epoch: 149 [26880/50000]\tLoss: 0.68\n","Training Epoch: 149 [27136/50000]\tLoss: 0.83\n","Training Epoch: 149 [27392/50000]\tLoss: 0.69\n","Training Epoch: 149 [27648/50000]\tLoss: 0.65\n","Training Epoch: 149 [27904/50000]\tLoss: 0.73\n","Training Epoch: 149 [28160/50000]\tLoss: 0.66\n","Training Epoch: 149 [28416/50000]\tLoss: 0.76\n","Training Epoch: 149 [28672/50000]\tLoss: 0.84\n","Training Epoch: 149 [28928/50000]\tLoss: 0.74\n","Training Epoch: 149 [29184/50000]\tLoss: 0.77\n","Training Epoch: 149 [29440/50000]\tLoss: 0.71\n","Training Epoch: 149 [29696/50000]\tLoss: 0.67\n","Training Epoch: 149 [29952/50000]\tLoss: 0.77\n","Training Epoch: 149 [30208/50000]\tLoss: 0.66\n","Training Epoch: 149 [30464/50000]\tLoss: 0.75\n","Training Epoch: 149 [30720/50000]\tLoss: 0.77\n","Training Epoch: 149 [30976/50000]\tLoss: 0.71\n","Training Epoch: 149 [31232/50000]\tLoss: 0.80\n","Training Epoch: 149 [31488/50000]\tLoss: 0.79\n","Training Epoch: 149 [31744/50000]\tLoss: 0.72\n","Training Epoch: 149 [32000/50000]\tLoss: 0.64\n","Training Epoch: 149 [32256/50000]\tLoss: 0.78\n","Training Epoch: 149 [32512/50000]\tLoss: 0.79\n","Training Epoch: 149 [32768/50000]\tLoss: 0.71\n","Training Epoch: 149 [33024/50000]\tLoss: 0.69\n","Training Epoch: 149 [33280/50000]\tLoss: 0.61\n","Training Epoch: 149 [33536/50000]\tLoss: 0.79\n","Training Epoch: 149 [33792/50000]\tLoss: 0.76\n","Training Epoch: 149 [34048/50000]\tLoss: 0.75\n","Training Epoch: 149 [34304/50000]\tLoss: 0.91\n","Training Epoch: 149 [34560/50000]\tLoss: 0.99\n","Training Epoch: 149 [34816/50000]\tLoss: 0.73\n","Training Epoch: 149 [35072/50000]\tLoss: 0.73\n","Training Epoch: 149 [35328/50000]\tLoss: 0.62\n","Training Epoch: 149 [35584/50000]\tLoss: 0.81\n","Training Epoch: 149 [35840/50000]\tLoss: 0.67\n","Training Epoch: 149 [36096/50000]\tLoss: 0.61\n","Training Epoch: 149 [36352/50000]\tLoss: 0.76\n","Training Epoch: 149 [36608/50000]\tLoss: 0.80\n","Training Epoch: 149 [36864/50000]\tLoss: 0.76\n","Training Epoch: 149 [37120/50000]\tLoss: 0.79\n","Training Epoch: 149 [37376/50000]\tLoss: 0.74\n","Training Epoch: 149 [37632/50000]\tLoss: 0.74\n","Training Epoch: 149 [37888/50000]\tLoss: 0.74\n","Training Epoch: 149 [38144/50000]\tLoss: 0.77\n","Training Epoch: 149 [38400/50000]\tLoss: 0.81\n","Training Epoch: 149 [38656/50000]\tLoss: 0.81\n","Training Epoch: 149 [38912/50000]\tLoss: 0.74\n","Training Epoch: 149 [39168/50000]\tLoss: 0.77\n","Training Epoch: 149 [39424/50000]\tLoss: 0.75\n","Training Epoch: 149 [39680/50000]\tLoss: 0.77\n","Training Epoch: 149 [39936/50000]\tLoss: 0.70\n","Training Epoch: 149 [40192/50000]\tLoss: 0.81\n","Training Epoch: 149 [40448/50000]\tLoss: 0.72\n","Training Epoch: 149 [40704/50000]\tLoss: 0.82\n","Training Epoch: 149 [40960/50000]\tLoss: 0.76\n","Training Epoch: 149 [41216/50000]\tLoss: 0.64\n","Training Epoch: 149 [41472/50000]\tLoss: 0.71\n","Training Epoch: 149 [41728/50000]\tLoss: 0.84\n","Training Epoch: 149 [41984/50000]\tLoss: 0.86\n","Training Epoch: 149 [42240/50000]\tLoss: 0.71\n","Training Epoch: 149 [42496/50000]\tLoss: 0.63\n","Training Epoch: 149 [42752/50000]\tLoss: 0.78\n","Training Epoch: 149 [43008/50000]\tLoss: 0.59\n","Training Epoch: 149 [43264/50000]\tLoss: 0.85\n","Training Epoch: 149 [43520/50000]\tLoss: 0.73\n","Training Epoch: 149 [43776/50000]\tLoss: 0.82\n","Training Epoch: 149 [44032/50000]\tLoss: 0.76\n","Training Epoch: 149 [44288/50000]\tLoss: 0.79\n","Training Epoch: 149 [44544/50000]\tLoss: 0.84\n","Training Epoch: 149 [44800/50000]\tLoss: 0.79\n","Training Epoch: 149 [45056/50000]\tLoss: 0.76\n","Training Epoch: 149 [45312/50000]\tLoss: 0.79\n","Training Epoch: 149 [45568/50000]\tLoss: 0.73\n","Training Epoch: 149 [45824/50000]\tLoss: 0.78\n","Training Epoch: 149 [46080/50000]\tLoss: 0.67\n","Training Epoch: 149 [46336/50000]\tLoss: 0.66\n","Training Epoch: 149 [46592/50000]\tLoss: 0.82\n","Training Epoch: 149 [46848/50000]\tLoss: 0.68\n","Training Epoch: 149 [47104/50000]\tLoss: 0.74\n","Training Epoch: 149 [47360/50000]\tLoss: 0.69\n","Training Epoch: 149 [47616/50000]\tLoss: 0.74\n","Training Epoch: 149 [47872/50000]\tLoss: 0.75\n","Training Epoch: 149 [48128/50000]\tLoss: 0.71\n","Training Epoch: 149 [48384/50000]\tLoss: 0.71\n","Training Epoch: 149 [48640/50000]\tLoss: 0.86\n","Training Epoch: 149 [48896/50000]\tLoss: 0.77\n","Training Epoch: 149 [49152/50000]\tLoss: 0.71\n","Training Epoch: 149 [49408/50000]\tLoss: 0.72\n","Training Epoch: 149 [49664/50000]\tLoss: 0.75\n","Training Epoch: 149 [49920/50000]\tLoss: 0.70\n","Training Epoch: 149 [50000/50000]\tLoss: 0.71\n","Time taken to train epoch 149: 27.14s\n","Testing Network for epoch:  149\n","Evaluation: Evaluation Time: 2.51s, Average loss: 0.0062, Accuracy: 0.6204, Recall: 0.6204, Precision: 0.6248\n","Early Stopping the model training as there no significant improvment in the eval loss.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"svWwRW3ossD7","executionInfo":{"status":"ok","timestamp":1602868667147,"user_tz":240,"elapsed":4128,"user":{"displayName":"Try It","photoUrl":"","userId":"11883681010807328450"}},"outputId":"442010d2-0d5d-47e6-8901-85d9c664c9f9","colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["# Run this code bloack to load the model and compute different metrics on validation dataset.  \n","from sklearn.metrics import precision_score, recall_score, accuracy_score\n","import torchvision.transforms as transforms\n","import os\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data import DataLoader\n","import sys\n","\n","# mean and std are computed on the training set. \n","mean = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n","std = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n","\n","batch_norm = True\n","dropout = False\n","setting = ''\n","if dropout:\n","    setting = 'Dropout'\n","elif batch_norm:\n","    setting = 'BatchNormalization'\n","else:\n","    setting = 'NoRegularization'\n","\n","DATA_ROOT = './data'\n","checkpoints_path = '/content/drive/My Drive/Deep Learning Assignment/checkpoints'\n","checkpoints_path = os.path.join(checkpoints_path, '{model}_{setting}_{optimizer}')\n","batch_size = 256\n","device = 'cpu'\n","try:\n","    model = VGG16(make_layers(config, batch_norm = batch_norm), dropout = dropout, init_weights=False)\n","except Exception as e:\n","    sys.exit(\"Please load the model by running the first block.\")\n","\n","if torch.cuda.is_available():\n","    device = 'cuda'\n","    model.cuda()\n","\n","# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n","\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)])\n","\n","# Load testing data from CIFAR100\n","test_data = torchvision.datasets.CIFAR100(root=DATA_ROOT, train=False, download=True, transform=transform_test)\n","test_loader = DataLoader(test_data, shuffle=True, num_workers=4, batch_size=batch_size)\n","cuda_available = torch.cuda.is_available()\n","\n","print(\"Model Weights at path: \", checkpoints_path.format(\n","            model=str(model.__class__.__name__), \n","            setting=setting, \n","            optimizer=str(optimizer.__class__.__name__)))\n","# Load model weights.\n","model.load_state_dict(torch.load(checkpoints_path.format(\n","            model=str(model.__class__.__name__), \n","            setting=setting, \n","            optimizer=str(optimizer.__class__.__name__)), map_location=torch.device(device)))\n","\n","\n","model.eval()\n","accuracy = 0.0\n","precision = 0.0\n","recall = 0\n","\n","with torch.no_grad():\n","    y_pred = []\n","    y_true = []\n","    for iter, (image, labels) in enumerate(test_loader):\n","        # print(\"iteration: {}\\ttotal {} iterations\".format(iter + 1, len(test_loader)))\n","        # Convert the inputs to GPU compatible tensors. \n","        if cuda_available:\n","            image = image.cuda()\n","            label = labels.cuda()\n","        # Predicting\n","        model_outputs = model(image)\n","        output_values, predicted = model_outputs.max(1)\n","        y_pred.extend(predicted.cpu().tolist())\n","        y_true.extend(labels.cpu().tolist())\n","\n","accuracy = accuracy_score(y_true, y_pred)\n","recall = recall_score(y_true, y_pred, labels=range(100), average = 'macro')#average='macro', zero_division=1)\n","precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n","\n","print('\\n')\n","print(\"Model Setting: \", '{model}_{setting}_{optimizer}'.format(\n","            model=str(model.__class__.__name__), \n","            setting=setting, \n","            optimizer=str(optimizer.__class__.__name__)))\n","\n","\n","print(\"Accuracy: \", accuracy)\n","print(\"Precision: \", precision)\n","print(\"Recall: \", recall)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Model Weights at path:  /content/drive/My Drive/Deep Learning Assignment/checkpoints/VGG16_BatchNormalization_Adam\n","\n","\n","Model Setting:  VGG16_BatchNormalization_Adam\n","Accuracy:  0.6233\n","Precision:  0.6246941412015978\n","Recall:  0.6233000000000001\n"],"name":"stdout"}]}]}